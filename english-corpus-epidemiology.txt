Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in defined populations .
It is a cornerstone of public health , and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare . Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review ).  Epidemiology has helped develop methodology used in clinical research , public health studies, and, to a lesser extent, basic research in the biological sciences. [1]
Major areas of epidemiological study include disease causation, transmission , outbreak investigation, disease surveillance , environmental epidemiology , forensic epidemiology , occupational epidemiology , screening , biomonitoring , and comparisons of treatment effects such as in clinical trials . Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment .
Epidemiology , literally meaning "the study of what is upon the people", is derived from Greek epi 'upon, among', demos 'people, district', and logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term " epizoology " is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology ). [2]
The distinction between "epidemic" and "endemic" was first drawn by Hippocrates , [3] to distinguish between diseases that are "visited upon" a population (epidemic) from those that "reside within" a population (endemic). [4] The term "epidemiology" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in Epidemiología Española . [4] Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic .
The term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure, depression and obesity . Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.
The Greek physician Hippocrates , known as the father of medicine , [5] [6] sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences. [7] Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (black bile, yellow bile, blood, and phlegm). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine. [8] He coined the terms endemic (for diseases usually found in some places but not in others) and epidemic (for diseases that are seen at some times but not others). [9]
In the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that these very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen 's miasma theory (poison gas in sick people). In 1543 he wrote a book De contagione et contagiosis morbis , in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Antonie van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease .
A physician ahead of his time, Quinto Tiberio Angelerio, managed the 1582 plague in the town of Alghero, Sardinia.
He was fresh from Sicily, which had endured a plague epidemic of its own in 1575.
Later he published a manual "ECTYPA PESTILENSIS STATUS ALGHERIAE SARDINIAE", detailing the 57 rules he had imposed upon the city.
A second edition, "EPIDEMIOLOGIA, SIVE TRACTATUS DE PESTE" was published in 1598.
Some of the rules he instituted, several as unpopular then as they are today, included lockdowns, physical distancing, washing groceries and textiles, restricting shopping to one person per household, quarantines, health passports, and others. Taken from Zaria Gorvett, BBC FUTURE 8th Jan 2021.
During the Ming Dynasty , Wu Youke (1582–1652) developed the idea that some diseases were caused by transmissible agents, which he called Li Qi (戾气 or pestilential factors) when he observed various epidemics rage around him between 1641 and 1644. [10] His book Wen Yi Lun (瘟疫论，Treatise on Pestilence/Treatise of Epidemic Diseases) can be regarded as the main etiological work that brought forward the concept. [11] His concepts were still being considered in analysing SARS outbreak by WHO in 2004 in the context of traditional Chinese medicine. [12]
Another pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated. [8]
John Graunt , a haberdasher and amateur statistician, published Natural and Political Observations ... upon the Bills of Mortality in 1662. In it, he analysed the mortality rolls in London before the Great Plague , presented one of the first life tables , and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.
John Snow is famous for his investigations into the causes of the 19th-century cholera epidemics, and is also known as the father of (modern) epidemiology. [13] [14] He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world. [15] [16] However, Snow's research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death due to the prevailing Miasma Theory of the time, a model of disease in which poor air quality was blamed for illness. This was used to rationalize high rates of infection in impoverished areas instead of addressing the underlying issues of poor nutrition and sanitation, and was proven false by his work. [17]
Other pioneers include Danish physician Peter Anton Schleisner , who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland . [18] [19] Another important pioneer was Hungarian physician Ignaz Semmelweis , who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill-received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister 'discovered' antiseptics in 1865 in light of the work of Louis Pasteur .
In the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross , Janet Lane-Claypon , Anderson Gray McKendrick , and others. [20] [21] [22] [23] In a parallel development during the 1920s, German-Swiss pathologist Max Askanazy and others founded the International Society for Geographical Pathology to systematically investigate the geographical pathology of cancer and other non-infectious diseases across populations in different regions. After World War II, Richard Doll and other non-pathologists joined the field and advanced methods to study cancer, a disease with patterns and mode of occurrences that could not be suitably studied with the methods developed for epidemics of infectious diseases. Geography pathology eventually combined with infectious disease epidemiology to make the field that is epidemiology today. [24]
Another breakthrough was the 1954 publication of the results of a British Doctors Study , led by Richard Doll and Austin Bradford Hill , which lent very strong statistical support to the link between tobacco smoking and lung cancer .
In the late 20th century, with the advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level and disease was broadly named " molecular epidemiology ". Specifically, " genetic epidemiology " has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes.
Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions.
While most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease progression represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual ("the unique disease principle"), [25] [26] considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer ) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges, including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs. [27] Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine , " molecular pathology " and "epidemiology" was integrated to create a new interdisciplinary field of " molecular pathological epidemiology " (MPE), [28] [29] defined as "epidemiology of molecular pathology and heterogeneity of disease". In MPE, investigators analyze the relationships between (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases. [30] The concept and paradigm of MPE have become widespread in the 2010s. [31] [32] [33] [34] [35] [36] [37]
By 2012, it was recognized that many pathogens' evolution is rapid enough to be highly relevant to epidemiology, and that therefore much could be gained from an interdisciplinary approach to infectious disease integrating epidemiology and molecular evolution to "inform control strategies, or even patient treatment." [38] [39]
Modern epidemiological studies can use advanced statistics and machine learning to create predictive models as well as to define treatment effects. [40] [41]
Epidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive (involving the assessment of data covering time, place, and person), analytic (aiming to further examine known associations or hypothesized relationships), and experimental (a term often equated with clinical or community trials of treatments and other interventions). In observational studies, nature is allowed to "take its course," as epidemiologists observe from the sidelines. Conversely, in experimental studies, the epidemiologist is the one in control of all of the factors entering a certain case study. [42] Epidemiological studies are aimed, where possible, at revealing unbiased relationships between exposures such as alcohol or smoking, biological agents , stress , or chemicals to mortality or morbidity . The identification of causal relationships between these exposures and outcomes is an important aspect of epidemiology. Modern epidemiologists use informatics as a tool.
Observational studies have two components, descriptive and analytical. Descriptive observations pertain to the "who, what, where and when of health-related state occurrence". However, analytical observations deal more with the ‘how’ of a health-related event. [42] Experimental epidemiology contains three case types: randomized controlled trials (often used for new medicine or drug testing), field trials (conducted on those at a high risk of contracting a disease), and community trials (research on social originating diseases). [42]
The term 'epidemiologic triad' is used to describe the intersection of Host , Agent , and Environment in analyzing an outbreak.
Case-series may refer to the qualitative study of the experience of a single patient, or small group of patients with a similar diagnosis, or to a statistical factor with the potential to produce illness with periods when they are unexposed.
The former type of study is purely descriptive and cannot be used to make inferences about the general population of patients with that disease. These types of studies, in which an astute clinician identifies an unusual feature of a disease or a patient's history, may lead to a formulation of a new hypothesis. Using the data from the series, analytic studies could be done to investigate possible causal factors. These can include case-control studies or prospective studies. A case-control study would involve matching comparable controls without the disease to the cases in the series. A prospective study would involve following the case series over time to evaluate the disease's natural history. [43]
The latter type, more formally described as self-controlled case-series studies, divide individual patient follow-up time into exposed and unexposed periods and use fixed-effects Poisson regression processes to compare the incidence rate of a given outcome between exposed and unexposed periods. This technique has been extensively used in the study of adverse reactions to vaccination and has been shown in some circumstances to provide statistical power comparable to that available in cohort studies.
Case-control studies select subjects based on their disease status. It is a retrospective study. A group of individuals that are disease positive (the "case" group) is compared with a group of disease negative individuals (the "control" group). The control group should ideally come from the same population that gave rise to the cases. The case-control study looks back through time at potential exposures that both groups (cases and controls) may have encountered. A 2×2 table is constructed, displaying exposed cases (A), exposed controls (B), unexposed cases (C) and unexposed controls (D). The statistic generated to measure association is the odds ratio (OR), which is the ratio of the odds of exposure in the cases (A/C) to the odds of exposure in the controls (B/D), i.e. OR = (AD/BC).
If the OR is significantly greater than 1, then the conclusion is "those with the disease are more likely to have been exposed," whereas if it is close to 1 then the exposure and disease are not likely associated. If the OR is far less than one, then this suggests that the exposure is a protective factor in the causation of the disease.
Case-control studies are usually faster and more cost-effective than cohort studies but are sensitive to bias (such as recall bias and selection bias ). The main challenge is to identify the appropriate control group; the distribution of exposure among the control group should be representative of the distribution in the population that gave rise to the cases. This can be achieved by drawing a random sample from the original population at risk. This has as a consequence that the control group can contain people with the disease under study when the disease has a high attack rate in a population.
A major drawback for case control studies is that, in order to be considered to be statistically significant, the minimum number of cases required at the 95% confidence interval is related to the odds ratio by the equation:
where N is the ratio of cases to controls.
As the odds ratio approaches 1, the number of cases required for statistical significance grows towards infinity; rendering case-control studies all but useless for low odds ratios. For instance, for an odds ratio of 1.5 and cases = controls, the table shown above would look like this:
For an odds ratio of 1.1:
Cohort studies select subjects based on their exposure status. The study subjects should be at risk of the outcome under investigation at the beginning of the cohort study; this usually means that they should be disease free when the cohort study starts. The cohort is followed through time to assess their later outcome status. An example of a cohort study would be the investigation of a cohort of smokers and non-smokers over time to estimate the incidence of lung cancer. The same 2×2 table is constructed as with the case control study. However, the point estimate generated is the relative risk (RR), which is the probability of disease for a person in the exposed group, P e = A / ( A + B ) over the probability of disease for a person in the unexposed group, P u = C / ( C + D ), i.e. RR = P e / P u .
As with the OR, a RR greater than 1 shows association, where the conclusion can be read "those with the exposure were more likely to develop disease."
Prospective studies have many benefits over case control studies. The RR is a more powerful effect measure than the OR, as the OR is just an estimation of the RR, since true incidence cannot be calculated in a case control study where subjects are selected based on disease status. Temporality can be established in a prospective study, and confounders are more easily controlled for. However, they are more costly, and there is a greater chance of losing subjects to follow-up based on the long time period over which the cohort is followed.
Cohort studies also are limited by the same equation for number of cases as for cohort studies, but, if the base incidence rate in the study population is very low, the number of cases required is reduced by ½.
Although epidemiology is sometimes viewed as a collection of statistical tools used to elucidate the associations of exposures to health outcomes, a deeper understanding of this science is that of discovering causal relationships.
" Correlation does not imply causation " is a common theme for much of the epidemiological literature. For epidemiologists, the key is in the term inference . Correlation, or at least association between two variables, is a necessary but not sufficient criterion for inference that one variable causes the other. Epidemiologists use gathered data and a broad range of biomedical and psychosocial theories in an iterative way to generate or expand theory, to test hypotheses, and to make educated, informed assertions about which relationships are causal, and about exactly how they are causal.
Epidemiologists emphasize that the " one cause – one effect " understanding is a simplistic mis-belief. [ citation needed ] Most outcomes, whether disease or death, are caused by a chain or web consisting of many component causes. [44] Causes can be distinguished as necessary, sufficient or probabilistic conditions. If a necessary condition can be identified and controlled (e.g., antibodies to a disease agent, energy in an injury), the harmful outcome can be avoided (Robertson, 2015). One tool regularly used to conceptualize the multicausality associated with disease is the causal pie model . [45]
In 1965, Austin Bradford Hill proposed a series of considerations to help assess evidence of causation, [46] which have come to be commonly known as the " Bradford Hill criteria ". In contrast to the explicit intentions of their author, Hill's considerations are now sometimes taught as a checklist to be implemented for assessing causality. [47] Hill himself said "None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required sine qua non ." [46]
Epidemiological studies can only go to prove that an agent could have caused, but not that it did cause, an effect in any particular case:
"Epidemiology is concerned with the incidence of disease in populations and does not address the question of the cause of an individual's disease. This question, sometimes referred to as specific causation, is beyond the domain of the science of epidemiology. Epidemiology has its limits at the point where an inference is made that the relationship between an agent and a disease is causal (general causation) and where the magnitude of excess risk attributed to the agent has been determined; that is, epidemiology addresses whether an agent can cause a disease, not whether an agent did cause a specific plaintiff's disease." [48]
In United States law, epidemiology alone cannot prove that a causal association does not exist in general. Conversely, it can be (and is in some circumstances) taken by US courts, in an individual case, to justify an inference that a causal association does exist, based upon a balance of probability .
The subdiscipline of forensic epidemiology is directed at the investigation of specific causation of disease or injury in individuals or groups of individuals in instances in which causation is disputed or is unclear, for presentation in legal settings.
Epidemiological practice and the results of epidemiological analysis make a significant contribution to emerging population-based health management frameworks.
Population-based health management encompasses the ability to:
Modern population-based health management is complex, requiring a multiple set of skills (medical, political, technological, mathematical, etc.) of which epidemiological practice and analysis is a core component, that is unified with management science to provide efficient and effective health care and health guidance to a population. This task requires the forward-looking ability of modern risk management approaches that transform health risk factors, incidence, prevalence and mortality statistics (derived from epidemiological analysis) into management metrics that not only guide how a health system responds to current population health issues but also how a health system can be managed to better respond to future potential population health issues. [49]
Examples of organizations that use population-based health management that leverage the work and results of epidemiological practice include Canadian Strategy for Cancer Control, Health Canada Tobacco Control Programs, Rick Hansen Foundation, Canadian Tobacco Control Research Initiative. [50] [51] [52]
Each of these organizations uses a population-based health management framework called Life at Risk that combines epidemiological quantitative analysis with demographics, health agency operational research and economics to perform:
Applied epidemiology is the practice of using epidemiological methods to protect or improve the health of a population. Applied field epidemiology can include investigating communicable and non-communicable disease outbreaks, mortality and morbidity rates, and nutritional status, among other indicators of health, with the purpose of communicating the results to those who can implement appropriate policies or disease control measures.
As the surveillance and reporting of diseases and other health factors become increasingly difficult in humanitarian crisis situations, the methodologies used to report the data are compromised. One study found that less than half (42.4%) of nutrition surveys sampled from humanitarian contexts correctly calculated the prevalence of malnutrition and only one-third (35.3%) of the surveys met the criteria for quality. Among the mortality surveys, only 3.2% met the criteria for quality. As nutritional status and mortality rates help indicate the severity of a crisis, the tracking and reporting of these health factors is crucial.
Vital registries are usually the most effective ways to collect data, but in humanitarian contexts these registries can be non-existent, unreliable, or inaccessible. As such, mortality is often inaccurately measured using either prospective demographic surveillance or retrospective mortality surveys. Prospective demographic surveillance requires much manpower and is difficult to implement in a spread-out population. Retrospective mortality surveys are prone to selection and reporting biases. Other methods are being developed, but are not common practice yet. [53] [54] [55] [56]
Different fields in epidemiology have different levels of validity. One way to assess the validity of findings is the ratio of false-positives (claimed effects that are not correct) to false-negatives (studies which fail to support a true effect). To take the field of genetic epidemiology, candidate-gene studies produced over 100 false-positive findings for each false-negative. By contrast genome-wide association appear close to the reverse, with only one false positive for every 100 or more false-negatives. [57] This ratio has improved over time in genetic epidemiology as the field has adopted stringent criteria. By contrast, other epidemiological fields have not required such rigorous reporting and are much less reliable as a result. [57]
Random error is the result of fluctuations around a true value because of sampling variability. Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.
There is random error in all sampling procedures. This is called sampling error .
Precision in epidemiological variables is a measure of random error. Precision is also inversely related to random error, so that to reduce random error is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.
There are two basic ways to reduce random error in an epidemiological study . The first is to increase the sample size of the study. In other words, add more subjects to your study. The second is to reduce the variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.
Note, that if sample size or number of measurements are increased, or a more precise measuring tool is purchased, the costs of the study are usually increased. There is usually an uneasy balance between the need for adequate precision and the practical issue of study cost.
A systematic error or bias occurs when there is a difference between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of systematic error is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate . Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).
A mistake in coding that affects all responses for that particular question is another example of a systematic error.
The validity of a study is dependent on the degree of systematic error. Validity is usually separated into two components:
Selection bias occurs when study subjects are selected or become part of the study as a result of a third, unmeasured variable which is associated with both the exposure and outcome of interest. [58] For instance, it has repeatedly been noted that cigarette smokers and non smokers tend to differ in their study participation rates. (Sackett D cites the example of Seltzer et al., in which 85% of non smokers and 67% of smokers returned mailed questionnaires.) [59] It is important to note that such a difference in response will not lead to bias if it is not also associated with a systematic difference in outcome between the two response groups.
Information bias is bias arising from systematic error in the assessment of a variable. [60] An example of this is recall bias. A typical example is again provided by Sackett in his discussion of a study examining the effect of specific exposures on fetal health: "in questioning mothers whose recent pregnancies had ended in fetal death or malformation (cases) and a matched group of mothers whose pregnancies ended normally (controls) it was found that 28% of the former, but only 20% of the latter, reported exposure to drugs which could not be substantiated either in earlier prospective interviews or in other health records". [59] In this example, recall bias probably occurred as a result of women who had had miscarriages having an apparent tendency to better recall and therefore report previous exposures.
Confounding has traditionally been defined as bias arising from the co-occurrence or mixing of effects of extraneous factors, referred to as confounders, with the main effect(s) of interest. [60] [61] A more recent definition of confounding invokes the notion of counterfactual effects. [61] According to this view, when one observes an outcome of interest, say Y=1 (as opposed to Y=0), in a given population A which is entirely exposed (i.e. exposure X = 1 for every unit of the population) the risk of this event will be R A1 . The counterfactual or unobserved risk R A0 corresponds to the risk which would have been observed if these same individuals had been unexposed (i.e. X = 0 for every unit of the population). The true effect of exposure therefore is: R A1 − R A0 (if one is interested in risk differences) or R A1 / R A0 (if one is interested in relative risk). Since the counterfactual risk R A0 is unobservable we approximate it using a second population B and we actually measure the following relations: R A1 − R B0 or R A1 / R B0 . In this situation, confounding occurs when R A0 ≠ R B0 . [61] (NB: Example assumes binary outcome and exposure variables.)
Some epidemiologists prefer to think of confounding separately from common categorizations of bias since, unlike selection and information bias, confounding stems from real causal effects. [58]
Few universities have offered epidemiology as a course of study at the undergraduate level. One notable undergraduate program exists at Johns Hopkins University , where students who major in public health can take graduate level courses, including epidemiology, during their senior year at the Bloomberg School of Public Health . [62]
Although epidemiologic research is conducted by individuals from diverse disciplines, including clinically trained professionals such as physicians, formal training is available through Masters or Doctoral programs including Master of Public Health (MPH), Master of Science of Epidemiology (MSc.), Doctor of Public Health (DrPH), Doctor of Pharmacy (PharmD), Doctor of Philosophy (PhD), Doctor of Science (ScD).  Many other graduate programs, e.g., Doctor of Social Work (DSW), Doctor of Clinical Practice (DClinP), Doctor of Podiatric Medicine (DPM), Doctor of Veterinary Medicine (DVM), Doctor of Nursing Practice (DNP), Doctor of Physical Therapy (DPT), or for clinically trained physicians, Doctor of Medicine (MD) or Bachelor of Medicine and Surgery (MBBS or MBChB) and Doctor of Osteopathic Medicine (DO), include some training in epidemiologic research or related topics, but this training is generally substantially less than offered in training programs focused on epidemiology or public health. Reflecting the strong historical tie between epidemiology and medicine, formal training programs may be set in either schools of public health and medical schools.
As public health/health protection practitioners, epidemiologists work in a number of different settings. Some epidemiologists work 'in the field'; i.e., in the community, commonly in a public health/health protection service, and are often at the forefront of investigating and combating disease outbreaks. Others work for non-profit organizations, universities, hospitals and larger government entities such as state and local health departments, various Ministries of Health, Doctors without Borders , the Centers for Disease Control and Prevention (CDC), the Health Protection Agency , the World Health Organization (WHO), or the Public Health Agency of Canada . Epidemiologists can also work in for-profit organizations such as pharmaceutical and medical device companies in groups such as market research or clinical development.
An April 2020 University of Southern California article noted that "The coronavirus epidemic ... thrust epidemiology – the study of the incidence, distribution and control of disease in a population – to the forefront of scientific disciplines across the globe and even made temporary celebrities out of some of its practitioners." [63]
On June 8, 2020, The New York Times published results of its survey of 511 epidemiologists asked "when they expect to resume 20 activities of daily life"; 52% of those surveyed expected to stop "routinely wearing a face covering" in one year or more. [64]
Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in defined populations .
It is a cornerstone of public health , and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare . Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review ).  Epidemiology has helped develop methodology used in clinical research , public health studies, and, to a lesser extent, basic research in the biological sciences. [1]
Major areas of epidemiological study include disease causation, transmission , outbreak investigation, disease surveillance , environmental epidemiology , forensic epidemiology , occupational epidemiology , screening , biomonitoring , and comparisons of treatment effects such as in clinical trials . Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment .
Epidemiology , literally meaning "the study of what is upon the people", is derived from Greek epi 'upon, among', demos 'people, district', and logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term " epizoology " is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology ). [2]
The distinction between "epidemic" and "endemic" was first drawn by Hippocrates , [3] to distinguish between diseases that are "visited upon" a population (epidemic) from those that "reside within" a population (endemic). [4] The term "epidemiology" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in Epidemiología Española . [4] Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic .
The term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure, depression and obesity . Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.
The Greek physician Hippocrates , known as the father of medicine , [5] [6] sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences. [7] Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (black bile, yellow bile, blood, and phlegm). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine. [8] He coined the terms endemic (for diseases usually found in some places but not in others) and epidemic (for diseases that are seen at some times but not others). [9]
In the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that these very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen 's miasma theory (poison gas in sick people). In 1543 he wrote a book De contagione et contagiosis morbis , in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Antonie van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease .
A physician ahead of his time, Quinto Tiberio Angelerio, managed the 1582 plague in the town of Alghero, Sardinia.
He was fresh from Sicily, which had endured a plague epidemic of its own in 1575.
Later he published a manual "ECTYPA PESTILENSIS STATUS ALGHERIAE SARDINIAE", detailing the 57 rules he had imposed upon the city.
A second edition, "EPIDEMIOLOGIA, SIVE TRACTATUS DE PESTE" was published in 1598.
Some of the rules he instituted, several as unpopular then as they are today, included lockdowns, physical distancing, washing groceries and textiles, restricting shopping to one person per household, quarantines, health passports, and others. Taken from Zaria Gorvett, BBC FUTURE 8th Jan 2021.
During the Ming Dynasty , Wu Youke (1582–1652) developed the idea that some diseases were caused by transmissible agents, which he called Li Qi (戾气 or pestilential factors) when he observed various epidemics rage around him between 1641 and 1644. [10] His book Wen Yi Lun (瘟疫论，Treatise on Pestilence/Treatise of Epidemic Diseases) can be regarded as the main etiological work that brought forward the concept. [11] His concepts were still being considered in analysing SARS outbreak by WHO in 2004 in the context of traditional Chinese medicine. [12]
Another pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated. [8]
John Graunt , a haberdasher and amateur statistician, published Natural and Political Observations ... upon the Bills of Mortality in 1662. In it, he analysed the mortality rolls in London before the Great Plague , presented one of the first life tables , and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.
John Snow is famous for his investigations into the causes of the 19th-century cholera epidemics, and is also known as the father of (modern) epidemiology. [13] [14] He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world. [15] [16] However, Snow's research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death due to the prevailing Miasma Theory of the time, a model of disease in which poor air quality was blamed for illness. This was used to rationalize high rates of infection in impoverished areas instead of addressing the underlying issues of poor nutrition and sanitation, and was proven false by his work. [17]
Other pioneers include Danish physician Peter Anton Schleisner , who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland . [18] [19] Another important pioneer was Hungarian physician Ignaz Semmelweis , who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill-received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister 'discovered' antiseptics in 1865 in light of the work of Louis Pasteur .
In the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross , Janet Lane-Claypon , Anderson Gray McKendrick , and others. [20] [21] [22] [23] In a parallel development during the 1920s, German-Swiss pathologist Max Askanazy and others founded the International Society for Geographical Pathology to systematically investigate the geographical pathology of cancer and other non-infectious diseases across populations in different regions. After World War II, Richard Doll and other non-pathologists joined the field and advanced methods to study cancer, a disease with patterns and mode of occurrences that could not be suitably studied with the methods developed for epidemics of infectious diseases. Geography pathology eventually combined with infectious disease epidemiology to make the field that is epidemiology today. [24]
Another breakthrough was the 1954 publication of the results of a British Doctors Study , led by Richard Doll and Austin Bradford Hill , which lent very strong statistical support to the link between tobacco smoking and lung cancer .
In the late 20th century, with the advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level and disease was broadly named " molecular epidemiology ". Specifically, " genetic epidemiology " has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes.
Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions.
While most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease progression represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual ("the unique disease principle"), [25] [26] considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer ) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges, including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs. [27] Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine , " molecular pathology " and "epidemiology" was integrated to create a new interdisciplinary field of " molecular pathological epidemiology " (MPE), [28] [29] defined as "epidemiology of molecular pathology and heterogeneity of disease". In MPE, investigators analyze the relationships between (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases. [30] The concept and paradigm of MPE have become widespread in the 2010s. [31] [32] [33] [34] [35] [36] [37]
By 2012, it was recognized that many pathogens' evolution is rapid enough to be highly relevant to epidemiology, and that therefore much could be gained from an interdisciplinary approach to infectious disease integrating epidemiology and molecular evolution to "inform control strategies, or even patient treatment." [38] [39]
Modern epidemiological studies can use advanced statistics and machine learning to create predictive models as well as to define treatment effects. [40] [41]
Epidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive (involving the assessment of data covering time, place, and person), analytic (aiming to further examine known associations or hypothesized relationships), and experimental (a term often equated with clinical or community trials of treatments and other interventions). In observational studies, nature is allowed to "take its course," as epidemiologists observe from the sidelines. Conversely, in experimental studies, the epidemiologist is the one in control of all of the factors entering a certain case study. [42] Epidemiological studies are aimed, where possible, at revealing unbiased relationships between exposures such as alcohol or smoking, biological agents , stress , or chemicals to mortality or morbidity . The identification of causal relationships between these exposures and outcomes is an important aspect of epidemiology. Modern epidemiologists use informatics as a tool.
Observational studies have two components, descriptive and analytical. Descriptive observations pertain to the "who, what, where and when of health-related state occurrence". However, analytical observations deal more with the ‘how’ of a health-related event. [42] Experimental epidemiology contains three case types: randomized controlled trials (often used for new medicine or drug testing), field trials (conducted on those at a high risk of contracting a disease), and community trials (research on social originating diseases). [42]
The term 'epidemiologic triad' is used to describe the intersection of Host , Agent , and Environment in analyzing an outbreak.
Case-series may refer to the qualitative study of the experience of a single patient, or small group of patients with a similar diagnosis, or to a statistical factor with the potential to produce illness with periods when they are unexposed.
The former type of study is purely descriptive and cannot be used to make inferences about the general population of patients with that disease. These types of studies, in which an astute clinician identifies an unusual feature of a disease or a patient's history, may lead to a formulation of a new hypothesis. Using the data from the series, analytic studies could be done to investigate possible causal factors. These can include case-control studies or prospective studies. A case-control study would involve matching comparable controls without the disease to the cases in the series. A prospective study would involve following the case series over time to evaluate the disease's natural history. [43]
The latter type, more formally described as self-controlled case-series studies, divide individual patient follow-up time into exposed and unexposed periods and use fixed-effects Poisson regression processes to compare the incidence rate of a given outcome between exposed and unexposed periods. This technique has been extensively used in the study of adverse reactions to vaccination and has been shown in some circumstances to provide statistical power comparable to that available in cohort studies.
Case-control studies select subjects based on their disease status. It is a retrospective study. A group of individuals that are disease positive (the "case" group) is compared with a group of disease negative individuals (the "control" group). The control group should ideally come from the same population that gave rise to the cases. The case-control study looks back through time at potential exposures that both groups (cases and controls) may have encountered. A 2×2 table is constructed, displaying exposed cases (A), exposed controls (B), unexposed cases (C) and unexposed controls (D). The statistic generated to measure association is the odds ratio (OR), which is the ratio of the odds of exposure in the cases (A/C) to the odds of exposure in the controls (B/D), i.e. OR = (AD/BC).
If the OR is significantly greater than 1, then the conclusion is "those with the disease are more likely to have been exposed," whereas if it is close to 1 then the exposure and disease are not likely associated. If the OR is far less than one, then this suggests that the exposure is a protective factor in the causation of the disease.
Case-control studies are usually faster and more cost-effective than cohort studies but are sensitive to bias (such as recall bias and selection bias ). The main challenge is to identify the appropriate control group; the distribution of exposure among the control group should be representative of the distribution in the population that gave rise to the cases. This can be achieved by drawing a random sample from the original population at risk. This has as a consequence that the control group can contain people with the disease under study when the disease has a high attack rate in a population.
A major drawback for case control studies is that, in order to be considered to be statistically significant, the minimum number of cases required at the 95% confidence interval is related to the odds ratio by the equation:
where N is the ratio of cases to controls.
As the odds ratio approaches 1, the number of cases required for statistical significance grows towards infinity; rendering case-control studies all but useless for low odds ratios. For instance, for an odds ratio of 1.5 and cases = controls, the table shown above would look like this:
For an odds ratio of 1.1:
Cohort studies select subjects based on their exposure status. The study subjects should be at risk of the outcome under investigation at the beginning of the cohort study; this usually means that they should be disease free when the cohort study starts. The cohort is followed through time to assess their later outcome status. An example of a cohort study would be the investigation of a cohort of smokers and non-smokers over time to estimate the incidence of lung cancer. The same 2×2 table is constructed as with the case control study. However, the point estimate generated is the relative risk (RR), which is the probability of disease for a person in the exposed group, P e = A / ( A + B ) over the probability of disease for a person in the unexposed group, P u = C / ( C + D ), i.e. RR = P e / P u .
As with the OR, a RR greater than 1 shows association, where the conclusion can be read "those with the exposure were more likely to develop disease."
Prospective studies have many benefits over case control studies. The RR is a more powerful effect measure than the OR, as the OR is just an estimation of the RR, since true incidence cannot be calculated in a case control study where subjects are selected based on disease status. Temporality can be established in a prospective study, and confounders are more easily controlled for. However, they are more costly, and there is a greater chance of losing subjects to follow-up based on the long time period over which the cohort is followed.
Cohort studies also are limited by the same equation for number of cases as for cohort studies, but, if the base incidence rate in the study population is very low, the number of cases required is reduced by ½.
Although epidemiology is sometimes viewed as a collection of statistical tools used to elucidate the associations of exposures to health outcomes, a deeper understanding of this science is that of discovering causal relationships.
" Correlation does not imply causation " is a common theme for much of the epidemiological literature. For epidemiologists, the key is in the term inference . Correlation, or at least association between two variables, is a necessary but not sufficient criterion for inference that one variable causes the other. Epidemiologists use gathered data and a broad range of biomedical and psychosocial theories in an iterative way to generate or expand theory, to test hypotheses, and to make educated, informed assertions about which relationships are causal, and about exactly how they are causal.
Epidemiologists emphasize that the " one cause – one effect " understanding is a simplistic mis-belief. [ citation needed ] Most outcomes, whether disease or death, are caused by a chain or web consisting of many component causes. [44] Causes can be distinguished as necessary, sufficient or probabilistic conditions. If a necessary condition can be identified and controlled (e.g., antibodies to a disease agent, energy in an injury), the harmful outcome can be avoided (Robertson, 2015). One tool regularly used to conceptualize the multicausality associated with disease is the causal pie model . [45]
In 1965, Austin Bradford Hill proposed a series of considerations to help assess evidence of causation, [46] which have come to be commonly known as the " Bradford Hill criteria ". In contrast to the explicit intentions of their author, Hill's considerations are now sometimes taught as a checklist to be implemented for assessing causality. [47] Hill himself said "None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required sine qua non ." [46]
Epidemiological studies can only go to prove that an agent could have caused, but not that it did cause, an effect in any particular case:
"Epidemiology is concerned with the incidence of disease in populations and does not address the question of the cause of an individual's disease. This question, sometimes referred to as specific causation, is beyond the domain of the science of epidemiology. Epidemiology has its limits at the point where an inference is made that the relationship between an agent and a disease is causal (general causation) and where the magnitude of excess risk attributed to the agent has been determined; that is, epidemiology addresses whether an agent can cause a disease, not whether an agent did cause a specific plaintiff's disease." [48]
In United States law, epidemiology alone cannot prove that a causal association does not exist in general. Conversely, it can be (and is in some circumstances) taken by US courts, in an individual case, to justify an inference that a causal association does exist, based upon a balance of probability .
The subdiscipline of forensic epidemiology is directed at the investigation of specific causation of disease or injury in individuals or groups of individuals in instances in which causation is disputed or is unclear, for presentation in legal settings.
Epidemiological practice and the results of epidemiological analysis make a significant contribution to emerging population-based health management frameworks.
Population-based health management encompasses the ability to:
Modern population-based health management is complex, requiring a multiple set of skills (medical, political, technological, mathematical, etc.) of which epidemiological practice and analysis is a core component, that is unified with management science to provide efficient and effective health care and health guidance to a population. This task requires the forward-looking ability of modern risk management approaches that transform health risk factors, incidence, prevalence and mortality statistics (derived from epidemiological analysis) into management metrics that not only guide how a health system responds to current population health issues but also how a health system can be managed to better respond to future potential population health issues. [49]
Examples of organizations that use population-based health management that leverage the work and results of epidemiological practice include Canadian Strategy for Cancer Control, Health Canada Tobacco Control Programs, Rick Hansen Foundation, Canadian Tobacco Control Research Initiative. [50] [51] [52]
Each of these organizations uses a population-based health management framework called Life at Risk that combines epidemiological quantitative analysis with demographics, health agency operational research and economics to perform:
Applied epidemiology is the practice of using epidemiological methods to protect or improve the health of a population. Applied field epidemiology can include investigating communicable and non-communicable disease outbreaks, mortality and morbidity rates, and nutritional status, among other indicators of health, with the purpose of communicating the results to those who can implement appropriate policies or disease control measures.
As the surveillance and reporting of diseases and other health factors become increasingly difficult in humanitarian crisis situations, the methodologies used to report the data are compromised. One study found that less than half (42.4%) of nutrition surveys sampled from humanitarian contexts correctly calculated the prevalence of malnutrition and only one-third (35.3%) of the surveys met the criteria for quality. Among the mortality surveys, only 3.2% met the criteria for quality. As nutritional status and mortality rates help indicate the severity of a crisis, the tracking and reporting of these health factors is crucial.
Vital registries are usually the most effective ways to collect data, but in humanitarian contexts these registries can be non-existent, unreliable, or inaccessible. As such, mortality is often inaccurately measured using either prospective demographic surveillance or retrospective mortality surveys. Prospective demographic surveillance requires much manpower and is difficult to implement in a spread-out population. Retrospective mortality surveys are prone to selection and reporting biases. Other methods are being developed, but are not common practice yet. [53] [54] [55] [56]
Different fields in epidemiology have different levels of validity. One way to assess the validity of findings is the ratio of false-positives (claimed effects that are not correct) to false-negatives (studies which fail to support a true effect). To take the field of genetic epidemiology, candidate-gene studies produced over 100 false-positive findings for each false-negative. By contrast genome-wide association appear close to the reverse, with only one false positive for every 100 or more false-negatives. [57] This ratio has improved over time in genetic epidemiology as the field has adopted stringent criteria. By contrast, other epidemiological fields have not required such rigorous reporting and are much less reliable as a result. [57]
Random error is the result of fluctuations around a true value because of sampling variability. Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.
There is random error in all sampling procedures. This is called sampling error .
Precision in epidemiological variables is a measure of random error. Precision is also inversely related to random error, so that to reduce random error is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.
There are two basic ways to reduce random error in an epidemiological study . The first is to increase the sample size of the study. In other words, add more subjects to your study. The second is to reduce the variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.
Note, that if sample size or number of measurements are increased, or a more precise measuring tool is purchased, the costs of the study are usually increased. There is usually an uneasy balance between the need for adequate precision and the practical issue of study cost.
A systematic error or bias occurs when there is a difference between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of systematic error is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate . Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).
A mistake in coding that affects all responses for that particular question is another example of a systematic error.
The validity of a study is dependent on the degree of systematic error. Validity is usually separated into two components:
Selection bias occurs when study subjects are selected or become part of the study as a result of a third, unmeasured variable which is associated with both the exposure and outcome of interest. [58] For instance, it has repeatedly been noted that cigarette smokers and non smokers tend to differ in their study participation rates. (Sackett D cites the example of Seltzer et al., in which 85% of non smokers and 67% of smokers returned mailed questionnaires.) [59] It is important to note that such a difference in response will not lead to bias if it is not also associated with a systematic difference in outcome between the two response groups.
Information bias is bias arising from systematic error in the assessment of a variable. [60] An example of this is recall bias. A typical example is again provided by Sackett in his discussion of a study examining the effect of specific exposures on fetal health: "in questioning mothers whose recent pregnancies had ended in fetal death or malformation (cases) and a matched group of mothers whose pregnancies ended normally (controls) it was found that 28% of the former, but only 20% of the latter, reported exposure to drugs which could not be substantiated either in earlier prospective interviews or in other health records". [59] In this example, recall bias probably occurred as a result of women who had had miscarriages having an apparent tendency to better recall and therefore report previous exposures.
Confounding has traditionally been defined as bias arising from the co-occurrence or mixing of effects of extraneous factors, referred to as confounders, with the main effect(s) of interest. [60] [61] A more recent definition of confounding invokes the notion of counterfactual effects. [61] According to this view, when one observes an outcome of interest, say Y=1 (as opposed to Y=0), in a given population A which is entirely exposed (i.e. exposure X = 1 for every unit of the population) the risk of this event will be R A1 . The counterfactual or unobserved risk R A0 corresponds to the risk which would have been observed if these same individuals had been unexposed (i.e. X = 0 for every unit of the population). The true effect of exposure therefore is: R A1 − R A0 (if one is interested in risk differences) or R A1 / R A0 (if one is interested in relative risk). Since the counterfactual risk R A0 is unobservable we approximate it using a second population B and we actually measure the following relations: R A1 − R B0 or R A1 / R B0 . In this situation, confounding occurs when R A0 ≠ R B0 . [61] (NB: Example assumes binary outcome and exposure variables.)
Some epidemiologists prefer to think of confounding separately from common categorizations of bias since, unlike selection and information bias, confounding stems from real causal effects. [58]
Few universities have offered epidemiology as a course of study at the undergraduate level. One notable undergraduate program exists at Johns Hopkins University , where students who major in public health can take graduate level courses, including epidemiology, during their senior year at the Bloomberg School of Public Health . [62]
Although epidemiologic research is conducted by individuals from diverse disciplines, including clinically trained professionals such as physicians, formal training is available through Masters or Doctoral programs including Master of Public Health (MPH), Master of Science of Epidemiology (MSc.), Doctor of Public Health (DrPH), Doctor of Pharmacy (PharmD), Doctor of Philosophy (PhD), Doctor of Science (ScD).  Many other graduate programs, e.g., Doctor of Social Work (DSW), Doctor of Clinical Practice (DClinP), Doctor of Podiatric Medicine (DPM), Doctor of Veterinary Medicine (DVM), Doctor of Nursing Practice (DNP), Doctor of Physical Therapy (DPT), or for clinically trained physicians, Doctor of Medicine (MD) or Bachelor of Medicine and Surgery (MBBS or MBChB) and Doctor of Osteopathic Medicine (DO), include some training in epidemiologic research or related topics, but this training is generally substantially less than offered in training programs focused on epidemiology or public health. Reflecting the strong historical tie between epidemiology and medicine, formal training programs may be set in either schools of public health and medical schools.
As public health/health protection practitioners, epidemiologists work in a number of different settings. Some epidemiologists work 'in the field'; i.e., in the community, commonly in a public health/health protection service, and are often at the forefront of investigating and combating disease outbreaks. Others work for non-profit organizations, universities, hospitals and larger government entities such as state and local health departments, various Ministries of Health, Doctors without Borders , the Centers for Disease Control and Prevention (CDC), the Health Protection Agency , the World Health Organization (WHO), or the Public Health Agency of Canada . Epidemiologists can also work in for-profit organizations such as pharmaceutical and medical device companies in groups such as market research or clinical development.
An April 2020 University of Southern California article noted that "The coronavirus epidemic ... thrust epidemiology – the study of the incidence, distribution and control of disease in a population – to the forefront of scientific disciplines across the globe and even made temporary celebrities out of some of its practitioners." [63]
On June 8, 2020, The New York Times published results of its survey of 511 epidemiologists asked "when they expect to resume 20 activities of daily life"; 52% of those surveyed expected to stop "routinely wearing a face covering" in one year or more. [64]
Accident-proneness is the idea that some people have a greater predisposition than others to suffer accidents , such as car crashes and industrial injuries . It may be used as a reason to deny any insurance on such individuals. [1]
The early work on this subject dates back to 1919, in a study by Greenwood and Woods, who studied workers at a British munitions factory and found that accidents were unevenly distributed among workers, with a relatively small proportion of workers accounting for most of the accidents. [2] Further work on accident-proneness was carried out in the 1930s and 1940s.
The subject is still being studied actively.  Research into accident-proneness is of great interest in safety engineering , where human factors such as pilot error, or errors by nuclear plant operators, can have massive effects on the reliability and safety of a system.
One of the areas of most interest and more profound research is the Aeronautical area, where accidents have been reviewed from psychological and human factors, to mechanical and technical failures. There has been many conclusive studies, that present that human factor has great influence on the results of those occurrences. [ citation needed ]
Statistical evidence clearly demonstrates that different individuals can have different rates of accidents from one another; for example, young male drivers are the group at highest risk for being involved in car accidents. There also seems to be substantial variation in personal accident rates between individuals. [ citation needed ]
However, a number of studies have cast doubt on whether accident-proneness actually exists as a distinct, persistent and independently verifiable physiological or psychological syndrome. Although substantial research has been devoted to this subject, there still seems to be no conclusive evidence either for or against the existence of accident proneness in this sense. [ citation needed ]
The exact nature and causes of accident-proneness, assuming that it exists as a distinct entity, are unknown.  Factors which have been considered as associated with accident-proneness have included absent-mindedness , clumsiness , carelessness , impulsivity , predisposition to risk -taking, and unconscious desires to create accidents as a way of achieving secondary gains.
Broad studies on the speed and accuracy using a specially designed test sheet of finding a specific figure on various people as Japanese, Brazil born Japanese, Chinese, Russian, Spanish, Filipinos, Thai and Central Americans with different educational backgrounds. The studies have revealed that educational background or study experience is the key factor of concentration capability. Screening new employees using this test gave drastic decrease of work accidents in several companies. [3]
In epidemiology and demography , age adjustment , also called age standardization , is a technique used to allow populations to be compared when the age profiles of the populations are quite different.
For example, in 2004/5, two Australian health surveys investigated rates of long-term circulatory system health problems (e.g. heart disease) in the general Australian population, and specifically in the Indigenous Australian population. In each age category over age 24, Indigenous Australians had markedly higher rates of circulatory disease than the general population: 5% vs 2% in age group 25–34, 12% vs 4% in age group 35–44, 22% vs 14% in age group 45–54, and 42% vs 33% in age group 55+. [1]
However, overall, these surveys estimated that 12% of all Indigenous Australians had long-term circulatory problems [1] compared to 18% of the overall Australian population. [2]
To understand this "apparent contradiction", we note that this only includes age groups over 24 and ignores those under. Indigenous figures are dominated by the younger age groups, which have lower rates of circulatory disease; this masks the fact that their risk at each age is higher than for non-Indigenous peers of the same age, if you simply pretend that no Indigenous people are under 24.
To get a more informative comparison between the two populations, a weighting approach is used. Older groups in the Indigenous population are weighted more heavily (to match their prevalence in the "reference population", i.e. the overall Australian population) and younger groups less heavily. This gives an "age-adjusted" morbidity rate approximately 30% higher than that for the general population, indicating that Indigenous Australians have a higher risk of circulatory disease. (Note that some residual distortion remains due to the wide age bands being used.) This is directly analogous to the standardized mortality ratio for mortality statistics.
To adjust for age under this direct method of standardization, age-specific rates in each group must be calculated, as well as the age structure of the standard population.
In order to adjust for age, a standard population must be selected. Some agencies which produce health statistics also publish standard populations for age adjustment.  Standard populations have been developed for specific countries [3] and regions. [4] World standard populations have also been developed to compare data from different countries, including the Segi World Standard and the World Health Organization (WHO) standard. [5] These agencies must balance between setting weights which may be used over a long period of time, which maximizes comparability of published statistics, and revising weights to be close to the current age distribution. When comparing data from a specific country or region, using a standard population from that country or region means that the age-adjusted rates are similar to the true population rates. [6] On the other hand, standardizing data using a widely used standard such as the WHO standard population allows for easier comparison with published statistics.
Age adjustment is commonly used when comparing prevalences in different populations. It is not used to derive life expectancy, which is calculated directly from the age-specific mortality rates, with no reference population required.
Age adjustment is also not appropriate when attempting to compare population totals (for instance, if we wanted to know the total number of hospital beds required for patients with circulatory diseases).
Stephanie Devaney (Chief Operating Officer)
The All of Us Research Program (previously known as the Precision Medicine Initiative Cohort Program [1] ) is a research program created in 2015 during the tenure of Barack Obama with $130 million [2] in funding that aims to make advances in tailoring medical care to the individual. [3] The mission of AoU is to accelerate health and medical breakthroughs, enabling individualized prevention, treatment and care.
The project aims to collect genetic and health data from one million volunteers. [4] The initiative was announced during the 2015 State of the Union Address , [5] and is run by the National Institutes of Health (NIH). The founding program director was Eric Dishman, who stepped down to become the Chief Innovation Officer. [1] In 2019, Joshua Denny was selected to be the second director. [6]
In October 2016, the project was renamed "All of Us". [7] [8] [9]
The research program was launched for national enrollment on May 6, 2018. [10]
The program is currently bilingual, with information and materials available in Spanish and English. Additional languages will be added in the future.
A set of All of Us core values [11] are guiding the development and implementation of the All of Us Research Program:
When you sign up for the All of Us Research Program, you are asked to take a survey that gathers information about your lifestyle and environment. Participants in All of Us can choose to give researchers access to their electronic health records (EHR), which includes information about their individual health problems and treatment they have received. They may also be asked to provide blood and urine samples and your physical measurements. [12]
Participants may also share data gathered by wearable devices (Fitbit) and in the future be asked to participate in clinical trials.
Program leaders plan to capture genomic data from select participants in 2019. [13]
Eligible adults (18 and over) can enroll with the program.  People who are not eligible are those in prison or people who cannot consent on their own [14] According to a sample consent form released in June 2018, participation in All of Us is voluntary and does not affect a participant's medical care. The form explains that if a participant quits the program, their samples will be destroyed.[16] Children may also be able to enroll in the program.[14]
By January 2018 an initial pilot project had enrolled about 10,000 people and 2022 was targeted for one million people. [15] As of May 2019, enrollment numbers at 1-year launch anniversary are 187,000+ participants, with more than 132,000 who have given biosamples. [16]
The NIH reported in May 2018 that they were pleased with high enrollment by underrepresented groups including communities of color and individuals with lower incomes. Up to three-quarters of beta phase participants came from those communities. [10] [17]
All of Us has more than 100 partners and champions working together to implement and support the mission and goals of the research program. [18] Google life sciences startup Verily Life Sciences , a Google "moonshot" with a goal of "transform[ing] the way we detect, prevent, and manage disease" [19] [20] is one partner.
The initiative was identified by a 2019 review as involving the public in every stage of the research. [21]
The All of Us Research Program budget has increase every year since it launched: FY2016 - $130 million; FY2017 - $230 million; and FY2018 - $290 million. [22] [23]
Professor Kenneth Weiss from Pennsylvania State University , in a skeptical review of this project in 2017, suggested that the funding could be better spent elsewhere. [24]
In the summer of 2019, one year after its official launch, All of Us had enrolled 230,000 participants, which represents almost one quarter of the program's goal of 1,000,000 individuals. Approximately 80% of those people are from groups that have been traditionally underrepresented in biomedical research. One of All of US's main goals is to include many people from diverse ancestries. [25] By June 2020, enrollment reached approximately 350,000 individuals. [26]
On May 27, 2020, the All of Us research program announced the launch of their research platform, the All of Us Researcher Workbench, for beta testing . Select data collected by the initiative, including electronic health records and survey responses from the first 225,000 program participants, will be available to approved researchers through the workbench. [27] Researchers may apply for access to the data if they have an NIH eRA Commons account (for identity verification) and are affiliated with an institution that has signed a data use agreement with All of Us. [28]
In June 2020, the NIH announced that research materials collected as part of the All of Us initiative will be used to address the COVID-19 pandemic . Blood samples collected from recent volunteers will be tested for SARS-CoV-2 antibodies in order to track prior infections within the US population. [29] Electronic health records shared by All of Us participants will also be evaluated for potential patterns associated with SARS-CoV-2 infection. All of Us also added monthly participant surveys with questions about the physical, mental, and socioeconomic impacts of the COVID-19 pandemic. [30]
Apparent infection rate is an estimate of the rate of progress of a disease, based on proportional measures of the extent of infection at different times.
Firstly, a proportional measure of the extent of infection is chosen as the disease extent metric. For example, the metric might be the proportion of leaf area affected by mildew, or the proportion of plants in a population showing dieback lesions. Measures of disease extent are then taken over time, and a mathematical model is fit. The model is based upon two assumptions:
There is a single model parameter r , which is the apparent infection rate. It can be calculated analytically using the formula
where

This article related to pathology is a stub . You can help Wikipedia by expanding it .
In survival analysis , the area compatibility factor , F , is used in indirect standardisation of population mortality rates .
where:
The expression can be thought of as the crude mortality rate for the standard population divided by what the crude mortality rate is for the region being studied, assuming the mortality rates are the same as for the standard population.
F is then multiplied by the crude mortality rate to arrive at the indirectly standardised mortality rate.

This statistics -related article is a stub . You can help Wikipedia by expanding it .
In the study of complex networks , assortative mixing , or assortativity , is a bias in favor of connections between network nodes with similar characteristics. [1] In the specific case of social networks , assortative mixing is also known as homophily .  The rarer disassortative mixing is a bias in favor of connections between dissimilar nodes.
In social networks, for example, individuals commonly choose to associate with others of similar age, nationality, location, race, income, educational level, religion, or language as themselves. [2] In networks of sexual contact , the same biases are observed, but mixing is also disassortative by gender – most partnerships are between individuals of opposite sex.
Assortative mixing can have effects, for example, on the spread of disease: if individuals have contact primarily with other members of the same population groups, then diseases will spread primarily within those groups.  Many diseases are indeed known to have differing prevalence in different population groups, although other social and behavioral factors affect disease prevalence as well, including variations in quality of health care and differing social norms.
Assortative mixing is also observed in other (non-social) types of networks, including biochemical networks in the cell, [3] computer and information networks, [4] systems engineering , [5] and others.
Of particular interest is the phenomenon of assortative mixing by degree , meaning the tendency of nodes with high degree to connect to others with high degree, and similarly for low degree.  Because degree is itself a topological property of networks, this type of assortative mixing gives rise to more complex structural effects than other types.  Empirically it has been observed that most social networks mix assortatively by degree, but most networks of other types mix disassortatively, [6] [7] although there are exceptions. [8] [9]

This applied mathematics -related article is a stub . You can help Wikipedia by expanding it .
This sociology -related article is a stub . You can help Wikipedia by expanding it .
As of 2011, circa 235 million people worldwide were affected by asthma , [2] and approximately 250,000 people die per year from the disease. [3] Low and middle income countries make up more than 80% of the mortality. [4] Rates vary between countries with prevalences between 1 and 18%. [3] It is more common in developed than developing countries . [3] One thus sees lower rates in Asia, Eastern Europe and Africa. [5] Within developed countries it is more common among those who are economically disadvantaged while in contrast in developing countries it is more common amongst the affluent. [3] [6] The reason for these differences is not well known. [3]
While asthma is twice as common in boys as girls, [3] severe asthma occurs at equal rates. [7] Among adults, however, asthma is twice as common in women as men. [7] [8]
Rates of asthma have increased significantly between the 1960s and 2008 [9] [10] with it being recognized as a major public health problem since the 1970s. [5] Some 9% of US children had asthma in 2001, compared with just 3.6% in 1980. The World Health Organization (WHO) reports that some 10% of the Swiss population have asthma as of 2007, compared with 2% some 25–30 years ago. [11] In the United States the age-adjusted prevalence of asthma increased from 7.3 to 8.2 percent during the years 2001 through 2009. [12]
Asthma affects approximately 7% of the population of the United States and causes approximately 4,210 deaths per year. [13] [14] [15] In 2005, asthma affected more than 22 million people, including 6 million children, and accounted for nearly 500,000 hospitalizations that same year. [16] In 2010, asthma accounted for more than one-quarter of admitted emergency department visits in the U.S. among children aged 1–9 years, and it was a frequent diagnosis among children aged 10–17 years. [17] From 2000 through 2010, the rate of pediatric hospital stays for asthma declined from 165 to 130 per 100,000 population, respectively, whereas the rate for adults remained about 119 per 100,000 population. [18]
Asthma prevalence in the U.S. is higher than in most other countries in the world, but varies drastically between ethnic populations. [19] Asthma prevalence is highest in Puerto Ricans, African Americans, Filipinos, Irish Americans, and Native Hawaiians, and lowest in Mexicans and Koreans. [20] [21] [22] Rates of asthma-related hospital admissions in 2010 were more than three times higher among African American children and two times higher for African American adults compared with White and Asian and Pacific Islander people. [18] Also, children who are born in low-income families have higher risk of asthma. [23]
Asthma prevalence also differs between populations of the same ethnicity who are born and live in different places. [24] U.S.-born Mexican populations, for example, have higher asthma rates than non-U.S. born Mexican populations that are living in the U.S. [25]
Asthma affects approximately 5% of the United Kingdom’s population. [26] In England, an estimated 261,400 people were newly diagnosed with asthma in 2005; 5.7 million people had an asthma diagnosis and were prescribed 32.6 million asthma-related prescriptions. [27]
Data depicts an increasing trend in asthma prevalence among Canada’s population. In 2000-2001 asthma prevalence was monitored at 6.5%; by 2010-2011 a 4.3% increase was shown, with asthma prevalence totaling 10.8% of Canada's population. [28]
Furthermore, asthma prevalence varies among the provinces of Canada; the highest prevalence is Ontario at 12.1%, and the lowest is Nunavut at 3.8%. [28] Though there is an overall decrease in the incidence of new asthma cases in Canada, prevalence is rising. This can be attributed to a decrease in case-specific mortality due to improved management and control of asthma and its symptoms.
It is approximated that 40 million Latin Americans live with asthma. [29]
In some reports, urban residency within Latin America has been found to be associated with an increased prevalence of asthma. [29] Childhood asthma prevalence was found to be higher than 15 percent in a majority of Latin American countries. [30] Similarly, a study published relating to asthma prevalence in Havana, Cuba estimated that approximately 9 percent of children under the age of 15 are undiagnosed for asthma, possible due to lack of resources in the region. [29]
The prevalence of asthma in adults in Japan is rapidly increasing, however there is a significant difference for the children in Japan. The mean prevalence of asthma in Japan has increased from about 1% to 10% or higher in children and to about 6–10% in adults since the 1960s. [31] There has been a 1.5 fold increase in the prevalence of asthma per decade in Japan from the 1960s. [31] Three surveys done from 1985, 1999 and 2006 show adult asthma is increasing, while the same surveys indicate that the prevalence of asthma in children is decreasing. [32] To compare this to another Asia-Pacific country the estimated prevalence of asthma in male and female children in Mongolia in a 2009 ISSAC study was 20.9% and 21.0% [33]
Data regarding the epidemiology of asthma in the continent of Asia as whole is scarce, particularly regarding adult populations. However, similarly to much of the rest of the globe, prevalence of childhood asthma appears to be rising. Systematic childhood studies, such as the International Study of Asthma and Allergies in Childhood (ISAAC), provide data regarding the epidemiology of asthma among Asia's youth population. Asthma prevalence among Asia’s adult population is less clear in comparison due to the comparatively higher monitoring of younger populations. However, the data available points to a positive correlation between age and asthma prevalence. Findings indicate that the prevalence of asthma among the Asian adult population is less than 5%; while findings pertaining to elderly populations illustrate a rate somewhere between 1.3-15.3%. [34]
In a review of studies on the prevalence of asthma among migrant populations, those born in high-income countries were found to have higher rates of asthma than migrants. Second-generation migrants had a higher risk of asthma than first-generation migrants, and the prevalence of asthma increases with longer time of residence in the host country. [35] This confirms the role of the environment in the development of asthma.
A survey conducted by the ISSAC Steering Committee conducted a study from 1992 to 1993 in adults aged 22 to 44 comparing the prevalence of asthma in 10 developed countries. An important note to consider is the population differences between these countries. The United States population in 1992 was 256.9 million, 14.5 times that of Australia (17.5 mil), and 4.5 times of the United Kingdom (57.51 mil). [36] [37] [38] However, Australia and the UK have a higher prevalence than the US by 2.4 times on the lower end and 4.6 times on the higher end. In another study taken in 1992 for Japan the prevalence of asthma in Japan was 13% [39] with a population of 124.2 million. [40]
20–44
30.3
Prevalence of asthma (4th column) in 11 different countries
(1st column) between the years 1992 and 1994 (2nd column)
in the ages 20–44 (3rd column) including Japan in 2005
The Atherosclerosis Risk in Communities ( ARIC ) Study is a multi-site, prospective, biracial cohort study funded by the National Heart, Lung, and Blood Institute of the National Institutes of Health . [1] [2] [3] The ARIC Study was designed to investigate the etiology and clinical outcomes of atherosclerosis . A total of 15,792 middle-aged (45–64 years) men and women were enrolled from four U.S. communities:
Participating organizations include four field centers ( Wake Forest Baptist Medical Center , University of Mississippi Medical Center , University of Minnesota , Johns Hopkins University ), data coordinating center ( University of North Carolina at Chapel Hill ), and many collaborating centers and laboratories.
The initial study visit took place in 1987-1989, the second in 1990-1992, the third in 1993-1995, the fourth in 1996-1998, and the fifth in 2011-2013. During these extensive examinations, information is obtained on demographics, medical history, medication use, and health behaviors.
To date, the ARIC study has published over 1,000 peer-reviewed journal articles in diverse areas of clinical and population research. Data from the ARIC study have become an important resource for the study of heart disease, kidney disease , diabetes , and cognitive decline. These data have also contributed to clinical practice guidelines and policy statements. ARIC is registered at ClinicalTrials.gov registry under trial identifier NCT00005131.
In epidemiology , the attack rate is the percentage of an at-risk population that contracts the disease during a specified time interval. [1] It is used in hypothetical predictions and during actual outbreaks of disease. An at-risk population is defined as one that has no immunity to the attacking pathogen which can be either a novel pathogen or an established pathogen. It is used to project the number of infections to expect during an epidemic . This aids in marshalling resources for delivery of medical care as well as production of vaccines and/or anti-viral and anti-bacterial medicines. [2] The rate is arrived at by taking the number of new cases in the population at risk and dividing by the number of persons at risk in the population.
Attention-Deficit/Hyperactivity Disorder (ADHD) is estimated to affect about 6 to 7 percent of people aged 18 and under when diagnosed via the DSM-IV criteria. [1] Hyperkinetic disorder when diagnosed via the ICD-10 criteria give rates of between 1 and 2 percent in this age group. [2]
Children in North America appear to have a higher rate of ADHD than children in Africa and the Middle East - however, this may be due to differing methods of diagnosis used in different areas of the world. [3] If the same diagnostic methods are used rates are more or less the same between countries. [4]
In 2020, a meta-analysis of studies found that 7.47% of children and adolescents across Africa have ADHD. [5] ADHD was found more often in boys, at a rate of 2:1. [5] The most common form of ADHD was inattentive (2.95% of total population), followed by hyperactive/impulsive (2.77%), then combined (2.44%). [5] While differences in prevalence rate were found internationally, it is not clear whether this reflects true differences or changes in methodology. [5]
A 2008 evaluation of the “KiGGS” survey, monitoring 14,836 girls and boys (age between 3 and 17 years), showed that 4.8% of the participants had an ADHD diagnosis. While 7.9% of all boys had ADHD, only 1.8% girls had it, too. Another 4.9% of the participants (6.4% boys : 3.6% girls) were suspected ADHD cases, because they showed a rate ≥7 on the Strengths and Difficulties Questionnaire (SDQ) scale. The number of ADHD diagnoses was 1.5% (2.4% : 0.6%) among preschool children (3–6 years old), 5,3 % (8.7% : 1.9%) at age 7–10 years, and had its peak at 7.1% (11.3% : 3.0%) in the age group of 11–13 years. Among 14 to 17 years old adolescents the rate was 5.6% (9.4% : 1.8%). [6]
Rates in Spain are estimated at 6.8% among people under 18. [7]
In some parts of England, there were waiting lists of five years or more for ADHD adult diagnostic assessment in 2019. [8]
In the United States it is diagnosed in 2-16 percent of school children. [10] The rates of diagnosis and treatment of ADHD are much higher on the east coast of the United States than on its west coast. [11] The frequency of the diagnosis differs between male children (10%) and female children (4%) in the United States. [12] This difference between genders may reflect either a difference in susceptibility or that females with ADHD are less likely to be diagnosed than males. [13] Boys outnumber girls across all three subtyping categories, but the exact magnitude of these differences seems to depend on both the informant (parent, teacher, etc.) and the subtype . In two community-based investigations, conducted by DuPaul and associates, boys outnumbered girls by only 2.2:1 in parent-generated samples and 2.3:1 in teacher-based input. [14]
Rates of ADHD diagnosis and treatment have increased in both the United Kingdom and the United States since the 1970s. This is believed to be primarily due to changes in how the condition is diagnosed [15] and how readily people are willing to treat it with medications rather than a true change in the frequency. [2] In the UK an estimated 0.5 per 1,000 children had ADHD in the 1970s, while 3 per 1,000 received ADHD medications in the late 1990s. In the UK in 2003, 3.6 percent of male children and less than 1 percent in female children had the diagnosis. [16] : 134 In the United States the number of children with the diagnosis increase from 12 per 1000 in the 1970s to 34 per 1000 in the late 1990s, [16] to 95 per 1,000 in 2007, [17] and 110 per 1,000 in 2011. [18] It is believed that the changes to the diagnostic criteria in 2013 from the DSM 4TR to the DSM 5 will increase the number of people with ADHD especially among adults. [19]
In epidemiology , attributable fraction among the exposed (AF e ) is  the proportion of incidents in the exposed group that are attributable to the risk factor. Term attributable risk percent among exposed is used if the fraction is expressed as a percentage. [1] It is calculated as A F e = ( I e − I u ) / I e = ( R R − 1 ) / R R {\displaystyle AF_{e}=(I_{e}-I_{u})/I_{e}=(RR-1)/RR} , where I e {\displaystyle I_{e}} is the incidence in the exposed group, I u {\displaystyle I_{u}} is the incidence in the unexposed group, and R R {\displaystyle RR} is the relative risk . [2]
It is used when an exposure increases the risk, as opposed to reducing it, in which case its symmetrical notion is preventable fraction among the unexposed .
Multiple synonyms of AF e are in use: attributable fraction, [1] [3] relative attributable risk, [1] attributable proportion among the exposed, [1] attributable risk among the exposed. [4]
Similarly, attributable risk percent (ARP) is used as a synonym for the attributable risk percent among the exposed. [3]
In climatology , fraction of attributable risk (FAR) is used to denote a proportion of adverse event risk attributable to the human influence on climate or other forcing factor. [5]
In epidemiology , attributable fraction for the population (AFp) is the proportion of incidents in the population that are attributable to the risk factor. Term attributable risk percent for the population is used if the fraction is expressed as a percentage. [1] It is calculated as A F p = ( I p − I u ) / I p {\displaystyle AF_{p}=(I_{p}-I_{u})/I_{p}} , where I p {\displaystyle I_{p}} is the incidence in the population, and I u {\displaystyle I_{u}} is the incidence in the unexposed group. [1]
Equivalently it can be calculated as A F p = P e ( R R − 1 ) 1 + P e ( R R − 1 ) {\displaystyle AF_{p}={\frac {P_{e}(RR-1)}{1+P_{e}(RR-1)}}} , where P e {\displaystyle P_{e}} is the exposed proportion of the population and R R {\displaystyle RR} is the relative risk not adjusted for confounders . [1] [2]
It is used when an exposure increases the risk, as opposed to reducing it, in which case its symmetrical notion is preventable fraction for the population .
Multiple synonyms of the attributable fraction for the population are in use: attributable proportion for the population, [1] population attributable proportion, [1] Levin's attributable risk, [1] population attributable risk [2] and population attributable fraction. [3] [4]
Similarly, population attributable risk percent (PAR) is used as a synonym for the attributable risk percent for the population. [1] [2] [5]
Attributable fraction for the population combines both the relative risk of an incident with respect to the factor, as well as the prevalence of the factor in the population. Values of AF p close to 1 indicate that both the relative risk is high, and that the risk factor is prevalent. In such case, removal of the risk factor will greatly reduce the number of the incidents in the population. The values of AF p close to 0, on the other hand, indicate that either the relative risk is low, or that the factor is not prevalent (or both). Removal of such factor from the population will have little effect. Because of this interpretation, the AFp is well suited for the public policy making. [6]
For example, in 1953 Levin's paper it was estimated that lung cancer has relative risk of 3.6 - 13.4 in smokers compared to non-smokers, and that the proportion of the population exposed to smoking was 0.5 - 0.96, resulting in the high value of attributable fraction for the population 0.56 - 0.92. [6]
Attributable fraction for the population can be generalized to the case where the multilevel exposure to the risk factor. In such case
A F p = ∑ i P i R R i − ∑ i P i ′ R R i ∑ i P i R R i {\displaystyle AF_{p}={\frac {\sum _{i}P_{i}RR_{i}-\sum _{i}P_{i}'RR_{i}}{\sum _{i}P_{i}RR_{i}}}} where P i {\displaystyle P_{i}} is the proportion of the population exposed to the level i {\displaystyle i} , P i ′ {\displaystyle P_{i}'} is the desired (ideal) proportion of the population exposed to the level i {\displaystyle i} , and R R i {\displaystyle RR_{i}} is the relative risk at exposure level i {\displaystyle i} . [4]
In epidemiology , attributable risk or excess risk is a term synonymous to risk difference , that has also been used to denote attributable fraction among the exposed and attributable fraction for the population . [1]
The epidemiology of autism is the study of the incidence and distribution of autism spectrum disorders (ASD). A 2012 review of global prevalence estimates of autism spectrum disorders found a median of 62 cases per 10,000 people. [1] However there is a lack of evidence from low- and middle-income countries . [1]
ASD averages a 4.3:1 male-to-female ratio in diagnosis. The number of children known to have autism has increased dramatically since the 1980s, at least partly due to changes in diagnostic practice; it is unclear whether prevalence has actually increased; [2] and as-yet-unidentified environmental risk factors cannot be ruled out. [3] The Centers for Disease Control ’s Autism and Developmental Disabilities Monitoring (ADDM) Network reports that in 2014, approximately 1 in 59 children in the United States (1 in 37 boys, and 1 in 151 girls), has been identified with an autism spectrum disorder (ASD). [4] This estimate is a 15% increase from the 1 in 68 rate in 2010, 86% increase from the 1 in 110 rate in 2006 and 154% increase from the 1 in 150 rate in 2000. [4] Diagnostic criteria of ASD has changed significantly since the 1980s; for example, U.S. special-education autism classification was introduced in 1994. [2]
Autism is a complex neurodevelopmental disorder . Many causes have been proposed, but its theory of causation is still questionable and ultimately unknown. [2] [5] The possibility  of autism is associated with several prenatal factors, including advanced paternal age and diabetes in the mother during pregnancy . [6] ASD is associated with several intellectual or emotional gifts. Some individuals perceive it to be connected to genetic disorders [7] and with epilepsy . [8] Autism is believed to be largely inherited , although the genetics of autism are complex, and it is unclear which genes are responsible. [9] Little evidence exists to support associations with specific environmental exposures. [2]
In rare cases, autism is strongly associated with agents that cause birth defects . [10] Other proposed causes , such as childhood vaccines , are controversial . The vaccine hypothesis has been extensively investigated and shown to be false, [11] lacking any scientific evidence . [3] Andrew Wakefield published a small study in 1998 in the United Kingdom suggesting a causal link between autism and the trivalent MMR vaccine . After data included in the report was shown to be deliberately falsified, the paper was retracted, and Wakefield was struck off the medical register in the United Kingdom. [12] [13] [14]
It is problematic to compare autism rates over the last three decades, as the diagnostic criteria for autism have changed with each revision of the Diagnostic and Statistical Manual (DSM), which outlines which symptoms meet the criteria for an ASD diagnosis. In 1983, the DSM did not recognize PDD-NOS or Asperger’s syndrome , and the criteria for autistic disorder (AD) were more restrictive. The previous edition of the DSM, DSM-IV, included autistic disorder, childhood disintegrative disorder , PDD-NOS, and Asperger’s syndrome. Due to inconsistencies in diagnosis and how much is still being learnt about autism, the most recent DSM ( DSM-5 ) only has one diagnosis, autism spectrum disorder (ASD), which encompasses each of the previous four disorders. According to the new diagnostic criteria for ASD, one must have both struggles  in social communication and interaction and restricted repetitive behaviors, interests and activities (RRBs).
ASD diagnoses continue to be over four times more common among boys (1 in 37) than among girls (1 in 151), and they are reported in all racial, ethnic and socioeconomic groups. Studies have been conducted in several continents (Asia, Europe and North America) that report a prevalence rate of approximately 1 to 2 percent. [15] A 2011 study reported a 2.6 percent prevalence of autism in South Korea . [16]
Although incidence rates measure autism prevalence directly, most epidemiological studies report other frequency measures, typically point or period prevalence, or sometimes cumulative incidence. Attention is focused mostly on whether prevalence is increasing with time. [2]
Epidemiology defines several measures of the frequency of occurrence of a disease or condition: [17]
When studying how conditions are caused, incidence rates are the most appropriate measure of condition frequency as they assess probability directly. However, incidence can be difficult to measure with rarer conditions such as autism. [17] In autism epidemiology, point or period prevalence is more useful than incidence, as the condition starts long before it is diagnosed, bearing in mind genetic elements it is inherent from conception, and the gap between initiation and diagnosis is influenced by many factors unrelated to chance. Research focuses mostly on whether point or period prevalence is increasing with time; cumulative incidence is sometimes used in studies of birth cohorts . [2]
The three basic approaches used to estimate prevalence differ in cost and in quality of results. The simplest and cheapest method is to count known autism cases from sources such as schools and clinics, and divide by the population. This approach is likely to underestimate prevalence because it does not count children who have not been diagnosed yet, and it is likely to generate skewed statistics because some children have better access to treatment. [18]
The second method improves on the first by having investigators examine student or patient records looking for probable cases, to catch cases that have not been identified yet. The third method, which is arguably the best, screens a large sample of an entire community to identify possible cases, and then evaluates each possible case in more detail with standard diagnostic procedures. This last method typically produces the most reliable, and the highest, prevalence estimates. [18]
Estimates of the prevalence of autism vary widely depending on diagnostic criteria, age of children screened, and geographical location. [19] Most recent reviews tend to estimate a prevalence of 1–2 per 1,000 for autism and close to 6 per 1,000 for ASD; [2] PDD-NOS is the vast majority of ASD, Asperger syndrome is about 0.3 per 1,000 and the atypical forms childhood disintegrative disorder and Rett syndrome are much rarer. [20]
A 2006 study of nearly 57,000 British nine- and ten-year-olds reported a prevalence of 3.89 per 1,000 for autism and 11.61 per 1,000 for ASD; these higher figures could be associated with broadening diagnostic criteria. [21] Studies based on more detailed information, such as direct observation rather than examination of medical records, identify higher prevalence; this suggests that published figures may underestimate ASD's true prevalence. [22] A 2009 study of the children in Cambridgeshire , England used different methods to measure prevalence, and estimated that 40% of ASD cases go undiagnosed, with the two least-biased estimates of true prevalence being 11.3 and 15.7 per 1,000. [23]
A 2009 U.S. study based on 2006 data estimated the prevalence of ASD in eight-year-old children to be 9.0 per 1,000 (approximate range 8.6–9.3). [24] A 2009 report based on the 2007 Adult Psychiatric Morbidity Survey by the National Health Service determined that the prevalence of ASD in adults was approximately 1% of the population, with a higher prevalence in males and no significant variation between age groups; [25] these results suggest that prevalence of ASD among adults is similar to that in children and rates of autism are not increasing. [26]
Attention has been focused on whether the prevalence of autism is increasing with time. Earlier prevalence estimates were lower, centering at about 0.5 per 1,000 for autism during the 1960s and 1970s and about 1 per 1,000 in the 1980s, as opposed to today's 15–17 per 1000. [2] [4]
The number of reported cases of autism increased dramatically in the 1990s and early 2000s, prompting investigations into several potential reasons: [28]
The reported increase is largely attributable to changes in diagnostic practices, referral patterns, availability of services, age at diagnosis, and public awareness. [2] [3] [27] A widely cited 2002 pilot study concluded that the observed increase in autism in California cannot be explained by changes in diagnostic criteria, [31] but a 2006 analysis found that special education data poorly measured prevalence because so many cases were undiagnosed, and that the 1994–2003 U.S. increase was associated with declines in other diagnostic categories, indicating that diagnostic substitution had occurred. [32]
A 2007 study that modeled autism incidence found that broadened diagnostic criteria, diagnosis at a younger age, and improved efficiency of case ascertainment, can produce an increase in the frequency of autism ranging up to 29-fold depending on the frequency measure, suggesting that methodological factors may explain the observed increases in autism over time. [33] A small 2008 study found that a significant number (40%) of people diagnosed with pragmatic language impairment as children in previous decades would now be given a diagnosis as autism. [34] A study of all Danish children born in 1994–99 found that children born later were more likely to be diagnosed at a younger age, supporting the argument that apparent increases in autism prevalence were at least partly due to decreases in the age of diagnosis. [35]
A 2009 study of California data found that the reported incidence of autism rose 7- to 8-fold from the early 1990s to 2007, and that changes in diagnostic criteria, inclusion of milder cases, and earlier age of diagnosis probably explain only a 4.25-fold increase; the study did not quantify the effects of wider awareness of autism, increased funding, and expanding support options resulting in parents' greater motivation to seek services. [36] Another 2009 California study found that the reported increases are unlikely to be explained by changes in how qualifying condition codes for autism were recorded. [37]
Several environmental factors have been proposed to support the hypothesis that the actual frequency of autism has increased. These include certain foods, infectious disease, pesticides . There is overwhelming scientific evidence against the MMR hypothesis and no convincing evidence for the thiomersal (or Thimerosal) hypothesis, so these types of risk factors have to be ruled out. [3] Although it is unknown whether autism's frequency has increased, any such increase would suggest directing more attention and funding toward addressing  environmental factors instead of continuing to focus on genetics. [38]
The prevalence of autism in Africa is unknown. [39]
The prevalence of autism in the Americas overall is unknown.
The rate of autism diagnoses in Canada was 1 in 450 in 2003. However, preliminary results of an epidemiological study conducted at Montreal Children's Hospital in the 200–2004 school year found a prevalence rate of 0.68% (or 1 per 147). [40]
A 2001 review of the medical research conducted by the Public Health Agency of Canada concluded that there was no link between MMR vaccine and either inflammatory bowel disease or autism. [41] The review noted, "An increase in cases of autism was noted by year of birth from 1979 to 1992; however, no incremental increase in cases was observed after the introduction of MMR vaccination." [41] After the introduction of MMR, "A time trend analysis found no correlation between prevalence of MMR vaccination and the incidence of autism in each birth cohort from 1988 to 1993." [41]
CDC's most recent estimate is that 1 out of every 59 children, or 16.8 per 1,000, have some form of ASD as of 2014. [42] The number of diagnosed cases of autism grew dramatically in the U.S. in the 1990s and early 2000s. For the 2006 surveillance year, identified ASD cases were an estimated 9.0 per 1000 children aged 8 years (95% confidence interval [CI] = 8.6–9.3). [24] These numbers measure what is sometimes called "administrative prevalence", that is, the number of known cases per unit of population, as opposed to the true number of cases. [32] This prevalence estimate rose 57% (95% CI 27%–95%) from 2002 to 2006. [24]
The National Health Interview Survey (NHIS) for 2014–2016 studied 30,502 US children and adolescents and found the weighted prevalence of ASD was 2.47% (24.7 per 1,000); 3.63% in boys and 1.25% in girls. Across the 3-year reporting period, the prevalence was 2.24% in 2014, 2.41% in 2015, and 2.76% in 2016. [43]
The number of new cases of autism spectrum disorder (ASD) in Caucasian boys is roughly 50% higher than found in Hispanic children, and approximately 30% more likely to occur than in Non-Hispanic white children in the United States. [2] [44]
A further study in 2006 concluded that the apparent rise in administrative prevalence was the result of diagnostic substitution, mostly for findings of mental retardation and learning disabilities. [32] "Many of the children now being counted in the autism category would probably have been counted in the mental retardation or learning disabilities categories if they were being labeled 10 years ago instead of today," said researcher Paul Shattuck of the Waisman Center at the University of Wisconsin–Madison , in a statement. [45]
A population-based study in Olmsted County , Minnesota county found that the cumulative incidence of autism grew eightfold from the 1980–83 period to the 1995–97 period. The increase occurred after the introduction of broader, more-precise diagnostic criteria, increased service availability, and increased awareness of autism. [46] During the same period, the reported number of autism cases grew 22-fold in the same location, suggesting that counts reported by clinics or schools provide misleading estimates of the true incidence of autism. [47]
A 2008 study in Venezuela reported a prevalence of 1.1 per 1,000 for autism and 1.7 per 1,000 for ASD. [48]
A journal reports that the median prevalence of ASD among 2–6-year-old children who are reported in China from 2000 upwards was 10.3/10,000. [49]
A 2008 Hong Kong study reported an ASD incidence rate similar to those reported in Australia and North America, and lower than Europeans. It also reported a prevalence of 1.68 per 1,000 for children under 15 years. [50]
A 2005 study of a part of Yokohama with a stable population of about 300,000 reported a cumulative incidence to age 7 years of 48 cases of ASD per 10,000 children in 1989, and 86 in 1990. After the vaccination rate of the triple MMR vaccine dropped to near zero and was replaced with MR and M vaccine, the incidence rate grew to 97 and 161 cases per 10,000 children born in 1993 and 1994, respectively, indicating that the combined MMR vaccine did not cause autism. [51] A 2004 Japanese autism association reported that about 360.000 people have typical Kanner-type autism .
A 2009 study reported that the annual incidence rate of Israeli children with a diagnosis of ASD receiving disability benefits rose from zero in 1982–1984 to 190 per million in 2004. It was not known whether these figures reflected true increases or other factors such as changes in diagnostic measures. [52]
Studies of autism frequency have been particularly rare in the Middle East . One rough estimate is that the prevalence of autism in Saudi Arabia is 18 per 10,000, slightly higher than the 13 per 10,000 reported in developed countries. [53] (compared to 168 per 10,000 in the USA)
In 1992, thiomersal -containing vaccines were removed in Denmark. A study at Aarhus University indicated that during the chemical's usage period (up through 1990), there was no trend toward an increase in the incidence of autism. Between 1991 and 2000 the incidence increased, including among children born after the discontinuation of thimerosal. [54]
France made autism the national focus for the year 2012 and the Health Ministry now evaluates the rate of autism to be 67 per 10,000 (1 out of 150). [55]
Eric Fombonne made some studies in the years 1992 and 1997. He found a prevalence of 16 per 10,000 for the global pervasive developmental disorder (PDD). [56] [57] The INSERM found a prevalence of 27 per 10,000 for the ASD and a prevalence of 9 per 10,000 for the early infantile autism in 2003. [58] Those figures are considered as underrated as the WHO gives figures between 30 and 60 per 10,000. [59] The French Minister of Health gives a prevalence of 4.9 per 10,000 on its website but it counts only early infantile autism . [60]
A 2008 study in Germany found that inpatient admission rates for children with ASD increased 30% from 2000 to 2005, with the largest rise between 2000 and 2001 and a decline between 2001 and 2003. Inpatient rates for all mental disorders also rose for ages up to 15 years, so that the ratio of ASD to all admissions rose from 1.3% to 1.4%. [61]
A 2009 study in Norway reported prevalence rates for ASD ranging from 0.21% to 0.87%, depending on assessment method and assumptions about non-response, suggesting that methodological factors explain large variances in prevalence rates in different studies. [62]
The incidence and changes in incidence with time are unclear in the United Kingdom . [63] The reported autism incidence in the UK rose starting before the first introduction of the MMR vaccine in 1989. [64] However, a perceived link between the two arising from the results of a fraudulent scientific study has caused considerable controversy, despite being subsequently disproved. [65] A 2004 study found that the reported incidence of pervasive developmental disorders in a general practice research database in England and Wales grew steadily during 1988–2001 from 0.11 to 2.98 per 10,000 person-years, and concluded that much of this increase may be due to changes in diagnostic practice. [66]
As late as the mid-1970s there was little evidence of a genetic role in autism; evidence from genetic epidemiology studies now suggests that it is one of the most heritable of all psychiatric conditions. [67] The first studies of twins estimated heritability to be more than 90%; in other words, that genetics explains more than 90% of autism cases. [9] When only one identical twin is autistic, the other often has learning or social disabilities. For adult siblings, the risk of having one or more features of the broader autism phenotype might be as high as 30%, [68] much higher than the risk in controls. [69] About 10–15% of autism cases have an identifiable Mendelian (single-gene) condition, chromosome abnormality , or other genetic syndrome, [68] and ASD is associated with several genetic disorders . [7]
Since heritability is less than 100% and symptoms vary markedly among identical twins with autism, environmental factors are most likely a significant cause as well. If some of the risk is due to gene-environment interaction the 90% heritability estimate may be too high; [2] new twin data and models with structural genetic variation are needed. [70]
Genetic linkage analysis has been inconclusive; many association analyses have had inadequate power. [70] Studies have examined more than 100 candidate genes; many genes must be examined because more than a third of genes are expressed in the brain and there are few clues on which are relevant to autism. [2]
Several studies found a strong association between the use of acetaminophen (e.g., Tylenol, Paracetamol) and autism [71] [72] Autism is also associated with several prenatal factors, including advanced age in either parent, diabetes, bleeding and use of psychiatric drugs in the mother during pregnancy. [6] Autism was found to be indirectly linked to prepregnancy obesity and low weight mothers. [73] It is not known whether mutations that arise spontaneously in autism and other neuropsychiatric disorders come mainly from the mother or the father, or whether the mutations are associated with parental age. [74] However, recent studies have identified advancing paternal age as a significant indicator for ASD. [75] Increased chance  of autism has also been linked to rapid "catch-up" growth for children born to mothers who had unhealthy weight at conception. [73]
A large 2008 population study of Swedish parents of children with autism found that the parents were more likely to have been hospitalized for a mental disorder, that schizophrenia was more common among the mothers and fathers, and that depression and personality disorders were more common among the mothers. [76]
It is not known how many siblings of autistic individuals are themselves autistic. Several studies based on clinical samples have given quite different estimates, and these clinical samples differ in important ways from samples taken from the general community. [77]
Autism has also been shown to cluster in urban neighborhoods of high socioeconomic status. One study from California found a three to fourfold increased risk of autism in a small 30 by 40 km region centered on West Hollywood , Los Angeles . [78]
Boys have a higher chance of being diagnosed with autism than girls. The ASD sex ratio averages 4.3:1 and is greatly modified by cognitive impairment: it may be close to 2:1 with mental retardation and more than 5.5:1 without. Recent studies have found no association with socioeconomic status , and have reported inconsistent results about associations with race or ethnicity . [2]
RORA deficiency may explain some of the difference in frequency between males and females. RORA protein levels are higher in the brains of typically developing females compared to typically developing males, providing females with a buffer against RORA deficiency. This is known as the Female protective effect . RORA deficiency has previously been proposed as one factor that may make males more vulnerable to autism. [79]
Autism is associated with several other conditions:

Badger culling in the United Kingdom is permitted under licence, within a set area and timescale, as a way to reduce badger numbers in the hope of controlling the spread of bovine tuberculosis (bTB). [1]
Humans can catch bTB, but public health control measures, including milk pasteurisation and the BCG vaccine , mean it is not a significant risk to human health. [2] The disease affects cattle and other farm animals (including pigs, goats, deer, sheep, alpacas and llamas), some species of wildlife including badgers and deer, and some domestic pets such as cats. Geographically, bTB has spread from isolated pockets in the late 1980s to cover large areas of the west and south-west of England and Wales in the 2010s. Some people believe this correlates with the lack of badger control. [3]
In October 2013, culling in England was controversially trialled in two pilot areas in west Gloucestershire and west Somerset . The main aim of these trials was to assess the humaneness of culling using "free shooting" (previous methods trapped the badgers in cages before shooting them). The trials were repeated in 2014 and 2015, and expanded to a larger area in 2016 and 2017.  In 2019, there was no UK-wide policy of badger culling; however in 2020 it was proposed to extend the areas in which culls took place to include parts of Oxfordshire, Lincolnshire, Leicestershire and Derbyshire, with a view to killing some 60,000 animals. [4]
European badgers ( Meles meles ) are not an endangered species, but they are among the most legally-protected wild animals in the UK, being shielded under the Protection of Badgers Act 1992 , the Wildlife and Countryside Act 1981 , and the Convention on the Conservation of European Wildlife and Natural Habitats . [5]
Prior to the 2012/13 badger cull, the government's Department for Environment, Food and Rural Affairs (Defra) stated that badger control was needed because "...we still need to tackle TB in order to support high standards of animal health and welfare, to promote sustainable beef and dairy sectors, to meet EU legal and trade requirements and to reduce the cost and burden on farmers and taxpayers." [6] This report listed the following reasons for bTB control:-
Humans can become infected by the Mycobacterium bovis bacterium, which causes the disease "bovine TB" (bTB). Between 1994 and 2011, there were 570 human cases of bovine TB in humans. Most of these cases are thought to be in people aged 45 or over, who could have been infected before milk pasteurisation became common in the UK. [7]
One route of transmission to humans is drinking infected, non-pasteurised milk ( pasteurisation kills the bacterium).  European badgers can become infected with bTB and transmit the disease to cattle, thereby posing a risk to the human food chain. Culling is a method used in parts of the UK to reduce the number of badgers and thereby reduce the incidence and spread of bTB that might infect humans.
Once an animal has contracted bTB, the disease can be spread through the sett via the exhalations or excretions of infected individuals. Modern cattle housing, which has good ventilation, makes this process relatively less effective, but in older-style cattle housing or in badger setts, the disease can spread more rapidly. Badgers range widely at night, potentially spreading bTB over long distances.  Badgers mark their territory with urine , which can contain a high proportion of bTB bacteria. According to the RSPCA, the infection rate among badgers is 4–6%. [8] [9]
In 2014, bTB was mostly concentrated in the south-west of England. It is thought to have re-emerged because of the 2001 foot and mouth disease outbreak which led to thousands of cattle being slaughtered and farmers all over the UK having to buy new stock. There appears to have been undiscovered bTB in some of these replacement animals. [8]
Action on eradicating bTB is a devolved issue. Defra works with the Devolved Administrations in Wales, Scotland and Northern Ireland for coherent and joined-up policies for the UK as a whole. The Chief Veterinary Officers and lead TB policy officials from each country meet on a monthly basis to discuss bTB issues through the UK bTB Liaison Group. [6]
The government had already paid substantial compensation to farmers because of the foot and mouth outbreak in 2001 followed by the bluetongue outbreak in 2007, against the background of EC Directives 77/391 and 78/52 on eradication of tuberculosis, brucellosis or enzootic bovine leucosis . In the 2001 foot and mouth outbreak, a total of £1.4 billion in compensation was paid. The Cattle Compensation (England) Order 2006 (SI2006/168) was overturned when the High Court decided the Order was unlawful; in the test case, farmers had been receiving compensation payments of around £1,000 on animals valued at over £3,000, but in extreme cases the discrepancy between animal value and compensation paid was over one thousand percent. This case was itself overturned on appeal in 2009. [10] [11] [ clarification needed ]
Some farmers' organisations and Defra are in favour of a policy of badger culling because of the mounting costs of the disease to farmers; cattle testing positive for a bTB test must be slaughtered and the farmer paid compensation.  Furthermore, these organisations feel that alternatives to culling are not cost-effective. [5]
In 2005, attempts to eradicate bTB in the UK cost £90 million. [12]
In 2009/10, controlling bTB cost the taxpayer £63 million in England with an additional £8.9 million spent on research. [13]
In 2010/11, nearly 25,000 cattle were slaughtered in England alone, and the cost to the taxpayer of disease control was £91 million; 90% of this amount was accounted for by Government-funded cattle testing and compensation payments to farmers for slaughtered animals. During 2010, 10.8% of herds in England were under restrictions, whilst in the West and South-West, this figure was more than double the average at 22.8%.  In a Defra report in 2011, it was stated that the number of new bTB incidents in England rose in 2010, compared to 2009, and suggested the disease situation was not improving.  It was concluded "...the cost to the taxpayer is huge – it is set to exceed £1 billion over the next ten years in England alone." [6]
There are also considerable costs for farmers, including losses incurred as a result of movement restrictions, having to buy replacement animals, and supporting the required programme of bTB testing (animals must be tested routinely, a repeat test is required if the first is positive, and a pre-movement test is needed if a herd has infection) in a herd. It is more difficult to quantify the costs to the industry but they must run into tens of millions of pounds a year.
The average cost of a bTB breakdown in a cattle herd in England is approximately £30,000. About £20,000 of this is paid by the Government, primarily as compensation for animals compulsorily slaughtered and costs of testing. This leaves approximately £10,000 needing to be paid by farmers as a result of their consequential losses (loss of earnings e.g. milk sales of culled cows), on-farm costs of testing, and disruption to business through movement restrictions. [5] [14]
The risk of humans contracting bTB from milk is extremely low if certain precautions are taken, and scientists have argued that badger culling is unnecessary. [15] [16] The low risk is accepted by Defra who wrote in a report published in 2011: "The risk to public health is very low these days, largely thanks to milk pasteurisation and the TB surveillance and control programme in cattle". [6]
Animal welfare groups such as the Badger Trust and the Royal Society for the Prevention of Cruelty to Animals (RSPCA) are opposed to what they consider random slaughter of badgers — which have special legal protection in the UK — in return for what they describe as a relatively small impact on bTB. [17] [18]
Cattle and badgers are not the only carriers of bTB. The disease can infect and be transmitted by domestic animals such as cats and dogs, wildlife such as deer and farm livestock such as horses and goats. Although the frequency of infection from other mammals is generally much less than in cattle and badgers other species of wildlife have been shown as a possible carriers of bTB. In some areas of South-West England, deer , especially fallow deer due to their gregarious behaviour, have been implicated as a possible maintenance host for transmission of bTB. [19] [20] It has been argued that in some localised areas, the risk of transmission to cattle from fallow deer is greater than it is from badgers. [19] [20] M. bovis was shown to be hosted and transmitted to humans by cats in March 2014 when Public Health England announced two people in England developed bTB infections after contact with a domestic cat. The two human cases were linked to nine cases of bTB infection in cats in Berkshire and Hampshire during 2013. These are the first documented cases of cat-to-human TB transmission. [21]
Research reported in 2016 indicates that bTB is not transmitted by direct contact between badgers and cattle, but through contaminated pasture and dung.  This has important implications for farm practices such as the spreading of slurry .  Using a GPS collar small enough to be worn by badgers, the researchers tracked more than 400 cattle when they were in the territories of 100 badgers.  In 65,000 observations only once did a badger get within 10 metres of a cow – the badgers preferred to be 50 m away.  Experts were quoted as saying expansion of the cull “flies in the face of scientific evidence” and that the cull is a “monstrous” waste of time and money. [22]
Under the Berne Convention on the Conservation of European Wildlife and Natural Habitats , the culling of badgers is only permitted as part of a bTB reduction strategy if there is no satisfactory alternative. [23]
There is widespread public support for an alternative to culling. In October 2012, MPs voted 147 in favour of a motion to stop the 2012/2013 badger cull and 28 against. The debate had been prompted by a petition on the government's e-petition website, which at the time had exceeded 150,000 signatories, and which had by June 2013 gathered around a quarter of a million signatories. By the time it closed on 7 September 2013 there were 303,929 signatures breaking the record for the largest number of people ever to sign a government e-petition. [24] [25]
In July 2008, Hilary Benn , the then Secretary of State for Environment, Food and Rural Affairs, made a statement which highlighted actions other than culling, including allocating funding of £20m to the development of an effective TB injectable vaccine for cattle and badgers, and an oral badger vaccine. [13]
In March 2010, Defra licensed a vaccine for badgers, called the Badger BCG. The vaccine is only effective on animals that do not already have the disease and it can only be delivered by injection. It is available on prescription, subject to a licence to trap badgers from Natural England, but only where injections are carried out by trained vaccinators. Defra funded a programme of vaccinations in 2010/11, and other organisations that have funded smaller vaccination programmes include the National Trust in Devon, the Gloucestershire Wildlife Trust , and a joint project by the National Farmers' Union and the Badger Trust. [26]
—Defra (2001). [27]
However, in England, the Government views badger vaccination as a necessary part of a package of measures for controlling bTB, because it estimates the cost of vaccination to be around £2,250 per square kilometre per annum, and notes that most landowners and farmers have little interest in paying this cost themselves. [27]
In Wales , badger vaccination is carried out in preference to culling. [28] Whilst a field trial into the vaccination of badgers is under way in the Republic of Ireland , as yet, neither culling nor vaccination is carried out in Northern Ireland , although the Northern Ireland Assembly has carried out a review into bTB that recently recommended an immediate investigation into the viability of culling and/or vaccination. [29] In autumn 2009, Scotland was declared officially tuberculosis-free under EU rules, so there are no proposals to cull badgers there. [30]
Although vaccinating cattle is a recognised method of avoiding killing wildlife but reducing the prevalence, incidence and spread of bTB in the cattle population and could also reduce the severity of a herd infection –  regardless of whether infection is introduced by wildlife or cattle –  it has three problems.
As of 2011, Defra have invested around £18 million in the development of cattle vaccines and associated diagnostic tools. [6]
All transmissible livestock diseases can be mitigated by generic good husbandry practices.  BTB risks can be reduced by carefully balanced diets for the cattle, careful sourcing of replacement stock, maintaining correct stocking densities and keeping sheds clean and well ventilated.
Many badgers in Europe were gassed during the 1960s and 1970s to control rabies . [33]
The organism that causes bTB, Mycobacterium bovis , was discovered in 1882, but it took until 1960 for compulsory tests for the disease to be brought in, previously testing was voluntary. Herds that were attested TB free were tested annually and received a premium of 1d per gallon for their milk. Those not tested were able to carry on trading without testing. A programme of test-and-slaughter began and was successful.  Until the 1980s, badger culling in the UK was undertaken in the form of gassing. By 1960 it was thought that bTB might have been eradicated in the UK, until 1971 when a new population of tuberculous badgers was located in Gloucestershire . Subsequent experiments showed that bTB can be spread from badgers to cattle, and some farmers tried to cull badgers on their land.  Wildlife protection groups lobbied Parliament which responded by passing the Badgers Act 1973 , making it an offence to attempt to kill, take, injure badgers or interfere with their setts without a licence. These laws are now contained in the Protection of Badgers Act 1992 . [34] [35]
In 1997, an independent scientific body issued the Krebs Report .  This concluded there was a lack of evidence about whether badger culling would help control the spread of bTB and proposed a series of trials.
The government then ordered an independently run series of trials, known as the Randomised Badger Culling Trials (RBCT). These trials, in which 11,000 badgers in selected areas were cage-trapped and killed, [36] were conducted from 1998 to 2005, although they were briefly suspended due to the outbreak of foot-and-mouth in 2001 . [37] The incidence of bTB in and around 10 large (100 km 2 ) areas in which annual badger culling occurred was compared with the incidence in and around 10 matched areas with no such culling. [38]
In 2003, as a result of initial findings from the RBCT, the reactive component of the culling where badgers were culled in and around farms where bTB was present in cattle, was suspended. This was because the RBCT recorded a 27% increase in bTB outbreaks in these areas of the trial compared to areas in which no culling took place. The advisory group of the trials concluded that reactive culling could not be used to control bTB. [13]
In December 2005, a preliminary analysis of the RBCT data showed that proactive culling, in which most badgers in a particular area were culled, reduced the incidence of bTB by 19% within the cull area, however, it increased by 29% within 2 km outside the cull area. The report therefore warned of a "perturbation effect" in which culling leads to changes in badger behaviour thereby increasing infections within the badger colonies and the migration of infected badgers to previously uninfected areas. Whilst culling produced a decreased badger population locally, it disrupted the badgers’ territorial system, causing any surviving badgers to range more widely, which itself led to a substantial increase in the incidence of the disease, and its wider dispersal. It also reported that a culling policy "would incur costs that were between four and five times higher than the economic benefits gained" and "if the predicted detrimental effects in the surrounding areas are included, the overall benefits achieved would fall to approximately one-fortieth of the costs incurred". [39] In summary, the report argued that it would be more cost-effective to improve cattle control measures, with zoning and supervision of herds, than it would be to cull badgers. [5]
In 2007, the final results of the trials, conducted by the Independent Scientific Group on Cattle TB, were submitted to David Miliband , the then Secretary of State for Environment, Food and Rural Affairs. The report stated that "badger culling can make no meaningful contribution to cattle TB control in Britain. Indeed, some policies under consideration are likely to make matters worse rather than better". According to the report:
detailed evaluation of RBCT and other scientific data highlights the limitations of badger culling as a control measure for cattle TB. The overall benefits of proactive culling were modest (representing an estimated 14 breakdowns prevented after culling 1,000 km 2 for five years), and were realised only after coordinated and sustained effort. While many other approaches to culling can be considered, available data suggest that none is likely to generate benefits substantially greater than those recorded in the RBCT, and many are likely to cause detrimental effects. Given its high costs and low benefits we therefore conclude that badger culling is unlikely to contribute usefully to the control of cattle TB in Britain, and recommend that TB control efforts focus on measures other than badger culling. [13] [39]
In October 2007, after considering the report and consulting other advisors, the then government's Chief Scientific Advisor, Professor Sir David King produced a report of his own which concluded that culling could indeed make a useful contribution to controlling bTB. [40] This was criticised by scientists, most notably in the editorial of "Nature", which implied King was being influenced by politics. [41]
In July 2008, Hilary Benn, the then Secretary of State for Environment, Food and Rural Affairs, refused to authorise a badger cull [42] because of the practicalities and cost of a cull and the scale and length of time required to implement it, with no guarantee of success and the potential for making the disease worse. Benn went on to highlight other measures that would be taken, including allocating £20m to the development of an effective injectable TB vaccine for both cattle and badgers, and an oral badger vaccine. [13]
In 2010, a scientific report [38] was published in which bTB incidence in cattle was monitored in and around RBCT areas after culling ended. The report showed that the benefits inside culled areas decreased over time and were no longer detectable three years after culling ceased. In areas adjoining those which culled, a trend indicated beneficial effects immediately after the end of culling were insignificant, and had disappeared 18 months after the cull ceased. The report also stated that the financial costs of culling an idealized 150 km 2 area would exceed the savings achieved through reduced bTB by factors of 2 to 3.5.  The report concluded "These results, combined with evaluation of alternative culling methods, suggest that badger culling is unlikely to contribute effectively to the control of cattle TB in Britain."
In November 2008, The Bovine TB Eradication Group for England was set up. This Group included Defra officials, members from the veterinary profession and farming industry representatives. Based on research published up to February 2010, the Group concluded that the benefits of the cull were not sustained beyond the culling and that it was ineffective method of controlling bTB in Britain.  They said:
Our findings show that the reductions in cattle TB incidence achieved by repeated badger culling were not sustained in the long term after culling ended and did not offset the financial costs of culling. These results, combined with evaluation of alternative culling methods, suggest that badger culling is unlikely to contribute effectively to the control of cattle TB in Britain. [13]
After the 2010 general election, the new Welsh Environment Minister, John Griffiths , ordered a review of the scientific evidence in favour of and against a cull.  The incoming Defra Secretary of State, Caroline Spelman , began her Bovine TB Eradication Programme for England, which she described as "a science-led cull of badgers in the worst-affected areas".  The Badger Trust put it differently, saying "badgers are to be used as target practice". Shadow Environment Secretary Mary Creagh said it was prompted by "short-term political calculation". [ citation needed ]
The Badger Trust brought court action against the government. On 12 July 2012, their case was dismissed in the High Court; the Trust appealed unsuccessfully. Meanwhile, the Humane Society International pursued a parallel case through the European Courts which was also unsuccessful. Rural Economy and Land Use Programme fellow Angela Cassidy has identified one of the major forces underlying the opposition to badger culls as originating in the historically positive fictional depictions of badgers in British literature. Cassidy further noted that modern negative depictions have recently seen a resurgence. [5] [43] [44]
In August 2015 it was announced culling would be rolled out in Dorset with a target of 615 to 835 badgers being culled there, while also being continued in Gloucestershire and Somerset . Licences were granted to allow six weeks of continuous culling in the three counties until 31 January. [45] In December 2015, Defra released documents confirming the badger cull had "met government targets" with 756 animals culled in Dorset, 432 in Gloucestershire and 279 in Somerset. [46] [47]
In 2009, the Welsh Assembly authorised a non-selective badger cull in the Tuberculosis Eradication (Wales) Order 2009; the Badger Trust sought a judicial review of the decision, but their application was declined. The Badger Trust appealed in Badger Trust v Welsh Ministers [2010] EWCA Civ 807; the Court of Appeal ruled that the 2009 Order should be quashed. [48] The Welsh Assembly replaced proposals for a cull in 2011 with a five-year vaccination programme following a review of the science.
As an attempt to reduce the economic costs of live cage-trapping followed by shooting used in the Randomised Badger Culling Trial, the post-2010 culls in England also allowed for the first time, "free shooting", i.e. shooting free-roaming badgers with firearms.  Licences to cull badgers under the Protection of Badgers Act 1992 are available from Natural England , who require applicants to show that they have the skills, training and resources to cull in an efficient, humane and effective way, and to provide a Badger Control Plan.  This meant that farmers were allowed to shoot the badgers themselves, or to employ suitably qualified persons to do this. The actual killing of the badgers was funded by the farmers, whereas the monitoring and data analysis was funded by Defra.
A Defra statement [49] published in October 2012, stated that "The aim of this monitoring is to test the assumption that controlled shooting is a humane culling technique." The statement makes no indication that the cull would assess the effectiveness of reducing bTB in the trial areas.
A Badger Trust statement [50] indicated the 2012/13 badger cull had three specific aims:
Again, the statement makes no indication that the cull would assess the effectiveness of reducing bTB in the trial areas.
Permission to allow free shooting for the first time during the cull of 2012/13 raised several concerns.
In 2014, the policing costs in Gloucestershire were £1.7 million over the seven-week period (£1,800 per badger) and in Somerset, the cost of policing amounted to £739,000 for the period. [51]
—Caroline Spelman
On 19 July 2011, Caroline Spelman , the then Secretary of State for Environment, Food and Rural Affairs, announced the Government response to the consultation.  It was proposed that a cull would be conducted within the framework of the new "Bovine TB Eradication Programme for England". [6] In view of concerns in response to the initial consultation, a further consultation would determine whether a cull could be effectively enforced and monitored by Natural England under the Protection of Badgers Act 1992. The cull would initially be piloted in two areas, before being extended to other parts of the country.
In December 2011, the Government announced that it intended to go forward with trial badger culls in two 150 km 2 areas. These would take place over a 6-week period with the aim of reducing the badger population by 70% in each area. Farmers and land-owners would be licensed to control badgers by shooting and would bear the costs of any culls. The Government was to bear the costs of licensing and monitoring the culls. [13] The Government would monitor:
In March 2012, the Government appointed members to an Independent Panel of Experts (IPE) to oversee the monitoring and evaluation of the pilot areas and report back to Government.  The panel’s role was to evaluate the effectiveness, humaneness and safety of the controlled shooting method, not the effectiveness of badger culling to control TB in cattle. [52]
The cull was to begin in 2012 [53] led by Defra. However, the Secretary of State for Environment, Owen Paterson, announced in a statement to Parliament on 23 October 2012 that a cull would be postponed until 2013 [52] with a wide range of reasons given. [54]
On 27 August 2013, a full culling programme began in two pilot areas, one mainly in West Somerset and the other mainly in West Gloucestershire with a part in Southeast Herefordshire , at an estimated cost of £7 million per trial area. Up to 5,094 badgers were to be shot. [28] [55] [56] [57] There were closed seasons during the cull, designed to prevent distress to animals or their dependent offspring. [58]
Shooters failed to kill the target of 70% of badgers in both trial areas during the initial 6-week cull.  During this time, 850 badgers were killed in Somerset and 708 in Gloucestershire. Of the badgers culled in Gloucestershire, 543 were killed through free shooting whilst 165 were cage-trapped and shot. In Somerset, 360 badgers were killed by free shooting and 490 by being cage-trapped then shot. [52]
Because the target of 70% badgers to be culled had not been achieved, the cull period was extended. During the 3-week extension in Somerset, an extra 90 badgers were culled, taking the total across the whole cull period to 940, representing a 65% reduction in the estimated badger population.  During the 5 weeks and 3 days extension in Gloucestershire, 213 further badgers were culled, giving an overall total of 921, representing a reduction of just under 40% in the estimated badger population. [13] [52]
Defra and Natural England were unwilling to divulge what data would be collected and the methods of collection during the pilot culls.  However, in a decision under the Freedom of information in the United Kingdom act dated 6 August 2013, [59] the Information Commissioner’s Office found that Defra was wrong to apply the Environmental Information Regulations in defence of its refusal to disclose information about the pilot cull methods. Defra originally intended to sample 240 badgers killed during the pilot culls [59] but confirmed only 120 badgers targeted were to be collected for examination of humaneness and that half of these badgers would be shot while caged. [60] [61] Therefore, only 1.1% of badgers killed by free shooting were tested for humaneness of shooting.  No badgers were to be tested for bTB. [62]
Details of the ongoing pilot culls were not released whilst they were being conducted and Defra declined to divulge how the success of the trials would be measured. [63] As a result, scientists, the RSPCA and other animal charities called for greater transparency over the pilot badger culls. [63] Environment Secretary, Owen Paterson , confirmed that the purpose of the pilot culls was to assess whether farmer-led culls deploying controlled shooting of badgers is suitable to be rolled-out to up to 40 new areas over the next four years. [64] Farming minister, David Heath , admitted in correspondence with Lord Krebs that the cull would "not be able to statistically determine either the effectiveness (in terms of badgers removed) or humaneness of controlled shooting". [65] Lord Krebs, who led the Randomised Badger Culling Trial in the 1990s, said the two pilots "will not yield any useful information". [65]
In explaining why the culling had missed the target, Environment Secretary Owen Paterson famously commented that "the badgers moved the goalposts." [66]
Defra has said it wishes its policy for controlling TB in cattle to be science-led. There is a substantial body of scientific evidence that indicates that culling badgers will not be an effective or cost-effective policy. The best informed independent scientific experts agree that culling on a large, long-term, scale will yield modest benefits and that it is likely to make things worse before they get better. It will also make things worse for farmers bordering on the cull areas.
—Lord Krebs, architect of the original Randomised Badger Culling Trials. [67]
Leaks reported by the BBC in February 2014 indicated that the Expert Panel found that less than half of all badgers were killed in both trial areas.  It was also revealed that between 6.8% and 18% of badgers took more than five minutes to die –  the standard originally set was that this should be less than 5%. [ citation needed ] [68]
It has been suggested that as culling was not selective, as many as six out of seven badgers killed could have been perfectly healthy and bTB free. [69]
Scientific experts agree that culling where there are "hard boundaries" to the cull zones, on a large and long-term scale, could yield modest benefits. If there are 'soft boundaries' allowing badgers to escape, then it will also make things worse for farmers bordering on the cull areas due to infected badgers dispersing: the so-called "perturbation" effect. [70]
The Food and Environment Research Agency (FERA) concluded "the form and duration of badger social perturbation is still poorly understood and significant changes to our assumption may alter the order of preference [of the proposed options]." [71] The Defra-commissioned FERA Report states: "Our modelling has shown that while the differences between the outcomes of strategies using culling and/or vaccinating badgers are quite modest (~15–40 CHBs prevented over 10 years), their risk profile is markedly different. Culling results in the known hazard of perturbation, leading to increased CHBs [Cattle Herd Breakdowns] in the periphery of the culling area. Culling also risks being ineffective or making the disease situation worse, if it is conducted partially (because of low compliance) or ineffectually (because of disruption or poor co-ordination) or it is stopped early (because of licensing issues). Vaccination carries no comparable risks or hazards."
The UK government claims that a sustained cull, conducted over a wide area in a co-ordinated and efficient manner, over a period of nine years, might achieve a 9–16% reduction in disease incidence. [5] However, many scientists and a coalition of animal-welfare and conservation groups including the RSPCA , the Wildlife Trusts and the RSPB , argue that a cull could risk local extermination of all badgers, and that a badger cull will not in any way solve the problem of bovine tuberculosis in cattle. [72] The British Veterinary Association say that data collected from research in other countries, suggests that the control of the disease in farms has only been successfully carried out by dealing with both cattle and wild reservoirs of infection. [2] However, in the introduction to the Final Report on the RBCT, the Chair of the Independent Scientific Group, John Bourne, states: "Scientific findings indicate that the rising incidence of disease can be reversed, and geographical spread contained, by the rigid application of cattle-based control measures alone" (2007, Bovine TB: The Scientific Evidence. Final Report of the Independent Scientific Group on Cattle TB,3-289). In practice it is very difficult to quantify the contribution any wildlife reservoir has to the spread of bovine tuberculosis, since culling is usually carried out alongside cattle control measures (using "all the tools in the tool box" approach):
"From Australian experience, Government has learnt that elimination of a wildlife host (feral Water Buffalo) needs to be followed by a long and extensive programme of cattle testing, slaughter, movement control and public awareness campaigns before bTB is eventually eradicated. And from New Zealand experience, population reduction of the wildlife host (possums) does not by itself reliably control bTB in cattle. In both Australia and New Zealand, Government was dealing with feral reservoirs of bTB rather than indigenous wildlife species, as is the case with the badger in this country" Wilsmore, A.J. and Taylor, N. M. (2005). [73]
John Bourne has also argued that the planned cull is likely only to increase the incidence of bovine tuberculosis , and that there should instead be much greater emphasis on cattle farming controls. He claims that "the cattle controls in operation at the moment are totally ineffective", partly because the tuberculin test used in cattle is not accurate, causing tests in herds to often show negative results even while still harbouring the disease. [36] Referring to the group's final report, he further argues that whilst cattle can get tuberculosis from badgers, the true problem is the other way around: "Badger infections are following, not leading, TB infections in cattle". Overall, he says, the cull will only do more  harm than good, because, "you just chase the badgers around, which makes TB worse". [36]
It is unclear what has been spent so far on planning and preparing for each pilot cull and who exactly is paying for what, i.e. what taxpayers are paying for and what the farming industry is paying for. Costings of the culls have not factored in socio-economic costs, such as tourism and any potential boycotts of dairy products from the cull zones. Others opposed to the cull argue that for economic reasons the government have chosen the most inhumane approach to disease eradication. Tony Dean, Chairperson of the Gloucestershire Badger Group, warns that some badgers will not be killed outright: "You have got to be a good marksman to kill a badger outright, with one shot... Many of the badgers will be badly injured. They will go back underground after being shot, probably badly maimed. They will die a long lingering death underground from lead poisoning etc. We are going to have a lot of cubs left underground where their mothers have been shot above ground." He also suggests that domestic pets will be at risk in the cull areas, as some farmers will mistake black and white cats and dogs for badgers. [74]
Many cull opponents cite vaccination of badgers and cattle as a better alternative to culling. In Wales, where a policy of vaccination in 2013 was into its second year, Stephen James, who is the National Farmers Union Cymru's spokesperson on the matter, argues that the economics of badger culling are "ridiculous" saying the cost per badger was £620. "That's a very expensive way of trying to control this disease when we know full well, from experience from other countries, that there are cheaper ways of doing it...if you vaccinate in the clean areas, around the edges of the endemic areas, then there's a better chance of it working." [74]
The Badger Trust national charity, believes that vaccination will also be more likely to help eradicate the disease. Referring to further studies by Animal Health and Veterinary Laboratories Agency (AHVLA) and the Food and Environment Research Agency (FERA),the group claims that vaccination reduces the risk of unvaccinated badger cubs testing tuberculosis positive, because "by the time cubs emerge and are available for vaccination they might have already been exposed [and are therefore resistant] to TB". [75] Steve Clark, a director of the group, has separately said that "vaccination also reduces the bacilli that is excreted by infected badgers. It doesn't cure them, but it reduces the possibility of any further infection...in the region of a 75% level of protection. The life span of a badger is about five years. So if you continue the vaccination project for five years, then the majority of animals that were there at the beginning will have died out and that vaccination programme is leading towards a clean and healthy badger population." [74]
According to Dr Robbie McDonald, Head of Wildlife and Emerging Diseases at FERA (the lead wildlife scientist for Defra and responsible for research on badgers) the benefit of culling a population is outweighed by the detrimental effect on neighbouring populations of badgers. He is reported as saying that a huge number of badgers would have to be killed to make a difference and while it is cheap and easy to exterminate animals in the early days of a cull it gets harder and more expensive as time goes on. [76]
A Defra-funded statistical analysis from 2013–2017 has shown reduced incidence of farms affected by bovine tuberculosis of 66% in Gloucestershire and 37% in Somerset. After two years of culling in Dorset, no change in incidence was observed. [77]
On 3 April 2014, Owen Paterson decided to continue the culling trials in 2014, in the same areas of Gloucestershire and Somerset as the 2012/13 cull.  On 20 May 2014, the Badger Trust applied for a judicial review of this policy in the High Court, claiming that Mr Paterson unlawfully failed to put into place an independent expert panel to oversee the process. [78]
In response to a Freedom of Information Act request submitted by the Humane Society International (HSI) UK, Defra said that for nearly a year, it had been conducting initial investigations into carbon monoxide gas dispersal in badger sett-like structures.  No live badgers have been gassed.  HSI expressed concerns about the extent to which gassing causes animal suffering. [78]
In September 2014, a second year of badger culling began in Gloucestershire and Somerset as during 2013/2014.  It had previously been stated that the cull was to be extended to a further 10 areas. [69]
The Badger Trust claimed at the High Court that this cull would take place without independent monitoring, however, Defra has denied this saying experts from Natural England and the Animal Health Veterinary Laboratory Agency will be monitoring the cull. [79]
In June 2015, the National Trust, one of the largest landowners in the UK, stated it would not be allowing badger cullers onto their land until the results of all 4 years of pilot trials were known. [80]
The 2014/15 cull targets had been lowered to 316 badgers in Somerset and 615 in Gloucestershire. Overall, the aim was for a reduction of 70% in badger populations over the successive culls. [81] This was to be achieved with an emphasis on trapping badgers in cages and shooting them at dawn, rather than "free shooting". [82]
As in the 2013/14 cull, hundreds of protesters entered the culling areas to disrupt the badgers causing them to remain down their setts and avoiding being trapped and/or shot, or to look for injured badgers. On 9 September 2014, two saboteurs in Gloucestershire found a badger trapped in a cage with cullers nearby. The police were called and the saboteurs pointed out that under government guidelines, trapped badgers should be released if there was a risk of interference from a third party. The sabateur organisation, "Stop the Cull" said police "did the right thing" and freed the badger. Gloucestershire police confirmed the standoff, which it said was resolved peacefully – adding the decision to release the badger was made by a contractor working for the cull operator. [83]
Dr Brian May , guitarist with the rock band Queen is a critic of badger culling in the UK. He has called for the 2014/15 cull to be cancelled. "It's almost beyond belief that the government is blundering ahead with a second year of inept and barbaric badger killing," he said. [83]
Organisations involved in protesting the cull include:
In the 2013/2014 cull, police from forces including Sussex, Warwickshire, Cornwall and the Metropolitan Police were brought in to help with policing, however, the police have said that in the 2014/2015 cull, there will be a focus on more community policing with local officers on patrol.  "It will be very focussed on Gloucestershire officers dealing will local issues." [84]
In probability and statistics , base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities . For example, if it were the case that 1% of the public were "medical professionals", and 99% of the public were not "medical professionals", then the base rate of medical professionals is simply 1%.
In the sciences , including medicine , the base rate is critical for comparison. It may at first seem impressive that 1,000 people beat their winter cold while using 'Treatment X', until we look at the entire 'Treatment X' population and find that the base rate of success is only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. "1,000 people... out of how many?") is available. Note that controls may likewise offer further information for comparison; maybe the control groups , who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that 'Treatment X' makes things worse, despite that initial proud claim about 1,000 people.
The normative method for integrating base rates ( prior probabilities ) and featural evidence ( likelihoods ) is given by Bayes' rule .
A large number of psychological studies have examined a phenomenon called base-rate neglect or base rate fallacy in which category base rates are not integrated with featural evidence in the normative manner. Mathematician Keith Devlin provides an illustration of the risks of this: He asks us to imagine that there is a type of cancer that afflicts 1% of all people. A doctor then says there is a test for that cancer which is about 80% reliable . He also says that the test provides a positive result for 100% of people who have the cancer, but it also results in a 'false positive' for 20% of people - who do not have the cancer. Now, if we test positive, we may be tempted to think it is 80% likely that we have the cancer. Devlin explains that, in fact, our odds are less than 5%. What is missing from the jumble of statistics is the most relevant base rate information. We should ask the doctor, "Out of the number of people who test positive (this is the base rate group that we care about), how many have the cancer?" [1] In assessing the probability that a given individual is a member of a particular class, we must account for other information besides the base rate. In particular, we must account for featural evidence. For example, when we see a person wearing a white doctor's coat and stethoscope , and prescribing medication, we have evidence which may allow us to conclude that the probability of this particular individual being a "medical professional" is considerably greater than the category base rate of 1%.

(wild type)
In epidemiology , the basic reproduction number , or basic reproductive number (sometimes called basic reproduction ratio or basic reproductive rate ), denoted R 0 {\displaystyle R_{0}} (pronounced R nought or R zero ), [19] of an infection is the expected number of cases directly generated by one case in a population where all individuals are susceptible to infection. [15] The definition assumes that no other individuals are infected or immunized (naturally or through vaccination ). Some definitions, such as that of the Australian Department of Health , add the absence of "any deliberate intervention in disease transmission". [20] The basic reproduction number is not the same as the effective reproduction number R {\displaystyle R} (usually written R t {\displaystyle R_{t}} [ t for time], sometimes R e {\displaystyle R_{e}} ), [21] which is the number of cases generated in the current state of a population, which does not have to be the uninfected state. R 0 {\displaystyle R_{0}} is a dimensionless number and not a rate, which would have units of time −1 , [22] or units of time like doubling time . [23]
R 0 {\displaystyle R_{0}} is not a biological constant for a pathogen as it is also affected by other factors such as environmental conditions and the behaviour of the infected population. R 0 {\displaystyle R_{0}} values are usually estimated from mathematical models, and the estimated values are dependent on the model used and values of other parameters. Thus values given in the literature only make sense in the given context and it is recommended not to use obsolete values or compare values based on different models. [24] R 0 {\displaystyle R_{0}} does not by itself give an estimate of how fast an infection spreads in the population.
The most important uses of R 0 {\displaystyle R_{0}} are determining if an emerging infectious disease can spread in a population and determining what proportion of the population should be immunized through vaccination to eradicate a disease. In commonly used infection models , when R 0 > 1 {\displaystyle R_{0}>1} the infection will be able to start spreading in a population, but not if R 0 < 1 {\displaystyle R_{0}<1} . Generally, the larger the value of R 0 {\displaystyle R_{0}} , the harder it is to control the epidemic. For simple models, the proportion of the population that needs to be effectively immunized (meaning not susceptible to infection) to prevent sustained spread of the infection has to be larger than 1 − 1 / R 0 {\displaystyle 1-1/R_{0}} . [25] Conversely, the proportion of the population that remains susceptible to infection in the endemic equilibrium is 1 / R 0 {\displaystyle 1/R_{0}} .
The basic reproduction number is affected by several factors, including the duration of infectivity of affected people, the infectiousness of the microorganism , and the number of susceptible people in the population that the infected people contact.
The roots of the basic reproduction concept can be traced through the work of Ronald Ross , Alfred Lotka and others, [26] but its first modern application in epidemiology was by George Macdonald in 1952, [27] who constructed population models of the spread of malaria .  In his work he called the quantity basic reproduction rate and denoted it by Z 0 {\displaystyle Z_{0}} . Calling the quantity a rate can be misleading, insofar as "rate" can then be misinterpreted as a number per unit of time. "Number" or "ratio" is now preferred.
Suppose that infectious individuals make an average of β {\displaystyle \beta } infection-producing contacts per unit time, with a mean infectious period of τ {\displaystyle \tau } . Then the basic reproduction number is: R 0 = β τ {\displaystyle R_{0}=\beta \,\tau } This simple formula suggests different ways of reducing R 0 {\displaystyle R_{0}} and ultimately infection propagation. It is possible to decrease the number of infection-producing contacts per unit time β {\displaystyle \beta } by reducing the number of contacts per unit time (for example staying at home if the infection requires contact with others to propagate) or the proportion of contacts that produces infection (for example wearing some sort of protective equipment). Hence, it can also be written as [28]
where c ¯ {\displaystyle {\overline {c}}} is the rate of contact between susceptible and infected individuals and T {\displaystyle T} is the transmissibility, i.e, the probability of infection given a contact. It is also possible to decrease the infectious period τ {\displaystyle \tau } by finding and then isolating, treating or eliminating (as is often the case with animals) infectious individuals as soon as possible.
Latent period is the transition time between contagion event and disease manifestation.  In cases of diseases with varying latent periods, the basic reproduction number can be calculated as the sum of the reproduction numbers for each transition time into the disease. An example of this is tuberculosis (TB). Blower and coauthors calculated from a simple model of TB the following reproduction number: [29] R 0 = R 0 FAST + R 0 SLOW {\displaystyle R_{0}=R_{0}^{\text{FAST}}+R_{0}^{\text{SLOW}}} In their model, it is assumed that the infected individuals can develop active TB by either direct progression (the disease develops immediately after infection) considered above as FAST tuberculosis or endogenous reactivation (the disease develops years after the infection) considered above as SLOW tuberculosis. [30]
In populations that are not homogeneous, the definition of R 0 {\displaystyle R_{0}} is more subtle. The definition must account for the fact that a typical infected individual may not be an average individual. As an extreme example, consider a population in which a small portion of the individuals mix fully with one another while the remaining individuals are all isolated. A disease may be able to spread in the fully mixed portion even though a randomly selected individual would lead to fewer than one secondary case. This is because the typical infected individual is in the fully mixed portion and thus is able to successfully cause infections. In general, if the individuals infected early in an epidemic are on average either more likely or less likely to transmit the infection than individuals infected late in the epidemic, then the computation of R 0 {\displaystyle R_{0}} must account for this difference. An appropriate definition for R 0 {\displaystyle R_{0}} in this case is "the expected number of secondary cases produced, in a completely susceptible population, produced by a typical infected individual". [31]
The basic reproduction number can be computed as a ratio of known rates over time: if an infectious individual contacts β {\displaystyle \beta } other people per unit time, if all of those people are assumed to contract the disease, and if the disease has a mean infectious period of 1 γ {\displaystyle {\dfrac {1}{\gamma }}} , then the basic reproduction number is just R 0 = β γ {\displaystyle R_{0}={\dfrac {\beta }{\gamma }}} .  Some diseases have multiple possible latency periods, in which case the reproduction number for the disease overall is the sum of the reproduction number for each transition time into the disease.  For example, Blower et al. [29] model two forms of tuberculosis infection: in the fast case, the symptoms show up immediately after exposure; in the slow case, the symptoms develop years after the initial exposure (endogenous reactivation). The overall reproduction number is the sum of the two forms of contraction: R 0 = R 0 F A S T + R 0 S L O W {\displaystyle R_{0}=R_{0}^{FAST}+R_{0}^{SLOW}} .
The basic reproduction number can be estimated through examining detailed transmission chains or through genomic sequencing . However, it is most frequently calculated using epidemiological models. [32] During an epidemic, typically the number of diagnosed infections N ( t ) {\displaystyle N(t)} over time t {\displaystyle t} is known. In the early stages of an epidemic, growth is exponential, with a logarithmic growth rate K := d ln ⁡ ( N ) d t . {\displaystyle K:={\frac {d\ln(N)}{dt}}.} For exponential growth, N {\displaystyle N} can be interpreted as the cumulative number of diagnoses (including individuals who have recovered) or the present number of infection cases; the logarithmic growth rate is the same for either definition. In order to estimate R 0 {\displaystyle R_{0}} , assumptions are necessary about the time delay between infection and diagnosis and the time between infection and starting to be infectious.
In exponential growth, K {\displaystyle K} is related to the doubling time T d {\displaystyle T_{d}} as K = ln ⁡ ( 2 ) T d . {\displaystyle K={\frac {\ln(2)}{T_{d}}}.}
If an individual, after getting infected, infects exactly R 0 {\displaystyle R_{0}} new individuals only after exactly a time τ {\displaystyle \tau } (the serial interval) has passed, then the number of infectious individuals over time grows as n E ( t ) = n E ( 0 ) R 0 t / τ = n E ( 0 ) e K t {\displaystyle n_{E}(t)=n_{E}(0)\,R_{0}^{t/\tau }=n_{E}(0)\,e^{Kt}} or ln ⁡ ( n E ( t ) ) = ln ⁡ ( n E ( 0 ) ) + ln ⁡ ( R 0 ) t / τ . {\displaystyle \ln(n_{E}(t))=\ln(n_{E}(0))+\ln(R_{0})t/\tau .} The underlying matching differential equation is d n E ( t ) d t = n E ( t ) ln ⁡ ( R 0 ) τ . {\displaystyle {\frac {dn_{E}(t)}{dt}}=n_{E}(t){\frac {\ln(R_{0})}{\tau }}.} or d ln ⁡ ( n E ( t ) ) d t = ln ⁡ ( R 0 ) τ . {\displaystyle {\frac {d\ln(n_{E}(t))}{dt}}={\frac {\ln(R_{0})}{\tau }}.} In this case, R 0 = e K τ {\displaystyle R_{0}=e^{K\tau }} or K = ln ⁡ R 0 τ {\displaystyle K={\frac {\ln R_{0}}{\tau }}} .
For example, with τ = 5 d {\displaystyle \tau =5~\mathrm {d} } and K = 0.183 d − 1 {\displaystyle K=0.183~\mathrm {d} ^{-1}} , we would find R 0 = 2.5 {\displaystyle R_{0}=2.5} .
If R 0 {\displaystyle R_{0}} is time dependent ln ⁡ ( n E ( t ) ) = ln ⁡ ( n E ( 0 ) ) + 1 τ ∫ 0 t ln ⁡ ( R 0 ( t ) ) d t {\displaystyle \ln(n_{E}(t))=\ln(n_{E}(0))+{\frac {1}{\tau }}\int \limits _{0}^{t}\ln(R_{0}(t))dt} showing that it may be important to keep ln ⁡ ( R 0 ) {\displaystyle \ln(R_{0})} below 0, time-averaged, to avoid exponential growth.
In this model, an individual infection has the following stages:
This is a SEIR model and R 0 {\displaystyle R_{0}} may be written in the following form [33] R 0 = 1 + K ( τ E + τ I ) + K 2 τ E τ I . {\displaystyle R_{0}=1+K(\tau _{E}+\tau _{I})+K^{2}\tau _{E}\tau _{I}.} This estimation method has been applied to COVID-19 and SARS . It follows from the differential equation for the number of exposed individuals n E {\displaystyle n_{E}} and the number of latent infectious individuals n I {\displaystyle n_{I}} , d d t ( n E n I ) = ( − 1 / τ E R 0 / τ I 1 / τ E − 1 / τ I ) ( n E n I ) . {\displaystyle {\frac {d}{dt}}{\begin{pmatrix}n_{E}\\n_{I}\end{pmatrix}}={\begin{pmatrix}-1/\tau _{E}&R_{0}/\tau _{I}\\1/\tau _{E}&-1/\tau _{I}\end{pmatrix}}{\begin{pmatrix}n_{E}\\n_{I}\end{pmatrix}}.} The largest eigenvalue of the matrix is the logarithmic growth rate K {\displaystyle K} , which can be solved for R 0 {\displaystyle R_{0}} .
In the special case τ I = 0 {\displaystyle \tau _{I}=0} , this model results in R 0 = 1 + K τ E {\displaystyle R_{0}=1+K\tau _{E}} , which is different from the simple model above ( R 0 = exp ⁡ ( K τ E ) {\displaystyle R_{0}=\exp(K\tau _{E})} ). For example, with the same values τ = 5 d {\displaystyle \tau =5~\mathrm {d} } and K = 0.183 d − 1 {\displaystyle K=0.183~\mathrm {d} ^{-1}} , we would find R 0 = 1.9 {\displaystyle R_{0}=1.9} , rather than the true value of 2.5 {\displaystyle 2.5} . The difference is due to a subtle difference in the underlying growth model; the matrix equation above assumes that newly infected patients are currently already contributing to infections, while in fact infections only occur due to the number infected at τ E {\displaystyle \tau _{E}} ago. A more correct treatment would require the use of delay differential equations . [34]
In reality, varying proportions of the population are immune to any given disease at any given time. To account for this, the effective reproduction number R e {\displaystyle R_{e}} is used, usually written as R t {\displaystyle R_{t}} , or the average number of new infections caused by a single infected individual at time t in the partially susceptible population. It can be found by multiplying R 0 {\displaystyle R_{0}} by the fraction S of the population that is susceptible. When the fraction of the population that is immune increases (i. e. the susceptible population S decreases) so much that R e {\displaystyle R_{e}} drops below 1, " herd immunity " has been achieved and the number of cases occurring in the population will gradually decrease to zero. [35] [36] [37]
Use of R 0 {\displaystyle R_{0}} in the popular press has led to misunderstandings and distortions of its meaning. R 0 {\displaystyle R_{0}} can be calculated from many different mathematical models . Each of these can give a different estimate of R 0 {\displaystyle R_{0}} , which needs to be interpreted in the context of that model. Therefore, the contagiousness of different infectious agents cannot be compared without recalculating R 0 {\displaystyle R_{0}} with invariant assumptions. R 0 {\displaystyle R_{0}} values for past outbreaks might not be valid for current outbreaks of the same disease. Generally speaking, R 0 {\displaystyle R_{0}} can be used as a threshold, even if calculated with different methods: if R 0 < 1 {\displaystyle R_{0}<1} , the outbreak will die out, and if R 0 > 1 {\displaystyle R_{0}>1} , the outbreak will expand. In some cases, for some models, values of R 0 < 1 {\displaystyle R_{0}<1} can still lead to self-perpetuating outbreaks. This is particularly problematic if there are intermediate vectors between hosts, such as malaria . [38] Therefore, comparisons between values from the "Values of R 0 {\displaystyle R_{0}} of well-known infectious diseases" table should be conducted with caution.
Although R 0 {\displaystyle R_{0}} cannot be modified through vaccination or other changes in population susceptibility, it can vary based on a number of biological, sociobehavioral, and environmental factors. [24] It can also be modified by physical distancing and other public policy or social interventions, [39] [24] although some historical definitions exclude any deliberate intervention in reducing disease transmission, including nonpharmacological interventions. [20] And indeed, whether nonpharmacological interventions are included in R 0 {\displaystyle R_{0}} often depends on the paper, disease, and what if any intervention is being studied. [24] This creates some confusion, because R 0 {\displaystyle R_{0}} is not a constant; whereas most mathematical parameters with "nought" subscripts are constants.
R {\displaystyle R} depends on many factors, many of which need to be estimated. Each of these factors adds to uncertainty in estimates of R {\displaystyle R} . Many of these factors are not important for informing public policy. Therefore, public policy may be better served by metrics similar to R {\displaystyle R} , but which are more straightforward to estimate, such as doubling time or half-life ( t 1 / 2 {\displaystyle t_{1/2}} ). [40] [41]
Methods used to calculate R 0 {\displaystyle R_{0}} include the survival function , rearranging the largest eigenvalue of the Jacobian matrix , the next-generation method, [42] calculations from the intrinsic growth rate, [43] existence of the endemic equilibrium, the number of susceptibles at the endemic equilibrium, the average age of infection [44] and the final size equation. Few of these methods agree with one another, even when starting with the same system of differential equations . [38] Even fewer actually calculate the average number of secondary infections. Since R 0 {\displaystyle R_{0}} is rarely observed in the field and is usually calculated via a mathematical model, this severely limits its usefulness. [45]
In the 2011 film Contagion , a fictional medical disaster thriller, a blogger's calculations for R 0 {\displaystyle R_{0}} are presented to reflect the progression of a fatal viral infection from case studies to a pandemic. The methods depicted were faulty. [39]
Bed bugs occur around the world. [1] Rates of infestations in developed countries while decreasing from the 1930s to the 1980s have increased dramatically since the 1980s. [1] [2] [3] Previous to this they were common in the developing world but rare in the developed world. [3] The increase in the developed world may have been caused by increased international travel, resistance to insecticides , and the use of new pest-control methods that do not affect bed bugs. [4] [5]
The fall in bed bug populations after the 1930s in the developed world is believed to be partly due to the usage of DDT to kill cockroaches , [6] which are a natural predator of the bed bug. The invention of the vacuum cleaner and simplification of furniture design may have also played a role. [6] Others believe it might simply be the cyclical nature of the organism. [7]
Bed bugs are increasing in Europe, USA, Canada and Australia. The infestations have been occurring in a wide range of facilities in the developed world in recent years including: hotels (from backpacker to five star), overnight trains, private homes, cruise ships, schools, hospitals and homeless shelters. [1] [6] These infestations are occasionally of both types of bed bugs (common and tropical). [1] The increased rates of infestations have been matched by increased media coverage. [8] Pest management companies have also seen a many fold increase in calls regarding bed bugs during the 2000s. [9]
Figures from one London borough show reported bed bug infestations doubling each year from 1995 to 2001. There is also evidence of a previous cycle of bed bug infestations in the U.K. in the mid-1980s. [10] In 2010 the increase of infestation in UK was estimated to be around 24%. [11] [ unreliable source? ] In the hot summer of 2018 it was reported that bedbug infestations had been increasing year-on-year in UK cities since 2006, with no sign of levelling off. This was considered likely to be due to increased temperatures. Bedbugs were commonly found in seats on London buses and Tube trains. [12]
Bed bugs have been reported in all 50 states. [1] The U.S. National Pest Management Association reported a 71% increase in bed bug calls between 2000 and 2005. [13] The Steritech Group, a pest-management company based in Charlotte , North Carolina , claimed that 25% of the 700 hotels they surveyed between 2002 and 2006 needed bed bug treatment. The resurgence led the United States Environmental Protection Agency to hold a National Bed Bug Summit in 2009. [14]
Numbers of reported incidents in New York City rose from 500 in 2004 to 10,000 in 2009 mostly in Brooklyn area. [15] [16] In August 2010, bed bugs were found in the Elle Fashion Hachette building in New York City. After suspected infestation, a beagle trained in sniffing bed bugs was used to confirm their presence. Office workers were told to work from home while the building was being treated. Although largely thought to only cause problems in less maintained and dirty environments, there is an increasing incidence of bed bugs for infesting indoor environments of high maintenance standards. [17] [18]
One recent theory about bed bug reappearance is that they never truly disappeared from the United States, but may have been forced to alternative hosts. Consistent with this is the finding the bed bug DNA shows no evidence of an evolutionary bottleneck. Furthermore, investigators have found high populations of bed bugs at poultry facilities in Arkansas . Poultry workers at these facilities may be spreading bed bugs, unknowingly carrying them to their places of residence and elsewhere after leaving work. [19] [20]
In November 2016 a media report noted that tropical bed bugs, Cimex hemipterus , which had been extirpated from the state during World War II , were discovered in Brevard County, Florida and were expected to spread in distribution within the United States. [21] [22]
Bed bug infestations have been becoming an increasing issue in urban environments. In a Toronto, Canada study, the mean number of treatments required per affected location was highest at dormitories, hotels, homeless shelters, and rooming houses. Suspected reasons for this increase include increasing world travel, high exchange rates of residents, reluctance to use insecticides because of concerns regarding toxicity, and insecticide resistance . 65 Toronto homeless shelters were surveyed and 31% reported past or present bed bug infestations. [23]
Bed bugs have historically been only a particular problem in developed countries in which they occur both in rural settings and urban setting associated with overcrowding. [ citation needed ]
Binge drinking is the practice of drinking approximately five drinks over a short period of time.  Definitions of binge drinking differ in small ways across cultures and across population subgroups.  For example, many studies use gender-specific measures of binge drinking (such as 5+ drinks for men and 4+ drinks for women). [1] The epidemiology of binge drinking likewise differs across cultures and population subgroups.
According to the National Health Survey 2004 conducted by the Health Promotion Board Singapore , binge drinking is defined as consumption of five or more alcoholic drinks over a short period of time.
The survey results showed that the frequency of binge drinking was 15.6% in males, 11.9% higher than that for females (3.7%). The largest proportion of males and females who binge drink fall within the 18 – 29 age group.
In 2007, Asia Pacific Breweries Singapore (APBS) spearheaded Get Your Sexy Back (GYSB), Singapore's first youth-for-youth initiative to promote responsible and moderate drinking among young adults. The programme seeks to widen awareness and educate individuals about responsible drinking behaviour by raising the social currency of moderation. The programme engages youths in events and activities that are close to their lifestyles, focusing on four major platforms – Music, Fashion, Sports and Friends to spread the message of responsible drinking.
The drinking age in most countries is either 16 or 18, though in many countries national or regional regulations ban the consumption and/or the sale of alcoholic drinks stronger than beer or wine to those less than 18 years of age. Licensees may sometimes choose to provide beverages such as diluted wine or beer mixed with lemonade ( shandy or Lager Top ) with a meal to encourage responsible consumption of alcohol. It is generally perceived that binge drinking is most prevalent in the Vodka Belt (most of Northern and some of Eastern Europe) and least common in the southern part of the continent, in Italy , France , Portugal and the Mediterranean (the Wine Belt ). [2]
Using a "5-drink, 30-days" (5 standard drinks in a row during the last 30 days) definition, Denmark leads European binge drinking, with 60% of 15–16-year-olds reporting participating in this behavior (and 61% reporting intoxication). [3] However, there currently appears to be at least some convergence of drinking patterns and styles between the northern and southern countries, with the south beginning to drink more like the north more so than the other way around.
A notable exception to the lower rates of binge drinking in Southern Europe is the Mediterranean island of Malta , which has adopted the British culture of binge drinking, and where teenagers, often still in their early teens, are able to buy alcohol and drink it in the streets of the main club district, Paceville , due to a lack of police enforcement of the legal drinking age of 17. [4] Statistics show that alcohol consumption in Malta exceeds that in the UK (but binge drinking is slightly lower and intoxication is significantly lower), [3] and report that Malta ranks 5th in the world in common binge drinking. [5] Maltese 15–16-year-olds report binge drinking at a rate of 50%, using a 5-drink, 30-day definition, but only 20% report intoxication in the past 30 days. [3]
Since the mid-1990s the botellón has been growing in popularity among young people. Botellón, which literally means "big bottle" in Spanish, is a drinking party or gathering that involves consuming alcohol, usually spirits (often mixed with soft drinks), in a public or semi-public place (beaches, parks, streets, etc.). This can be considered a case of binge drinking since most people that attend it consume three to five drinks in less than five hours. [ citation needed ] Among 15–16-year-olds, 23% report being intoxicated in the past 30 days. [3]
Binge drinking in Russia (" Zapoy " (" Запой ") in Russian ), often takes the form of two or more days of continuous drunkenness. Sometimes it can even last up to a week. One study found that among men ages 25–54, about 10% had at least one episode of zapoy in the past year, which can be taken as a sign that one has a drinking problem. [6]
Almost half of working-age men in Russia who die are killed by alcohol abuse, reducing Russia's male life expectancy significantly. [6] [7] [8] Vodka is the preferred alcoholic beverage, and Russia is notably considered part of the Vodka Belt . Using a 5-drink, past 30 days definition, 38% of Russian 15–16-year-olds have binged and 27% became intoxicated, a percentage that is on par with other European countries, and even lower than some. [3]
In the UK, parallels have been drawn between binge drinking and the Gin Crisis of 18th century England. [9] Some areas of the media are spending a great deal of time reporting on what they see as a social ill that is becoming more prevalent as time passes. In 2003, the cost of binge drinking was estimated as £20 billion a year. [10] In response, the government has introduced measures to deter disorderly behavior and sales of alcohol to people under 18, with special provisions in place during the holiday season. In January 2005, it was reported that one million admissions to UK emergency department units each year are alcohol-related; in many cities, Friday and Saturday nights are by far the busiest periods for ambulance services.
The culture of drinking in the UK is markedly different from that of some other European nations. In mainland Europe, alcohol tends to be consumed more slowly over the course of an evening, often accompanied by a restaurant meal. In Scandinavia, occasional bouts of heavy drinking are the norm. In the UK (as well as Ireland ), by contrast, alcohol is commonly consumed in rapid binges, leading to more regular instances of severe intoxication. In this way the British combine Northern European volumes of consumption with frequency resembling that of Southern Europe. This "drinking urgency" may have been inspired by traditional pre-midnight pub closing hours in the UK, whereas bars in continental Europe would typically remain open for the entire night. This may have stemmed from the Defence of the Realm Act 1914 , emergency legislation dating back to the first world war regulating pub opening times with the intention of getting workers out of the pub and into the munitions factories. Consequently, it was criticised for being draconian and denying the working classes their pleasures. This is one of the reasons for introducing the Licensing Act 2003 which came into effect in England and Wales in 2005, and which allows 24 hour licensing (although not all bars have taken advantage of the change). Some observers, however, believed it would exacerbate the problem. [11]
As of 2008, results have been mixed and inconsistent across the country. [12] Among young people (under 25), binge drinking (and drinking in general) in England appears to have declined since the late 1990s according to the National Health Service. [13] [14]
While being drunk (outside of a student context) in mainland Europe is widely viewed as being socially unacceptable, [15] in the UK the reverse is true in many social circles. Particularly amongst young adults, there is often a certain degree of peer pressure to get drunk during a night out. [16] This culture is increasingly becoming viewed by politicians and the media as a serious problem that ought to be tackled, partly due to health reasons, but mostly due to its association with violence and anti-social behaviour . [17]
Using a 5-drink, 30-days definition, British 15–16-year-olds binge drink at a rate of 54%, the fourth highest in Europe, and 46% report intoxication in the past 30 days. [18]
The British TV channel Granada produces a program called Booze Britain , which documents the binge drinking culture by following groups of young adults.
As a reaction to the binge drinking epidemic in Britain, several charities have been created to raise awareness of the dangers of binge drinking and promote responsible drinking. These charities notably include Alcohol Concern and Drinkaware.
Canadian binge drinking rates are comparable to the United States, and resemble most the geographically similar states that border on it. For example, 29% of 15- to 19-year-olds (35% male, 22% female) and 37% of 20- to 24-year-olds (47% male, 17.9% female) report having 5 or more drinks on one occasion, 12 or more times a year in 2000–01. [19]
In university, binge drinking is especially common during the first week of orientation, commonly known as "frosh week." The first ever known study comparing the drinking patterns of Canadian and American college students under age 25 (in 1998 and 1999, respectively) found that although Canadian students were more likely to drink, American students drank more heavily overall. [20]
"Heavy alcohol use" was defined as usually having 5/4 drinks or more on the days that the person drinks in the past 30 days (American) or 2–3 months (Canadian). Among past year drinkers, 41% and 35% of American and Canadian students, respectively, reported participated in this behavior. Among the total sample, it was 33% and 30%, respectively. Differences included the lack of a gender gap in Canada compared with America, as well some as age-related differences. Canadians exceeded Americans in reported heavy alcohol use until age 19 (especially among the 1% percentage of students under 18), at which point Americans overtook and then began to exceed Canadians, especially among 21- and 22-year-olds. After age 23, there was no longer much of a difference. [20] In Canada, the legal drinking age is 18 or 19, depending on the province.
A relatively popular drinking game among the Canadian skateboarders and heavy metal culture is "wizard sticks", in which drinkers tape a stack of their empty beer cans to the can from which they are currently drinking. The name comes from the fact that when the stack gets tall enough, it resembles a wizard's staff. [21]
Despite having a legal drinking age of 21, binge drinking in the United States remains very prevalent among high school and college students. Using the popular 5/4 definition of "binge drinking", one study found that, in 1999, 44% of American college students (51% male, 40% female) engaged in this practice at least once in the past two weeks. [22]
One can also look at the prevalence of "extreme drinking" as well. A more recent study of US first-semester college freshmen in 2003 found that, while 41% of males and 34% of females "binged" (using the 5/4 threshold) at least once in the past two weeks, 20% of males and 8% of females drank 10/8 or more drinks (double the 5/4 threshold) at least once in the same period, and 8% of males and 2% of females drank at least 15/12 drinks (triple the threshold). [23]
A main concern of binge drinking on college campuses is how the negative consequences of binge drinking affect the students. A study done by the Harvard School of Public Health reported that students who engage in binge drinking experience numerous problems such as: missing class, engaging in unplanned or unsafe sexual activity, being victims of sexual assault, unintentional injuries, and physical ailments. [24] In 2008 the U.S Surgeon General estimated that around 5,000 Americans aged under 21 die each year from alcohol-related injuries involving underage drinking. [25] Rates of binge drinking in women have been increased; high risk drinking puts these women at increased risk of the negative long-term effects of alcohol consumption . [26]
The population of people who binge drink mainly comprises young adults aged 18–29, although it is by no means rare among older adults. For example, in 2007 (using a 5-drinks definition per occasion for both genders), 42% of 18- to 25-year-olds "binged" at least once a month, while 20% of 16–17-year-olds and 19% of those over age 35 did so. [27] The peak age is 21. Prevalence varies widely by region, with the highest rates being in the North Central states. [28] Binge drinking is more common in men than it is in women. [1]
The annual Monitoring the Future survey found that, in 2007, 10% of 8th graders, 22% of 10th graders, and 26% of 12th graders report having had five or more drinks at least once in the past two weeks. [29] The same survey also found that alcohol was considered somewhat easier to obtain than cigarettes for 8th and 10th graders, even though the minimum age to purchase alcohol is 21 in all 50 states, while for cigarettes it is 18.
The following table represents the percentage of those age 12-20 who illegally binge drink in the United States. [30]
Binge drinking is a common pattern among Native Americans in both Canada and the United States. Anastasia M. Shkilnyk , who conducted an observational study of the Asubpeeschoseewagong First Nation of Northwestern Ontario in the late 1970s when they were demoralized by Ontario Minamata disease , has observed that heavy Native American drinkers may not be physiologically dependent on alcohol, but abuse it by engaging in binge drinking, a practice associated with child neglect, violence, and impoverishment. After binges during which entire families and their friends drink until they are unconscious and their funds are exhausted, they go about their business without drinking. [31]
In 2004–2005, statistics from the National Health Survey [32] show that among the general population over 18; 88% of males and 60% of females engaged in binge drinking at least once in the past year, with 12% and 4%, respectively, doing so at least once a week. Among 18- to 24-year-olds, 49% of males and 21% of females did so at least once a week. At the time, the definition for "binge drinking" corresponded to 7 or more standard Australian drinks per occasion for males and 5 or more for females, roughly equivalent to (but slightly less than) the 5/4 (standard American) drinks definition. [32]
In March 2008, the Australian government earmarked A$ 53 million towards a campaign against binge drinking, citing two studies done in the past eight years which showed that binge drinking in Australia was at what Prime Minister Kevin Rudd called "epidemic levels". [33] On June 15, the Australian Medical Association released new guidelines defining binge drinking as four standard Australian drinks a night. [34]
The last survey of drinking habits by the Australian Bureau of Statistics found there was an increase in drinking outside the home. In 1999, 34 percent of spending on alcoholic drinks took place on licensed premises. By 2004 this figure had risen to 38 percent. This figure is expected to fall in 2008 in Australia because of stricter licensing laws, smoking bans in pubs and the extra premium people have to pay for buying alcohol in a bar. [35]
Concerns over binge drinking by teenagers has led to a review of liquor advertising being announced by the New Zealand government in January 2006. The review considered regulation of sport sponsorship by liquor companies, which at present is commonplace. Previously the drinking age in New Zealand was 20, then dropped to 18 in 1999. [36]
In direct conjunction with the age-lowering, the Police were found to strictly enforce the on-license (bar, restaurant) code for underage-drinking, less so for the off-licences (liquor stores, supermarkets). As a result, young people ages 15–17 found it significantly harder to get into (or be served at) bars and restaurants than it was before with a poorly enforced (though higher) drinking age of 20. This asymmetric enforcement led to a period of many of New Zealand's youth getting strangers to purchase high alcohol content beverages for them (e.g. cheap vodka or rum) at liquor stores. [37]
A propensity to consume an entire bottle of spirits developed and led to an instant increase in the number of youths under 18 being admitted to A&E hospitals. The price of alcohol at supermarkets and liquor stores had also gone down, and the number of outlets had mushroomed as well. [38] Alcohol remains cheap, and sweet, spirit-based ready to drink beverages (similar to alcopops ) remain popular among young people. [39]
An example of this binge drinking mentality, often seen amongst university students, is the popularity of drinking games such as Edward Wineyhands and Scrumpy Hands, similar to the American drinking game Edward Fortyhands . A recent study showed that 37% of undergraduates binged at least once in the past week. [40] The New Zealand health service classifies Binge Drinking as anytime a person consumes five or more standard drinks in a sitting.
In epidemiology and biomedicine , biological plausibility is the proposal of a causal association — a relationship between a putative cause and an outcome — that is consistent with existing biological and medical knowledge.
Biological plausibility is one component of a method of reasoning that can establish a cause-and-effect relationship between a biological factor and a particular disease or adverse event. It is also an important part of the process of evaluating whether a proposed therapy (drug, vaccine, surgical procedure, etc.) has a real benefit to a patient. This concept has application to many controversial public affairs debates, such as that over the causes of adverse vaccination outcomes .
Biological plausibility is an essential element of the intellectual background of epidemiology. The term originated in the seminal work of determining the causality of smoking-related disease ( The Surgeon General’s Advisory Committee on Smoking and Health [1964]).
It is generally agreed that to be considered “causal”, the association between a biological factor and a disease (or other bad outcome) should be biologically coherent. That is to say, it should be plausible and explicable biologically according to the known facts of the natural history and biology of the disease in question.
Other important criteria in evaluations of disease and adverse event causality include consistency , strength of association , specificity and a meaningful temporal relationship . These are known collectively as the Bradford-Hill criteria , after the great English epidemiologist who proposed them in 1965. However, Austin Bradford Hill himself de-emphasized "plausibility" among the other criteria:
It will be helpful if the causation we suspect is biologically plausible. But this is a feature I am convinced we cannot demand. What is biologically plausible depends upon the biological knowledge of the day. To quote again from my Alfred Watson Memorial Lecture [1962], there was
In short, the association we observe may be one new to science or medicine and we must not dismiss it too light-heartedly as just too odd. As Sherlock Holmes advised Dr. Watson , "when you have eliminated the impossible, whatever remains, however improbable, must be the truth." [1]
The preliminary research leading up to a randomized clinical trial (RCT) of a drug or biologic has been termed "plausibility building". This involves the gathering and analysis of biochemical, tissue or animal data which are eventually found to point to a mechanism of action or to demonstrate the desired biological effect. This process is said to confer biological plausibility. Since large, definitive RCTs are extremely expensive and labor-intensive, only sufficiently promising therapies are thought to merit the attention and effort of final confirmation (or refutation) in them.
In distinction to biological plausibility , clinical data from epidemiological studies , case reports , case series and small, formal open or controlled clinical trials may confer clinical plausibility . According to the strictest criteria, a therapy is sufficiently scientifically plausible to merit the time and expense of definitive testing only if it is either biologically or clinically plausible. [2] It has been observed that, despite its importance, biological plausibility is lacking for most complementary and alternative medicine therapies. [2]
BioSense is a program of the Centers for Disease Control and Prevention (CDC) that tracks health problems as they evolve and provides public health officials with the data, information and tools they need to better prepare for and coordinate responses to safeguard and improve the health of the American people.
By integrating local and state-level information, CDC will provide a timely and cohesive picture at the regional (i.e., multistate) and national levels and improve BioSense's utility. 
The key components of the BioSense program redesign are to:
[1]
Mandated in the Public Health Security and Bioterrorism Preparedness Response Act of 2002, the CDC BioSense Program was launched in 2003 to establish an integrated national public health surveillance system for early detection and rapid assessment of potential bioterrorism -related illness. [1]
By November 2011, the Redesigned BioSense (or BioSense 2.0) will develop a community-controlled environment (architecturally distributed in a cloud-based model) governed by the Association of State and Territorial Health Officials (ASTHO), in coordination with the Council of State and Territorial Epidemiologists (CSTE), National Association of County and City Health Officials (NACCHO), and International Society for Disease Surveillance (ISDS).  ASTHO will offer this service to states for receiving and managing syndromic surveillance information.
The cloud-based BioSense 2.0 environment allows State and Local health departments to access data that will support potential expansions of their syndromic surveillance systems under the Meaningful Use program. States that elect to use this utility will each have a secure "zone" that they control and can use to manage or share their syndromic surveillance information. [2]

The Bradford Hill criteria , otherwise known as Hill's criteria for causation , are a group of nine principles that can be useful in establishing epidemiologic evidence of a causal relationship between a presumed cause and an observed effect and have been widely used in public health research. They were established in 1965 by the English epidemiologist Sir Austin Bradford Hill . [1]
In 1996, Fredricks and Relman remarked on Hill's criteria in their seminal paper on microbial pathogenesis . [2]
In 1965, the English statistician Sir Austin Bradford Hill proposed a set of nine criteria to provide epidemiologic evidence of a causal relationship between a presumed cause and an observed effect. (For example, he demonstrated the connection between cigarette smoking and lung cancer.) The list of the criteria is as follows: [1]
Bradford Hill's criteria had been widely accepted as useful guidelines for investigating causality in epidemiological studies but their value has been questioned because they have become somewhat outdated. [3]
In addition, their method of application is debated. [ citation needed ] Some proposed options how to apply them include:
An argument against the use of Bradford Hill criteria as exclusive considerations in proving causality is that the basic mechanism of proving causality is not in applying specific criteria—whether those of Bradford Hill or counterfactual argument—but in scientific common sense deduction . [9] Others argue that the specific study from which data has been produced is important, and while the Bradford Hill criteria may be applied to test causality in these scenarios, the study type may rule out deducing or inducing causality, and the criteria are only of use in inferring the best explanation of this data. [10]
Debate over the scope of application of the criteria includes, whether they can be applied to social sciences . [11] The argument proposes that there are different motives behind defining causality; the Bradford Hill criteria applied to complex systems such as health sciences are useful in prediction models where a consequence is sought; explanation models as to why causation occurred are deduced less easily from Bradford Hill criteria because the instigation of causation, rather than the consequence, is needed for these models.
Researchers have applied Hill’s criteria for causality in examining the evidence in several areas of epidemiology, including connections between ultraviolet B radiation , vitamin D and cancer , [12] [13] vitamin D and pregnancy and neonatal outcomes, [14] alcohol and cardiovascular disease outcomes, [15] infections and risk of stroke , [16] nutrition and biomarkers related to disease outcomes, [17] and sugar-sweetened beverage consumption and the prevalence of obesity and obesity-related diseases . [18] They have also been used in non-human epidemiological studies, such as on the effects of neonicotinoid pesticides on honey bees . [19] Their use in quality improvement of health care services has been proposed, highlighting how quality improvement methods can be used to provide evidence for the criteria. [20]
Since the description of the criteria, many methods to systematically evaluate the evidence supporting a causal relationship have been published, for example the five evidence-grading criteria of the World Cancer Research Fund (Convincing; Probable; Limited evidence – suggestive; Limited evidence – no conclusion; Substantial effect on risk unlikely). [21]
Worldwide, breast cancer is the most common invasive cancer in women.  (The most common form of cancer is non-invasive non-melanoma skin cancer ; non-invasive cancers are generally easily cured, cause very few deaths, and are routinely excluded from cancer statistics.)  Breast cancer comprises 22.9% of invasive cancers in women [2] and 16% of all female cancers. [3]
In 2008, breast cancer caused 458,503 deaths worldwide (13.7% of cancer deaths in women and 6.0% of all cancer deaths for men and women together). [2] Lung cancer , the second most common cause of cancer-related death in women, caused 12.8% of cancer deaths in women (18.2% of all cancer deaths for men and women together). [2]
The number of cases worldwide has significantly increased since the 1970s, a phenomenon partly attributed to the modern lifestyles. [4] [5]
Breast cancer is strongly related to age, with only 5% of all breast cancers occurring in women under 40 years old. [6]
The incidence of breast cancer varies greatly around the world: it is lowest in less-developed countries and greatest in the more-developed countries. [7] In the twelve world regions, the annual age-standardized incidence rates per 100,000 women are as follows: in Eastern Asia, 18; South Central Asia, 22; sub-Saharan Africa, 22; South-Eastern Asia, 26; North Africa and Western Asia, 28; South and Central America, 42; Eastern Europe, 49; Southern Europe, 56; Northern Europe, 73; Oceania, 74; Western Europe, 78; and in North America, 90. [8]
The lifetime risk for breast cancer in the United States is usually given as about 1 in 8 (12%) of women by age 95, with a 1 in 35 (3%) chance of dying from breast cancer. [10] This calculation assumes that all women live to at least age 95, except for those who die from breast cancer before age 95. [11] Recent work, using real-world numbers, indicate that the actual risk is probably less than half the theoretical risk. [12]
The United States has the highest annual incidence rates of breast cancer in the world; 128.6 per 100,000 in whites and 112.6 per 100,000 among African Americans. [10] [13] It is the second-most common cancer (after skin cancer) and the second-most common cause of cancer death (after lung cancer) in women. [10] In 2007, breast cancer was expected to cause 40,910 deaths in the US (7% of cancer deaths; almost 2% of all deaths). [14] This figure includes 450-500 annual deaths among men out of 2000 cancer cases. [15]
In the US, both incidence and death rates for breast cancer have been declining in the last few years. [14] [16] In the US, the age-adjusted incidence of breast cancer per 100,000 women rose from around 102 cases per year in the 1970s to around 141 in the late 1990s, and has since fallen, holding steady around 125 since 2003. Age-adjusted deaths from breast cancer per 100,000 women rose slightly from 31.4 in 1975 to 33.2 in 1989 and have declined steadily since, to 20.5 in 2014. [17] Nevertheless, a US study conducted in 2005 indicated that breast cancer remains the most feared disease, [18] even though heart disease is a much more common cause of death among women. [19] Studies suggest that women overestimate their risk of breast cancer. [20]
Breast cancer is the most common cancer in the UK (around 49,900 women and 350 men were diagnosed with the disease in 2011), and it is the third most common cause of cancer death (around 11,600 women and 75 men died in 2012). [22] The age-standardised incidence rate of breast cancer is 113.4 per 100,000 populations in Wales and there has been a significant increase in the incidence of breast cancer in Wales over the last three decades, which is likely to be partly due to the introduction of the National Health Service Breast Screening Programme. [23]
"Breast cancer in less developed countries, such as those in South America, is a major public health issue. It is a leading cause of cancer-related deaths in women in countries such as Argentina, Uruguay, and Brazil. The expected numbers of new cases and deaths due to breast cancer in South America for the year 2001 are approximately 70,000 and 30,000, respectively." [24] However, because of a lack of funding and resources, treatment is not always available to those suffering with breast cancer. It has also been shown that while the overall incidence of breast cancer appears to be higher in Caucasian women, black African women tend to present at a younger age and with a more aggressive disease pattern, a pattern that has also been reported among black women born and bred in London suggesting a more genetic link rather than only an environmental cause or late presentation. [25]
Data on breast cancer in Sub Saharan Africa is available, though extremely limited compared to developed countries. [26] Breast cancer has the highest incidence among Sub Saharan African women, and has now also the highest mortality rate in many of the countries in the region, before cervical cancer. [27] Breast cancer causes 20% of cancer deaths in women and represents 25% of cancers diagnosed. [27] Incidence rates of breast cancer varies from region to region in Sub Saharan Africa and are 30.4, 26.8, 38.6 and 38.9 respectively in Eastern Africa, Central Africa, Western Africa and Southern Africa. [28] Sub Saharan Africa has lower incidence rates for breast cancer than developed countries but the mortality rates reflected in the region are much higher. [27] [29] Many reasons were found to be the source of this disparity, including the fact that breast cancer is diagnosed at later stages in Sub Saharan Africa. [29] For example, while Central Africa had a mortality/incidence ratio of 0.55 in 2012, the US had only 0.16. [26] In addition to being diagnosed at later stages, breast cancer in Sub Saharan Africa was also found to have an earlier onset compared to western countries. [27] Screening is considered an important tool to tackle the late stage diagnosis of breast cancer by most policy makers in African Countries, especially given that treatment is greatly limited by the lack of resources. [26] [30] More research is also required to produce more updated data on breast cancer and better understand the variances there and how they affect the burden of the disease in the region. [26] [29] [30]
One of the major challenges in reducing the burden of breast cancer in Sub Saharan Africa remains the lack of National Cancer Control Programs and the lack of human as well as financial resources. [26] The majority of countries lack integrated prevention and treatment programs, which complicates the control of the disease in those countries. Also, the regions disposes of a disproportionally low number of cancer registries, along with resources and facilities for treatment. [27] This all factors into the different countries' difficulties to ensure that women at high risk are identified and that the disease is diagnosed early enough to have better chances of being treated. [26] [27] The lack of affordable and effective treatment methods also renders the efforts to promote early detection because those affected are then faced with inaccessible and unaffordable resources in the cases where they are available. [26] The challenges to tackling breast cancer in Africa are varied, not fully understood, and further complicated by possible unique risk factors that could be highlighted by further studies, [27] but developing strategies that foster early detection are viewed in literature as a priority for effective fight against the disease. [31]
Male breast cancer is a much less talked about issue due to its lower incidence with less than 1% of breast cancer in Sub Saharan Africa. [32] A review of the disease found that male to female ratio was higher in Sub Saharan Countries than in developed countries and that onset of the disease occurred on average 7 years latter in men than in women. [32] There is a noticeable decrease in the male to female breast cancer ratio in recent years but that might be associated to the recent increase in female breast cancer in the region. There is still little understanding of the causes of the higher risk for male breast cancer in Sub Saharan Africa and on male breast cancer in general, leading to poor clinical management of the disease. [32]
General:
The epidemiology of cancer is the study of the factors affecting cancer , as a way to infer possible trends and causes.  The study of cancer epidemiology uses epidemiological methods to find the cause of cancer and to identify and develop improved treatments.
This area of study must contend with problems of lead time bias and length time bias . Lead time bias is the concept that early diagnosis may artificially inflate the survival statistics of a cancer, without really improving the natural history of the disease. Length bias is the concept that slower growing, more indolent tumors are more likely to be diagnosed by screening tests, but improvements in diagnosing more cases of indolent cancer may not translate into better patient outcomes after the implementation of screening programs. A related concern is overdiagnosis , the tendency of screening tests to diagnose diseases that may not actually impact the patient's longevity. This problem especially applies to prostate cancer and PSA screening . [3]
Some cancer researchers have argued that negative cancer clinical trials lack sufficient statistical power to discover a benefit to treatment. This may be due to fewer patients enrolled in the study than originally planned. [4]
State and regional cancer registries are organizations that abstract clinical data about cancer from patient medical records. These institutions provide information to state and national public health groups to help track trends in cancer diagnosis and treatment. One of the largest and most important cancer registries is Surveillance Epidemiology and End Results (SEER), administered by the US Federal government . [5]
Health information privacy concerns have led to the restricted use of cancer registry data in the United States Department of Veterans Affairs [6] [7] [8] and other institutions. [9] The American Cancer Society predicts that approximately 1,690,000 new cancer cases will be diagnosed and 577,000 Americans will ultimately die of cancer in 2012. [10]
Observational epidemiological studies that show associations between risk factors and specific cancers mostly serve to generate hypotheses about potential interventions that could reduce cancer incidence or morbidity . Randomized controlled trials then test whether hypotheses generated by epidemiological studies and laboratory research actually result in reduced cancer incidence and mortality. In many cases, findings from observational epidemiological studies are not confirmed by randomized controlled trials.
The most significant risk factor is age.  According to cancer researcher Robert A. Weinberg , "If we lived long enough, sooner or later we all would get cancer." [11] Essentially all of the increase in cancer rates between prehistoric times and people who died in England between 1901 and 1905 is due to increased lifespans. [11]
Although the age-related increase in cancer risk is well-documented, the age-related patterns of cancer are complex. Some types of cancer, like testicular cancer, have early-life incidence peaks, for reasons unknown. Besides, the rate of age-related increase in cancer incidence varies between cancer types with, for instance, prostate cancer incidence accelerating much faster than brain cancer. [12] It has been proposed that the age distribution of cancer incidence can be viewed as the distribution of probability to accumulate the required number of driver events by the given age. [13]
Over a third of cancer deaths worldwide (and about 75-80% of cancers in the United States [14] ) are due to potentially modifiable risk factors. The leading modifiable risk factors worldwide are:
Men with cancer are twice as likely as women to have a modifiable risk factor for their disease. [15]
Other lifestyle and environmental factors known to affect cancer risk (either beneficially or detrimentally) include the use of exogenous hormones (e.g., hormone replacement therapy causes breast cancer ), exposure to ionizing radiation and ultraviolet radiation, and certain occupational and chemical exposures. [ citation needed ]
Every year, at least 200,000 people die worldwide from cancer related to their workplace. [16] Millions of workers run the risk of developing cancers such as pleural and peritoneal mesothelioma from inhaling asbestos fibers, or leukemia from exposure to benzene at their workplaces. [16] Currently, most cancer deaths caused by occupational risk factors occur in the developed world. [16] It is estimated that approximately 20,000 cancer deaths and 40,000 new cases of cancer each year in the U.S. are attributable to occupation. [17]
In the U.S. cancer is second only to cardiovascular disease as the leading cause of death; [18] in the UK it is the leading cause of death. [19] In many developing countries cancer incidence (insofar as this can be measured) appears much lower, most likely because of the higher death rates due to infectious disease or injury. With the increased control over malaria and tuberculosis in some Third World countries, incidence of cancer is expected to rise; in the Eastern Mediterranean region, for example, cancer incidence is expected to increase by 100% to 180% in the next 15 years due to increases in life expectancy, an increasing proportion of elderly people, and the successful control of childhood disease. [20] This is termed the epidemiologic transition in epidemiological terminology.
Cancer epidemiology closely mirrors risk factor spread in various countries. Hepatocellular carcinoma ( liver cancer) is rare in the West but is the main cancer in China and neighbouring countries, most likely due to the endemic presence of hepatitis B and aflatoxin in that population. Similarly, with tobacco smoking becoming more common in various Third World countries, lung cancer incidence has increased in a parallel fashion.
According to the National Cancer Registry Programme of the India Council of Medical Research (ICMR), more than 1300 Indians die every day due to cancer. Between 2012 and 2014, the mortality rate due to cancer increased by approximately 6%. In 2012,  there were 478,180 deaths out of 2,934,314 cases reported. In 2013 there were 465,169 deaths out of  3,016,628 cases. In 2014, 491,598 people died in out of 2,820,179 cases. [21] According to the Population Cancer Registry of Indian Council of Medical Research, the incidence and mortality of cancer is highest in the north-eastern region of the country. [22] Breast cancer is the most common, and stomach cancer is the leading cause of death by cancer for the population as a whole. Breast cancer and lung cancer kill the most women and men respectively. [23]
In Canada, as of 2007, cancer is the number one cause of death, contributing to 29.6% of all deaths in the country. The second highest cause of death is cardiovascular diseases resulting in 21.5% of deaths. As of 2011, prostate cancer was the most common form of cancer among males (about 28% of all new cases) and breast cancer the most common in females (also about 28% of all new cases).
The leading cause of death in both males and females is lung cancer, which contributes to 26.8% of all cancer deaths. Statistics indicate that between the ages of 20 and 50 years, the incidence rate of cancer is higher amongst women whereas after 50 years of age, the incidence rate increases in men. Predictions by the Canadian Cancer Society indicate that with time, there will be an increase in the rates of incidence of cancer for both males and females. Cancer will thus continue to be a persistent issue in years to come.
In the United States, cancer is responsible for 25% of all deaths with 30% of these from lung cancer. The most commonly occurring cancer in men is prostate cancer (about 25% of new cases) and in women is breast cancer (also about 25%). Cancer can occur in children and adolescents, but it is uncommon (about 150 cases per million in the U.S.), with leukemia the most common. [18] In the first year of life the incidence is about 230 cases per million in the U.S., with the most common being neuroblastoma . [24] Data from 2004 to 2008 in the United States indicates that the overall age-adjusted incidence of cancer was approximately 460 per 100,000 men and women per year. [25]
Cancer is responsible for about 25% of all deaths in the U.S., and is a major public health problem in many parts of the world. The statistics below are estimates for the U.S. in 2008, and may vary substantially in other countries. They exclude basal and squamous cell skin cancers, and carcinoma in situ in locations other than the urinary bladder. [18] As seen, breast/prostate cancer, lung cancer and colorectal cancer are responsible for approximately half of cancer incidence. The same applies for cancer mortality, but with lung cancer replacing breast/prostate cancer as the main cause.
In 2016, an estimated 1,685,210 new cases of cancer will be diagnosed in the United States and 595,690 people will die from the disease. [26]
Most common cancers in US males, by occurrence [18]
in US males, by mortality [18]
in US females, by occurrence [18]
in US females, by mortality [18]
In the developed world, one in three people will develop cancer during their lifetimes.  If all cancer patients survived and cancer occurred randomly, the normal lifetime odds of developing a second primary cancer (not the first cancer spreading to a new site) would be one in nine. [27] However, cancer survivors have an increased risk of developing a second primary cancer, and the odds are about two in nine. [27] About half of these second primaries can be attributed to the normal one-in-nine risk associated with random chance. [27]
The increased risk is believed to be primarily due to the same risk factors that produced the first cancer, such as the person's genetic profile, alcohol and tobacco use, obesity, and environmental exposures, and partly due, in some cases, to the treatment for the first cancer, which might have included mutagenic chemotherapeutic drugs or radiation. [27] Cancer survivors may also be more likely to comply with recommended screening, and thus may be more likely than average to detect cancers. [27]
Childhood cancer and cancer in adolescents is rare (about 150 cases per million yearly in the US). Leukemia (usually acute lymphoblastic leukemia ) is the most common cancer in children aged 1–14 in the U.S., followed by the central nervous system cancers , neuroblastoma , Wilms' tumor , and non-Hodgkin's lymphoma . [18] Statistics from the SEER program of the US NCI demonstrate that childhood cancers increased 19% between 1975 and 1990, mainly due to an increased incidence in acute leukemia. Since 1990, incidence rates have decreased. [28]
The age of peak incidence of cancer in children occurs during the first year of life, in infants .  The average annual incidence in the United States, 1975–1995, was 233 per million infants. [28] Several estimates of incidence exist.  According to SEER, [28] in the United States:
Teratoma (a germ cell tumor ) often is cited as the most common tumor in this age group, but most teratomas are surgically removed while still benign, hence not necessarily cancer. Prior to the widespread routine use of prenatal ultrasound examinations, the incidence of sacrococcygeal teratomas diagnosed at birth was 25 to 29 per million births.
Female and male infants have essentially the same overall cancer incidence rates, a notable difference compared to older children.
White infants have higher cancer rates than black infants.  Leukemias accounted for a substantial proportion of this difference: the average annual rate for white infants (48.7 per million) was 66% higher than for
black infants (29.4 per million). [28]
Relative survival for infants is very good for neuroblastoma, Wilms' tumor and retinoblastoma , and fairly good (80%) for leukemia, but not for most other types of cancer.
General:
Cancer survival rates vary by the type of cancer , stage at diagnosis, treatment given and many other factors, including country.  In general survival rates are improving, although more so for some cancers than others.  Survival rate can be measured in several ways, median life expectancy having advantages over others in terms of meaning for people involved, rather than as an epidemiological measure. [1] [2]
However, survival rates are currently often measured in terms of 5-year survival rates, which is the percentage of people who live at least five years after being diagnosed with cancer, and relative survival rates compare people with cancer to people in the overall population. [3]
Several types of cancer are associated with high survival rates, including breast , prostate , testicular and colon cancer . Brain and pancreatic cancers have much lower median survival rates which have not improved as dramatically over the last forty years. [4] Indeed, pancreatic cancer has one of the worst survival rates of all cancers. Small cell lung cancer has a five-year survival rate of 4% according to Cancer Centers of America's Website. [5] The American Cancer Society reports 5-year relative survival rates of over 70% for women with stage 0-III breast cancer with a 5-year relative survival rate close to 100% for women with stage 0 or stage I breast cancer. The 5-year relative survival rate drops to 22% for women with stage IV ( metastatic ) breast cancer. [3]
In cancer types with high survival rates, incidence is usually higher in the developed world , where longevity is also greater. Cancers with lower survival rates are more common in developing countries . [6] The highest cancer survival rates are in countries such as South Korea , Japan , Israel , Australia , and the United States . [7]
In the United States there has been an increase in the 5-year relative survival rate between people diagnosed with cancer in 1975-1977 (48.9%) and people diagnosed with cancer in 2007-2013 (69.2%); these figures coincide with a 20% decrease in cancer mortality from 1950 to 2014. [8]
In males, researchers suggest that the overall reduction in cancer death rates is due in large part to a reduction in tobacco use over the last half century, estimating that the reduction in lung cancer caused by tobacco smoking accounts for about 40% of the overall reduction in cancer death rates in men and is responsible for preventing at least 146,000 lung cancer deaths in men during the time period 1991-2003. [9]
The most common cancer among women in the United States is breast cancer (123.7 per 100,000), followed by lung cancer (51.5 per 100,000) and colorectal cancer (33.6 per 100,000), but lung cancer surpasses breast cancer as the leading cause of cancer death among women. [10] Researchers attribute the reduction in breast cancer mortality to improved treatment, including the increased use in adjuvant chemotherapy . [11]
The National Institute of Health (NIH) attributes the increase in the 5-year relative survival of prostate cancer (from 69% in the 1970s to 100% in 2006) to screening and diagnosis and due to the fact that men that participate in screening tend to be healthier and live longer than the average man and testing techniques that are able to detect slow growing cancer before they become life-threatening. [12]
The most common type of cancer among children and adolescents is leukemia , followed by brain and other central nervous system tumors. Survival rates for most childhood cancers have improved, with a notable improvement in acute lymphoblastic leukemia (the most common childhood cancer). Due to improved treatment, the 5-year survival rate for acute lymphoblastic leukemia has increased from less than 10% in the 1960s to about 90% during the time period 2003-2009. [13]
The improvement in survival rates for many cancers in the last half century is due to improved understanding about the causes of cancer and the availability of new treatment options, which are continually evolving. Where surgery was previously the only option for treatment, cancer is now treated with radiation and chemotherapy , including combination chemotherapy that favors treatment with many drugs over just one. [14] Availability and access to clinical trials has also led to more targeted therapy and improved knowledge of treatment efficacy. There are currently over 60,000 clinical trials related to cancer registered on ClinicalTrials.gov, so novel approaches to cancer treatment are continuing to be developed. [15] The NCI lists over 100 targeted therapies that have been approved for the treatment of 26 different cancer types by the United States Food and Drug Administration. [16]
The Carstairs index is an index of deprivation used in spatial epidemiology to identify Socio-economic confounding .
The index was developed by Vera Carstairs and Russell Morris , and published in 1991 as Deprivation and Health in Scotland . [1] The work focusses on Scotland, and was an alternative to the Townsend Index of deprivation to avoid the use of households as denominators. [2] The Carstairs index is based on four Census variables: low social class, lack of car ownership, overcrowding and male unemployment and the overall index reflects the material deprivation of an area, in relation to the rest of Scotland. Carstairs indices are calculated at the postcode sector level, with average population sizes of approximately 5,000 persons.
The Carstairs index makes use of data collected at the Census to calculate the relative deprivation of an area, therefore there have been four versions: 1981, 1991, 2001 and 2011. The Carstairs indices are routinely produced and published [3] by the MRC/CSO Social and Public Health Sciences Unit at the University of Glasgow.
The components of the Carstairs score are unweighted, and so to ensure that they all have equal influence over the final score, each variable is standardised to have a populatation-weighted mean of zero, and a variance of one, using the z-score method. [1] The Carstairs index for each area is the sum of the standardised values of the components. Indices may be positive or negative, with negative scores indicating that the area has a lower level of deprivation, and positive scores suggesting the area has a relatively higher level of deprivation.
The indices are typically ordered from lowest to highest, and grouped into population quintiles. In the 1981, 1991 and 2001 indices, quintile 1 represented the least [4] deprived areas, and quintile 5, represented the most deprived. In 2011, the order was reversed, in line with the ordering of the Scottish Index of Multiple Deprivation . [5]
The low social class component of the 1981 and 1991 Carstairs index was created using the Registrar General's Social Class (later Social Class for Occupation). In 2001, this was superseded by the National Statistics Socio-economic Classification (NS-SEC). This meant that the definition of low social class had to be amended to reflect the approximate operational categories. [6] The definition of overcrowding was amended between 1981 and 1991, due to the inclusion of kitchens of at least 2 metres wide into the room count in the Census. [7]
In epidemiology , a case fatality rate ( CFR ) – sometimes called case fatality risk or case-fatality ratio – is the proportion of deaths from a certain disease compared to the total number of people diagnosed with the disease for a particular period. A CFR is conventionally expressed as a percentage and represents a measure of disease severity. [1] CFRs are most often used for diseases with discrete, limited-time courses, such as outbreaks of acute infections.  A CFR can only be considered final when all the cases have been resolved (either died or recovered). The preliminary CFR, for example, during an outbreak with a high daily increase and long resolution time would be substantially lower than the final CFR.
The mortality rate –  often confused with the CFR  –  is a measure of the relative number of deaths (either in general, or due to a specific cause) within the entire population per unit of time. [2] A CFR, in contrast, is the number of dead among the number of diagnosed cases only. [3]
Sometimes the term case fatality ratio is used interchangeably with case fatality rate , but they are not the same. A case fatality ratio is a comparison between two different case fatality rates, expressed as a ratio. It is used to compare the severity of different diseases or to assess the impact of interventions. [4]
From a mathematical point of view, CFRs, which take values between 0 and 1 (or 0% and 100%, i.e. , nothing and unity), are actually a measure of risk ( case fatality risk ) – that is, they are a proportion of incidence , although they don't reflect a disease's incidence . They are neither rates , incidence rates , nor ratios (none of which are limited to the range 0–1). They do not take into account time from disease onset to death. [5] [6]
Like the case fatality rate, the term infection fatality rate (IFR) also applies to infectious disease outbreaks, but represents the proportion of deaths among all infected individuals, including all asymptomatic and undiagnosed subjects. It is closely related to the CFR, but attempts to additionally account for inapparent infections among healthy people. [7] The  IFR differs from the CFR in that it aims to estimate the fatality rate in both sick and healthy infected: the detected disease (cases) and those with an undetected disease (asymptomatic and not tested group). [8] (Individuals who are infected, but show no symptoms, are said to have "unapparent", "silent" or "subclinical" infections and may inadvertently infect others.) By definition, the IFR cannot exceed the CFR, because the former adds asymptomatic cases to its denominator.
If 100 people in a community are diagnosed with the same disease, and 9 of them subsequently die from the effects of the disease, the CFR would be 9%. If some of the cases have not yet resolved (neither died nor fully recovered) at the time of analysis, a later analysis might take into account additional deaths and arrive at a higher estimate of the CFR, if the unresolved cases were included as recovered in the earlier analysis. Alternatively, it might later be established that a higher number of people were infected with the pathogen, resulting in an IFR lower than the CFR.
A half dozen examples will suggest the range of possible CFRs for diseases in the real world:

A case–control study (also known as case–referent study ) is a type of observational study in which two existing groups differing in outcome are identified and compared on the basis of some supposed causal attribute. Case–control studies are often used to identify factors that may contribute to a medical condition by comparing subjects who have that condition/disease (the "cases") with patients who do not have the condition/disease but are otherwise similar (the "controls"). [1] They require fewer resources but provide less evidence for causal inference than a randomized controlled trial . A case–control study produces only an odds ratio, which is an inferior measure of strength of association compared to relative risk.
The case–control is a type of epidemiological observational study. An observational study is a study in which subjects are not randomized to the exposed or unexposed groups, rather the subjects are observed in order to determine both their exposure and their outcome status and the exposure status is thus not determined by the researcher.
Porta's Dictionary of Epidemiology defines the case–control study as: an observational epidemiological study of persons with the disease (or another outcome variable) of interest and a suitable control group of persons without the disease (comparison group, reference group). [2] The potential relationship of a suspected risk factor or an attribute to the disease is examined by comparing the diseased and nondiseased subjects with regard to how frequently the factor or attribute is present (or, if quantitative, the levels of the attribute) in each of the groups (diseased and nondiseased)." [2]
For example, in a study trying to show that people who smoke (the attribute ) are more likely to be diagnosed with lung cancer (the outcome ), the cases would be persons with lung cancer, the controls would be persons without lung cancer (not necessarily healthy), and some of each group would be smokers. If a larger proportion of the cases smoke than the controls, that suggests, but does not conclusively show, that the hypothesis is valid.
The case–control study is frequently contrasted with cohort studies , wherein exposed and unexposed subjects are observed until they develop an outcome of interest. [2] [3]
Controls need not be in good health; inclusion of sick people is sometimes appropriate, as the control group should represent those at risk of becoming a case. [4] Controls should come from the same population as the cases, and their selection should be independent of the exposures of interest. [5]
Controls can carry the same disease as the experimental group, but of another grade/severity, therefore being different from the outcome of interest. However, because the difference between the cases and the controls will be smaller, this results in a lower power to detect an exposure effect.
As with any epidemiological study, greater numbers in the study will increase the power of the study. Numbers of cases and controls do not have to be equal. In many situations, it is much easier to recruit controls than to find cases. Increasing the number of controls above the number of cases, up to a ratio of about 4 to 1, may be a cost-effective way to improve the study. [4]
A prospective study watches for outcomes, such as the development of a disease, during the study period and relates this to other factors such as suspected risk or protection factor(s). The study usually involves taking a cohort of subjects and watching them over a long period. The outcome of interest should be common; otherwise, the number of outcomes observed will be too small to be statistically meaningful (indistinguishable from those that may have arisen by chance). All efforts should be made to avoid sources of bias such as the loss of individuals to follow up during the study. Prospective studies usually have fewer potential sources of bias and confounding than retrospective studies.
A retrospective study, on the other hand, looks backwards and examines exposures to suspected risk or protection factors in relation to an outcome that is established at the start of the study. Many valuable case-control studies, such as Lane and Claypon's 1926 investigation of risk factors for breast cancer, were retrospective investigations. Most sources of error due to confounding and bias are more common in retrospective studies than in prospective studies. For this reason, retrospective investigations are often criticised. If the outcome of interest is uncommon, however, the size of prospective investigation required to estimate relative risk is often too large to be feasible. In retrospective studies the odds ratio provides an estimate of relative risk. One should take special care to avoid sources of bias and confounding [1] in retrospective studies.
Case–control studies are a relatively inexpensive and frequently used type of epidemiological study that can be carried out by small teams or individual researchers in single facilities in a way that more structured experimental studies often cannot be.  They have pointed the way to a number of important discoveries and advances. The case–control study design is often used in the study of rare diseases or as a preliminary study where little is known about the association between the risk factor and disease of interest. [7]
Compared to prospective cohort studies they tend to be less costly and shorter in duration. In several situations, they have greater statistical power than cohort studies, which must often wait for a 'sufficient' number of disease events to accrue.
Case–control studies are observational in nature and thus do not provide the same level of evidence as randomized controlled trials . The results may be confounded by other factors, to the extent of giving the opposite answer to better studies. A meta-analysis of what was considered 30 high-quality studies concluded that use of a product halved a risk, when in fact the risk was, if anything, increased. [8] [9] It may also be more difficult to establish the timeline of exposure to disease outcome in the setting of a case–control study than within a prospective cohort study design where the exposure is ascertained prior to following the subjects over time in order to ascertain their outcome status. The most important drawback in case–control studies relates to the difficulty of obtaining reliable information about an individual's exposure status over time. Case–control studies are therefore placed low in the hierarchy of evidence .
One of the most significant triumphs of the case–control study was the demonstration of the link between tobacco smoking and lung cancer, by Richard Doll and Bradford Hill . They showed a statistically significant association in a large case–control study. [10] Opponents argued for many years that this type of study cannot prove causation, but the eventual results of cohort studies confirmed the causal link which the case–control studies suggested, [11] [12] and it is now accepted that tobacco smoking is the cause of about 87% of all lung cancer mortality in the US.
Case–control studies were initially analyzed by testing whether or not there were significant differences between the proportion of exposed subjects among cases and controls. [13] Subsequently, Cornfield [14] pointed out that, when the disease outcome of interest is rare, the odds ratio of exposure can be used to estimate the relative risk (see rare disease assumption ). The validity of the odds ratio depends highly on the nature of the disease studied, on the sampling methodology and on the type of follow-up. Although in classical case-control studies, it remains true that the odds ratio can only approximate the relative risk in the case of rare diseases, there is a number of other types of studies (case-cohort, nested case-control, cohort studies) in which it was later shown that the odds ratio of exposure can be used to estimate the relative risk or the incidence rate ratio of exposure without the need for the rare disease assumption. [13] [15] [16]
When the logistic regression model is used to model the case-control data and the odds ratio is of interest, both the prospective and retrospective likelihood methods will lead to identical maximum likelihood estimations for covariate, except for the intercept. [17] The usual methods of estimating more interpretable parameters than odds ratios -- such as risk ratios, levels, and differences -- is biased if applied to case-control data, but special statistical procedures provide easy to use consistent estimators. [18]
Tetlock and Gardner claimed that the contributions of medical science to increasing human longevity and public health were negligible, and too often negative, until Scottish physician Archie Cochrane was able to convince the medical establishment to adopt randomized control trials after World War II. [19]
In the field of epidemiology , the causal mechanisms responsible for diseases can be understood using the causal pie model, where each pie in the diagram represent a theoretical causal mechanism for a given disease, which is also called a sufficient cause . [1] Each pie is made up many component factors, otherwise known as component causes . In this framework, each component cause represents an event or condition required for a given disease or outcome. A component cause that appears in every pie is called a necessary cause as the outcome cannot occur without it. [2]
Cause , also known as etiology ( / iː t i ˈ ɒ l ə dʒ i / ) and aetiology , is the reason or origination of something. [1]
The word is derived from the Greek αἰτιολογία , aitiologia , "giving a reason for" ( αἰτία , aitia , "cause"; and -λογία , -logia ). [2]
In medicine, the term refers to the causes of diseases or pathologies . [3] Where no etiology can be ascertained, the disorder is said to be idiopathic .
Traditional accounts of the causes of disease may point to the " evil eye ". [4] The Ancient Roman scholar Marcus Terentius Varro put forward early ideas about microorganisms in a 1st-century BC book titled On Agriculture . [5]
Medieval thinking on the etiology of disease showed the influence of Galen and of Hippocrates . [6] Medieval European doctors generally held the view that disease was related to the air and adopted a miasmatic approach to disease etiology. [7]
Etiological discovery in medicine has a history in Robert Koch 's demonstration that the tubercle bacillus ( Mycobacterium tuberculosis complex) causes the disease tuberculosis , Bacillus anthracis causes anthrax , and Vibrio cholerae causes cholera .  This line of thinking and evidence is summarized in Koch's postulates . But proof of causation in infectious diseases is limited to individual cases that provide experimental evidence of etiology.
In epidemiology , several lines of evidence together are required to infer causation. Sir Austin Bradford-Hill demonstrated a causal relationship between smoking and lung cancer, and summarized the line of reasoning in the epidemiological criteria for causation. Dr. Al Evans, a US epidemiologist, synthesized his predecessors' ideas in proposing the Unified Concept of Causation.
Further thinking in epidemiology was required to distinguish causation from association or statistical correlation . Events may occur together simply due to chance , bias or confounding , instead of one event being caused by the other. It is also important to know which event is the cause. Careful sampling and measurement are more important than sophisticated statistical analysis to determine causation. Experimental evidence involving interventions (providing or removing the supposed cause) gives the most compelling evidence of etiology.
Related to this, sometimes several symptoms always appear together, or more often than what could be expected, though it is known that one cannot cause the other. These situations are called syndromes , and normally it is assumed that an underlying condition must exist that explains all the symptoms.
Other times there is not a single cause for a disease, but instead a chain of causation from an initial trigger to the development of the clinical disease. An etiological agent of disease may require an independent co-factor, and be subject to a promoter (increases expression) to cause disease. An example of all the above, which was recognized late, is that peptic ulcer disease may be induced by stress, requires the presence of acid secretion in the stomach, and has primary etiology in Helicobacter pylori infection. Many chronic diseases of unknown cause may be studied in this framework to explain multiple epidemiological associations or risk factors which may or may not be causally related, and to seek the actual etiology.
Some diseases, such as diabetes or hepatitis , are syndromically defined by their signs and symptoms , but include different conditions with different etiologies. These are called heterogeneous conditions .
Conversely, a single etiology, such as Epstein-Barr virus , may in different circumstances produce different diseases such as mononucleosis , nasopharyngeal carcinoma , or Burkitt's lymphoma .
An endotype is a subtype of a condition, which is defined by a distinct functional or pathobiological mechanism. This is distinct from a phenotype , which is any observable characteristic or trait of a disease , such as morphology , development, biochemical or physiological properties, or behavior, without any implication of a mechanism. It is envisaged that patients with a specific endotype present themselves within phenotypic clusters of diseases.
One example is asthma, which is considered to be a syndrome , consisting of a series of endotypes. [8] This is related to the concept of disease entity .
Other example could be AIDS , where an HIV infection can produce several clinical stages. AIDS is defined as the clinical stage IV of the HIV infection. [9]
In law, medicine, and statistics, cause of death is an official determination of conditions resulting in a human 's death , which may be recorded on a death certificate . A cause of death is determined by a medical examiner . The cause of death is a specific disease or injury, in contrast to the manner of death which is a small number of categories like "natural", "accident", "suicide", and "homicide", [1] which have different legal implications. [2]
International Classification of Disease (ICD) codes are often used to record manner and cause of death in a systematic way that makes it easy to compile statistics and more feasible to compare events across jurisdictions. [3]
A study published in Preventing Chronic Disease found that only one-third of New York City resident physicians reported believing that the present system of documentation was accurate. Half reported the inability to record "what they felt to be the correct cause of death", citing reasons such as technical limitation and instruction to "put something else". Nearly four-fifths reported being unaware that determinations of "probable", "presumed", or "undetermined" could be made, and fewer than three percent reported ever updating a death certificate when conflicting lab results or other new information became available, and cardiovascular disease was indicated as "the most frequent diagnosis inaccurately reported". [4]
Causes of death are sometimes disputed by relatives or members of the public, particularly when some degree of uncertainty or ambiguity exists in relation to the cause of death. On occasion, such disputes may result from, or sometimes instigate, a conspiracy theory .
Public perception of the relative risk of death by various causes is biased by personal experience and by media coverage. The phrase " hierarchy of death " is sometimes used to describe the factors that cause some deaths to get more attention than others.
Though some opponents of abortion consider it a cause of death, conventionally medical authorities do not confer personhood on fetuses that are not viable outside the womb, and thus abortions are not reported as deaths in these statistics. [5]
Health departments discourage listing old age as the cause of death because doing so does not benefit public health or medical research . [6] Aging is not a scientifically recognized cause of death; It is considered that there is always a more direct cause, although it may be unknown in certain cases and could be one of a number of aging-associated diseases . As an indirect or non-determinative factor, biological aging is the biggest contributor to deaths worldwide. It is estimated that of the roughly 150,000 people who die each day across the globe, about two thirds—100,000 per day—die of age-related causes. [7] In industrialized nations the proportion is much higher, reaching 90%. [7] In recent years there are official claims about possibility to recognize aging itself as a disease . [8] [9] [10] [11] [12] If this will be so, the situation can change.
There are also popular notions that someone can be "scared to death" or die of loneliness or heartbreak. Experiencing fear, extreme stress, or both can cause changes in the body that can, in turn, lead to death. For example, it is possible that overstimulation of the vagus nerve —which decreases heart rate in a mechanism related to the behavior of apparent death (also known as "playing dead" and "playing possum")—is the cause of documented cases of psychogenic death .  The flight or fight response to fear or stress has the opposite effect, increasing heart rate through stress hormones , and can cause cardiovascular problems (especially in those with pre-existing conditions).  This is the proposed mechanism for the observed increase in the death rate due to cardiac arrest after widely experienced acutely stressful events such as terrorism, military attacks, and natural disasters (even among those who are not in the affected area) and for documented deaths in muggings and other frightening events which caused no traumatic physical harm. [13] [14] The proximal medical cause of death in these cases is likely to be recorded as cardiac failure or vagal inhibition (which also has other potential causes such as blows to certain parts of the body and nerve injuries). [15]
One specific condition observed to result from acute stress, takotsubo cardiomyopathy , is nicknamed "broken heart syndrome", but the stress need not be relationship-related and need not be negative. [16]
Chikungunya is a mosquito-borne alpha virus that was first isolated after a 1952 outbreak in modern-day Tanzania. [1] The virus has circulated in forested regions of sub-Saharan African in cycles involving nonhuman primate hosts and arboreal mosquito vectors. [1] Phylogenetic studies indicate that the urban transmission cycle—the transmission of a pathogen between humans and mosquitoes that exist in urban environments—was established on multiple occasions from strains occurring on the eastern half of Africa in non-human primate hosts. [1] This emergence and spread beyond Africa may have started as early as the 18th century. [1] Currently, available data does not indicate whether the introduction of chikungunya into Asia occurred in the 19th century or more recently, but this epidemic Asian strain causes outbreaks in India and continues to circulate in Southeast Asia. [1]
A number of chikungunya outbreaks have occurred since 2005.  An analysis of the chikungunya virus 's genetic code suggests that the increased severity of the 2005–present outbreak may be due to a change in the genetic sequence, altering the virus' viral coat protein, which potentially allows it to multiply more easily in mosquito cells. [2] The change allows the virus to use  the Asian tiger mosquito (an invasive species ) as a vector in addition to the more strictly tropical main vector, Aedes aegypti . In July 2006, a team analyzed the virus' RNA and determined the genetic changes that have occurred in various strains of the virus and identified those genetic sequences which led to the increased virulence of recent strains. [2]
The largest outbreak of chikungunya ever recorded at the time occurred on the island of Réunion in the  western rim  of the Indian Ocean from late March 2005 to February 2006. [3] At its height, the incidence peaked at about 25,000 cases per week or 3500 daily in early 2006. After an initial peak in May 2005, the incidence decreased and remained stable through the summer hemisphere winter, rising again at the beginning of October 2005. By mid-December, when southern hemisphere summer temperatures are favorable for the mosquito vector, the incidence began to rise dramatically into the first two months of 2006. [4] The number of reported cases was thought to be underestimated. The French government sent several hundred troops to help eradicate mosquitoes. [5] Although confirmed cases were much lower, some estimates based on extrapolations from the number detected by sentinel physicians suggested that as many as 110,000 of Réunion's population of 800,000 people may have been infected. [6] Twelve cases of meningoencephalitis cases were confirmed to be associated with chikungunya infection. [7] Other countries in the southwest Indian Ocean reported cases as well, including Mauritius and the Seychelles , [8] and in Madagascar , the Comoros , and Mayotte .
In 2006, there was a large outbreak in India. States affected by the outbreak were Andhra Pradesh , Andaman & Nicobar Islands , Tamil Nadu , Karnataka , Maharashtra , Gujarat , Madhya Pradesh , Kerala and Delhi . [9] The initial cases were reported from Hyderabad and Secunderabad as well as from Anantpur district as early as November and December 2005 and is continue unabated. In Hyderabad alone an average practitioner saw anywhere between 10 and 20 cases every day. Some deaths have been reported but it was thought to be due mainly to the inappropriate use of antibiotics and anti-inflammatory tablets. The major cause of mortality is due to severe dehydration, electrolyte imbalance and loss of glycemic control. Recovery is the rule except for about 3 to 5% incidence of prolonged arthritis. As this virus can cause thrombocytopenia , injudicious use of these drugs can cause erosions in the gastric epithelium leading to exsanguinating upper GI bleed (due to thrombocytopenia). Also the use of steroids for the control of joint pains and inflammation is dangerous and completely unwarranted. On average there are around 5,300 cases being treated every day. This figure is only from public sector. The figures from the private sector combined would be much higher.
There have been reports of large scale outbreak of this virus in Southern India. At least 80,000 people in Gulbarga, Tumkur, Bidar, Raichur, Bellary, Chitradurga, Davanagere, Kolar and Bijapur districts in Karnataka state are known to have been affected since December 2005. [10]
A separate outbreak of chikungunya fever was reported from Malegaon town in Nasik district, Maharashtra state, in the first two weeks of March 2006, resulting in over 2000 cases. In Orissa state, at most 5000 cases of fever with muscle aches and headache were reported between February 27 and March 5, 2006. [11]
In Bangalore , the state capital of Karnataka (India), there seemed to be an outbreak of chikungunya in May 2006 with arthralgia/arthritis and rashes. As well as in the neighbouring state of Andhra Pradesh. In the 3rd week of May 2006 the outbreak of chikungunya in North Karnataka was severe. All the North Karnataka districts specially Gulbarga, Koppal, Bellary, Gadag, Dharwad were affected. The people of this region are hence requested to be alert. Stagnation of water which provides fertile breeding grounds for the vector ( Aedes aegypti ) should be avoided. The latest outbreak is in Tamil Nadu , India - 20,000 cases have been reported in June 2006. Earlier it was found spreading mostly in the outskirts of Bangalore, but now it has started spreading in the city also (Updated 30/06/2006). More than 300,000 people are affected in Karnataka as of July 2006. [12]
Reported on 29/06/2006, Chennai—fresh cases of this disease has been reported in local hospitals. A heavy effect has been reflected in south TN districts like Kanyakumari and Tirunelveli. Residents of Chennai are warned against the painful disease.
June 2006— Andaman Islands (India) chikungunya cases had been registered virtually for the first time in the month of June 2006. In the beginning of the September cases have gone as much as in thousands. As reported in a local news magazine it has taken the state of epidemic in Andamans. Health authorities are doing their best to handle the situation. Relapsed cases have been noticed with severe pain and swelling in the lower limbs, vomiting and general weakness.
As of July 2006, nearly 50,000 people were affected in Salem, Tamil Nadu . [13]
As of August 2006, nearly 100,000 people were infected in Tamil Nadu. Chennai , capital of Tamil Nadu is one of the worst affected.
On 24 August 2006, The Hindu newspaper reported that the Indian states of Tamil Nadu, Karnataka, Andhra Pradesh, Maharashtra, Madhya Pradesh, Gujarat and Kerala had reported 1.1 million (11 lakh) cases.  The government's claim of no deaths is questioned. [14]
In September 2007, 130 cases were confirmed in the province of Ravenna, Northern Italy, in the contiguous towns of Castiglione di Cervia and Castiglione di Ravenna . One person died. The source of the outbreak was an Indian from Kerala, India. [15]
By the end of September 2009, the Thai Ministry of Health reported more than 42,000 cases during the previous year in 50 provinces in the south of Thailand, including the popular tourist destination of Phuket . [16] About 14 years had lapsed since the last appearance of the disease. [17] In May 2009 the provincial hospital in Trang Province prematurely delivered a 2.7 kg (6 pound) male baby from his chikungunya-infected mother in the hopes of preventing mother-foetus virus transmission. After a cesarean delivery , the physicians discovered that he had also been infected with the chikungunya virus, and put him under intensive care . The child died at six days from respiratory complications, possibly the only death from the outbreak, but the cause of death may not have been chikungunya since the child was delivered prematurely . [18] The Thai physicians gave a preliminary presumption that chikungunya virus might be transmitted from a mother to her foetus . [19]
Outbreaks in the Pacific Islands began in New Caledonia in 2011 and have since occurred in a number of Pacific countries. [20] [21] Fully 1/2 of the entire population of French Polynesia has come down with chikungunya Asian genotype (130,000 cases with 14 dead), exploding from a month earlier with 35,000 cases in December 2014; [22] the first ever case was in 2013.
An outbreak occurred in Cambodia with at least 1500 confirmed cases. Provinces for which affection was confirmed were: Preah Vihear, Battambang, Kampong Thom, Kampong Chhnang, Kandal, Kampong Speu and Takeo. [23] [24]
In December 2013, it was confirmed that chikungunya was being locally transmitted in the Americas for the first time in the French Caribbean dependency of St. Martin, [25] with 66 confirmed cases and suspected cases of around 181. [26] It is the first time in the Americas that the disease has spread to humans from a population of infected mosquitoes. [27]
By mid-January 2014, a number of cases had been confirmed in five countries: St. Martin, Saint Barthélemy , Martinique , Guadeloupe , and the British Virgin Islands . [28] At the start of April, at least ten nations had reported cases.  By the start of May, there were more than 4,100 probable cases, and 31,000 suspected cases spanning 14 countries, including French Guiana , the only non-island nation with at least one reported case.  On May 1, the Caribbean Public Health Agency (CARPHA) declared a Caribbean-wide epidemic of the virus. [29]
As of 21 January 2014, no cases had been reported in Puerto Rico . [30] But by 15 July 2014, over 400 cases had been reported and health authorities believed the number of actual cases (i.e., including unreported cases) was much higher. [31] By November 2014 the Pan American Health Organization reported about 800,000 suspected chikungunya cases in the Caribbean alone. [32]
On July 17, 2014, the first chikungunya case acquired in the United States was reported in Florida by the Centers for Disease Control and Prevention in a man who had not recently traveled outside the United States. [33] Shortly after another case was reported of a person in Florida being infected by the virus, not having traveled outside the U.S. [34]
These were the first two cases where the virus was passed directly by mosquitoes to persons on the U.S. mainland. [35] Aside from the locally acquired infections, there were 484 other cases reported in the United States as of 5 August 2014. [34]
As of 11 September 2014, the number of reported cases in Puerto Rico for the year was 1,636. [36] By 28 October, that number had increased to 2,974 confirmed cases with over 10,000 cases suspected. [37]
In September 2014, the Central University of Venezuela stated that there could be between 65,000 and 117,000 Venezuelans infected with chikungunya. [38] Health Minister Nancy Pérez stated that only 400 Venezuelans were infected with chikungunya [39] [40]
On October 20, 2014, 11 locally acquired cases of chikungunya were reported in Montpellier , Languedoc-Roussillon , in the South of France. 449 imported cases of chikungunya were also reported throughout France during the period May–November 2014. [41]
As of December 2014, Costa Rica had 47 reported cases of chikungunya, 40 of which originated abroad, while 7 were locally acquired. [42]
In June 2014 six cases of the virus were confirmed in Brazil, two in the city of Campinas in the state of São Paulo. The six cases are Brazilian army soldiers who had recently returned from Haiti, where they were participating in the reconstruction efforts as members of the United Nations Stabilisation Mission in Haiti . [43] The information was officially released by Campinas municipality, which considers that it has taken the appropriate actions. [44]
On 25 September 2014, official authorities in El Salvador report over 30,000 confirmed cases of this new epidemic. [45]
On 7 November 2014 Mexico reported an outbreak of chikungunya, acquired by local transmission, in southern state of Chiapas . The outbreak extends across the coastline from the Guatemala border to the neighbouring state of Oaxaca . Health authorities have reported a cumulative load of 39 laboratory-confirmed cases (by the end of week 48). No suspect cases have been reported. [46]
The first cases were officially confirmed in July 2014. [47] [48] Between that month and the end of 2014, as reported by the Colombian Health Institute ( Instituto Nacional de Salud - INS (in Spanish) ), there were 82,977 clinically confirmed cases and 611 cases confirmed through laboratory tests, bringing the total of confirmed cases during 2014 in Colombia to 83,588, 7 of which led to deaths. [49] [50] These cases were reported in the following regions: Amazonas, Atlántico, Arauca, Antioquia, Barranquilla, Bolívar, Boyacá, Caldas, Cartagena, Casanare, Cauca, Cesar, Córdoba, Cundinamarca, Huila, La Guajira, Magdalena, Meta, Putumayo, Nariño, Norte de Santander, Sucre, Santander, Santa Marta, Risaralda, Tolima, San Andrés and Valle del Cauca. [49] [50] According to news outlets, as of January 2015 at least one major city ( Medellín ) has issued sanitary alerts due to the expanding epidemic. [51] By January 2015 the epidemic is considered to be in the initial expansion phase and it is expected by the Colombian National Health Institute (Instituto Nacional de Salud - INS) that the total number of cases will reach around 700,000 by the end of 2015 due to the in-country massive travel of tourists to and from regions where cases of the disease have been confirmed and the vector A. aegypti is indigenous . [47] [52] [53] It is expected that the disease will become endemic and sustain itself, with a pattern of outbreaks similar to dengue fever , due to the fact that both vector and natural reservoirs are indigenous in large areas of the country. [54]
On 24 September 2015, the Ministry of Health and Social Protection of Colombia officially declared the country free of chikungunya. There were 441,000 reported cases but the government estimated the infected to reach the 873,000. [55]
The earliest case was reported on 7 January 2019 in Diosso , Republic of the Congo , and an outbreak was declared by the government on 9 February. By 14 April, 6,149 suspected cases had been reported, with Kouilou Department worst affected (47% of cases); suspected cases have also been reported in the Bouenza , Brazzaville , Lékoumou , Niari , Plateaux , Pointe-Noire and Pool departments. There have been no deaths reported. [56]
Prevalence of childhood obesity has increased dramatically worldwide. In 2010 that the prevalence of childhood obesity during the past two to three decades, much like the United States , has increased in most other industrialized nations, excluding Russia and Poland . [1] Between the early 1970s and late 1990s, prevalence of childhood obesity doubled or tripled in Australia , Brazil , Canada , Chile , Finland , France , Germany , Greece , Japan , the UK , and the USA. [1]
A 2010 article from the American Journal of Clinical Nutrition analyzed global prevalence from 144 countries in preschool children (less than 5 years old). [2] Cross-sectional surveys from 144 countries were used and overweight and obesity were defined as preschool children with values >3SDs from the mean. [2] They found an estimated 42 million obese children under the age of five in the world of which close to 35 million lived in developing countries.11 Additional findings included worldwide prevalence of childhood overweight and obesity increasing from 4.2% (95% CI: 3.2%, 5.2%) in 1990 to 6.7% (95% CI: 5.6%, 7.7%) in 2010 and expecting to rise to 9.1% (95% CI: 7.3%, 10.9%), an estimated 60 million overweight and obese children in 2020. [2]
Children are often viewed as the vulnerable population, needing more attention from government policies and family. The media also portrays this in shows and movies, which can bring a negative effect towards parents whose children are obese by placing blame and responsibility solely in the parents. [3]
Childhood obesity in the United States, has been a serious problem among children and adolescents, and can cause serious health problems among our youth. According to the CDC, as of 2015-2016, in the United States, 18.5% of children and adolescents have obesity, which affects approximately 13.7 million children and adolescents. It affects children of all ages and some ethnic groups more than others, 25.8% Hispanics, 22.0% non-Hispanic blacks, 14.1% non-Hispanic white children are affected by obesity. [4] Prevalence has remained high over the past three decades across most age, sex, racial/ethnic, and socioeconomic groups, and represents a three-fold increase from one generation ago and is expected to continue rising. [5] [6]
Prevalence of pediatric obesity also varies with state. The highest rates of childhood obesity are found in the southeastern states of which Mississippi was found to have the highest rate of overweight/obese children, 44.5%/21.9% respectively. [7] The western states were found to have the lowest prevalence, such as Utah (23.1%) and Oregon (9.6%). [8]
From 2003-2007, there was a twofold increase in states reporting prevalence of pediatric obesity greater than or equal to 18%.7 Oregon was the only state showing decline from 2003 to 2007 (decline by 32%), and using children in Oregon as a reference group , obesity in children in Illinois , Tennessee , Kentucky , West Virginia , Georgia , and Kansas has doubled. [8]
The likelihood of obesity in children was found to increase significantly with decreasing levels of household income , lower neighborhood access to parks or sidewalks, increased television viewing time, and increased recreational computer time. [9] Black and Hispanic children are more likely to be obese compared to white (Blacks OR=1.71 and Hispanics=1.76). [9]
According to the CDC,
For the 2015–2016 year, the CDC found that the prevalence of obesity for children aged 2–19 years old, in the U.S., was 18.5%. [4] The current trends show that children aged 12–19 years old, have obesity levels 2.2% higher than children 6–11 years old (20.6% vs. 18.4%), and children 6–11 years old have obesity levels 4.5% higher than children aged 2–5 years old (18.4% vs. 13.9%). Boys, 6–19 years old, have a 6.1% higher prevalence of obesity, than boys aged 2–5 years old (20.4% vs. 14.3%). While girls aged 12–19 years old, have a 7.4% greater prevalence of obesity, than girls aged 2–5 years old (20.9% vs. 13.5%). [4]
A 2010 NCHS Data Brief published by the CDC found interesting trends in prevalence of childhood obesity. [10] The prevalence of obesity among boys from households with an income at or above 350% the poverty level was found to be 11.9%, while boys with a household income level at or above 130% of the poverty level was 21.1%. [10] The same trend followed in girls. Girls with a household income at or above 350% of the poverty level has an obesity prevalence of 12.0%, while girls with a household income 130% below the poverty level had a 19.3% prevalence. [10]
These trends were not consistent when stratified according to race. “The relationship between income and obesity prevalence is significant among non-Hispanic white boys; 10.2% of those living in households with income at or above 350% of the poverty level are obese compared with 20.7% of those in households below 130% of the poverty level.” [10] The same trend follows in non-Hispanic white girls (10.6% of those living at or above 350% of the poverty level are obese, and 18.3% of those living below 130% of the poverty level are obese) [10]
There is no significant trend in prevalence by income level for either boys or girls among non-Hispanic black and Mexican-American children and adolescents. [10] “In fact, the relationship does not appear to be consistent; among Mexican-American girls, although the difference is not significant, 21.0% of those living at or above 350% of the poverty level are obese compared with 16.2% of those living below 130% of the poverty level.” [10]
Additional findings also include that the majority of children and adolescents are not low income children. [10] The majority of non-Hispanic white children and adolescents also live in households with income levels at or above 130% of the poverty level. [10] Approximately 7.5 million children live in households with income levels above 130% of the poverty level compared to 4.5 million children in households with income at or above 130% of the poverty level. [10]
The importance of identifying the incidence of age-related onset of obesity is vital to understanding when intervention opportunities are most important. Similarly, identifying the incidence of childhood obesity within a respective race, ethnicity, and socioeconomic status, can also help delineate other areas of intervention opportunities for certain populations. A systematic review on the incidence of childhood obesity, found that childhood obesity in the U.S. declines with age. [11] The age-and-sex related incidence of obesity was found to be "4.0% for infants 0–1.9 years, 4.0% for preschool-aged children 2.0–4.9 years, 3.2% for school-aged children 5.0–12.9 years, and 1.8% for adolescents 13.0–18.0 years." When the incidence of childhood obesity, was isolated for the socioeconomically disadvantaged, or for racial/ethnic minority groups, obesity incidence was discovered to be, "4.0% at ages 0–1.9 years, 4.1% at 2.0–4.9 years, 4.4% at 5.0–12.9 years, and 2.2% at 13.0–18.0 years." [11]
Based on a 2015-2016 National Health and Nutritional Examination Survey (NHANES), researchers at Duke University, found that the incidence of childhood obesity is on the rise, with a notable rise in preschool boys (2.0-4.9 years), and girls aged 16.0-19.0 years old.  The Duke University researchers also discovered that although it had been believed that obesity in children had been on a decline in recent years, obesity in children at all ages has actually been increasing. [12]
In epidemiology , a clinical case definition , [1] a clinical definition , or simply a case definition [2] lists the clinical criteria by which public health professionals determine whether a person's illness is included as a case in an outbreak investigation—that is, whether a person is considered directly affected by an outbreak. Absent an outbreak, case definitions are used in the surveillance of public health in order to categorize those conditions present in a population (e.g., incidence and prevalence ).
A case definition defines a case by placing limits on time, person, place, and shared definition with data collection of the phenomenon being studied. [3] [4] Time criteria may include all cases of a disease identified from, for example, January 1, 2008 to March 1, 2008. Person criteria may include age, gender, ethnicity, and clinical characteristics such as symptoms (e.g. cough and fever) and the results of clinical tests (e.g. pneumonia on chest X-ray). Place criteria will usually specify a geographical entity such as a town, state, or country, but may be as small as an institution, a school class, or a restaurant meal session. Shared definition of the phenomenon impacts the study methods and ensures terminology is used in a consistent manner.
Case definitions are often used to label individuals as suspect, probable, or confirmed cases. For example, in the investigation of an outbreak of pneumococcal pneumonia in a nursing home the case definition may be specified as:
By creating a case definition, public health professionals are better equipped to study an outbreak and determine possible causes.
As investigations proceed, a case definition may be expanded or narrowed, a characteristic of the dynamic nature of outbreak investigations. At any given time, the case definition is supposed to be the gold standard to diagnose a given disease. A sensitive case definition, often applied early in an outbreak, will capture all cases, but will include many non-cases. A specific case definition, usually applied after the outbreak is considered more well understood, will exclude most non-cases, but will also exclude some actual cases. [5]
The term diagnostic criteria designates a case definition with a specific combination of signs , symptoms , and test results that the clinician uses to attempt to determine the correct diagnosis.
Some examples of diagnostic criteria are:

When diagnostic criteria are universally accepted they can be considered a "clinical definition" because they define the limits of the affected population, determining which patients are inside and outside of the set.
A clinical definition should be regarded as a statistical analysis tool, and not a substitute for a pathological definition when this is required. [6] Posthumous diagnosis allows to establish the sensitivity and specificity of the clinical definitions [7]
Clinical epidemiology is a subfield of epidemiology specifically focused on issues relevant to clinical medicine . The term was first introduced by John R. Paul in his presidential address to the American Society for Clinical Investigation in 1938. [1] [2] It is sometimes referred to as "the basic science of clinical medicine". [3]
When he coined the term "clinical epidemiology" in 1938, John R. Paul defined it as "a marriage between quantitative concepts used by epidemiologists to study disease in populations and decision-making in the individual case which is the daily fare of clinical medicine". [4] According to Stephenson & Babiker (2000), "Clinical epidemiology can be defined as the investigation and control of the distribution and determinants of disease." [5] Walter O. Spitzer has highlighted the ways in which the field of clinical epidemiology is not clearly defined. However, he felt that, despite criticism of the term, it was a useful way to define a specific subfield of epidemiology. [6] In contrast, John M. Last felt that the term was an oxymoron, and that its increasing popularity in many different medical schools was a serious problem. [4]

This medical article is a stub . You can help Wikipedia by expanding it .
Cognitive epidemiology is a field of research that examines the associations between intelligence test scores (IQ scores or extracted g -factors ) and health, more specifically morbidity (mental and physical) and mortality. Typically, test scores are obtained at an early age, and compared to later morbidity and mortality. In addition to exploring and establishing these associations, cognitive epidemiology seeks to understand causal relationships between intelligence and health outcomes. Researchers in the field argue that intelligence measured at an early age is an important predictor of later health and mortality differences. [1] [2]
A strong inverse correlation between early life intelligence and mortality has been shown across different populations, in different countries, and in different epochs." [3] [4] [5]
A study of one million Swedish men found showed "a strong link between cognitive ability and the risk of death." [6] [7] [8] [9]
A similar study of 4,289 former US soldiers showed a similar relationship between IQ and mortality . [7] [8] [10]
The strong correlation between intelligence and mortality has raised questions as to how better public education could delay mortality. [11]
There is a known inverse correlation between socioeconomic position and health. A 2006 study found that controlling for IQ caused a marked reduction in this association. [12]
Research in Scotland has shown that a 15-point lower IQ meant people had a fifth less chance of seeing their 76th birthday, while those with a 30-point disadvantage were 37% less likely than those with a higher IQ to live that long. [13]
Another Scottish study found that once individuals had reached old age (79 in this study), it was no longer childhood intelligence or current intelligence scores that best predicted mortality but the relative decline in cognitive abilities from age 11 to age 79. They also found that fluid abilities were better predictors of survival in old age than crystallized abilities . [14]
The relationship between childhood intelligence and mortality has even been found to hold for gifted children , those with an intelligence over 135. A 15-point increase in intelligence was associated with a decreased risk of mortality of 32%. This relationship was present until an intelligence score of 163 at which point there was no further advantage of a higher intelligence on mortality risk. [15]
A meta-analysis of the relationship between intelligence and mortality found that there was a 24% increase in mortality for a 1 SD (15 point) drop in IQ score. This meta-analysis also concluded that the association between intelligence and mortality was similar for men and women despite sex differences in disease prevalence and life expectancies. [16]
A whole population follow-up over 68 years showed that the association with overall mortality was also present for most major causes of death. The exceptions were cancers unrelated to smoking and suicide [17]
There is also a strong inverse correlation between intelligence and adult morbidity. Long term sick leave in adulthood has been shown to be related to lower cognitive abilities, [18] as has likelihood of receiving a disability pension. [19]
Among the findings of cognitive epidemiology is that men with a higher IQ have less risk of dying from coronary heart disease . [20] The association is attenuated, but not removed, when controlling for socio-economic variables, such as educational attainment or income. This suggests that IQ may be an independent risk factor for mortality.
One study found that low verbal, visuospatial and arithmetic scores were particularly good predictors of coronary heart disease. [21] Atherosclerosis or thickening of the artery walls due to fatty substances is a major factor in heart disease and some forms of stroke. It has also been linked to lower IQ. [22]
Lower intelligence in childhood and adolescence correlates with an increased risk of obesity . One study found that a 15-point increase in intelligence score was associated with a 24% decrease in risk of obesity at age 51. [23] The direction of this relationship has been greatly debated with some arguing that obesity causes lower intelligence, however, recent studies have indicated that a lower intelligence increases the chances of obesity. [24]
Higher intelligence in childhood and adulthood has been linked to lower blood pressure and a lower risk of hypertension . [25]
Strong evidence has been found in support of a link between intelligence and stroke , with those with lower intelligence being at greater risk of stroke. One study found visuospatial reasoning was the best predictor of stroke compared to other cognitive tests . Further this study found that controlling for socioeconomic variables did little to attenuate the relationship between visuospatial reasoning and stroke. [21]
Studies exploring the link between cancer and intelligence have come to varying conclusions. A few studies, which were mostly small have found an increased risk of death from cancer in those with lower intelligence. [26] [27] Other studies have found an increased risk of skin cancer with higher intelligence. [27] [28] However, on the whole most studies have found no consistent link between cancer and intelligence. [28] [29]
Bipolar disorder is a mood disorder characterized by periods of elevated mood known as mania or hypomania and periods of depression . Anecdotal and biographical evidence popularized the idea that sufferers of bipolar disorder are tormented geniuses that are uniquely equipped with high levels of creativity and superior intelligence. [30] Bipolar disorder is relatively rare, affecting only 2.5% of the population, as it is also the case with especially high intelligence . The uncommon nature of the disorder and rarity of high IQ pose unique challenges in sourcing large enough samples that are required to conduct a rigorous analysis of the association between intelligence and bipolar disorder. [31] Nevertheless, there has been much progress starting from the mid-90s, with several studies beginning to shed a light on this elusive relationship. [32]
One such study examined individual compulsory school grades of Swedish pupils between the ages of 15 and 16 to find that individuals with excellent school performance had a nearly four times increased rate to develop a variation of bipolar disorder later in life than those with average grades. The same study also found that students with lowest grades were at a moderately increased risk of developing bipolar disorder with nearly a twofold increase when compared to average-grade students. [33]
A New Zealand study of 1,037 males and females from the 1972–1973 birth cohort of Dunedin suggests that lower childhood IQs were associated with an increased risk of developing schizophrenia spectrum disorders , major depression , and generalized anxiety disorder in adulthood; whereas higher childhood IQ predicted an increased likelihood of mania. This study only included eight cases of mania and thus should only be used to support already existing trends. [34]
In the largest study yet published analyzing the relationship between bipolar disorder and intelligence, Edinburgh University researchers looked at the link between intelligence and bipolar disorder in a sample of over one million men enlisted in the Swedish army during a 22-year follow-up period. Regression results showed that the risk of hospitalization for bipolar disorder with comorbidity to other mental health illnesses decreased in a linear pattern with an increase in IQ. However, when researchers restricted the analysis to men without any psychiatric comorbidity, the relationship between bipolar disorder and intelligence followed a J-curve .
These findings suggest that men of extremely high intelligence are at a higher risk of experiencing bipolar in its purest form, and demands future investigation of the correlation between extreme brightness and pure bipolar . [31]
Additional support of a potential association between high intelligence and bipolar disorder comes from biographical and anecdotal evidence, and primarily focus on the relationship between creativity and bipolar disorder . [35] Doctor Kay Redfield Jamison has been a prolific writer on the subject publishing several articles and an extensive book analyzing the relationship between the artistic temperament and mood disorders. [36] Although a link between bipolar disorder and creativity has been established , there is no confirming evidence suggesting any significant relationship between creativity and intelligence. [37] Additionally, even though some of these studies suggest a potential benefit to bipolar disorder in regards to intelligence, there is significant amount of controversy as to the individual and societal cost of this presumed intellectual advantage. Bipolar disorder is characterized by periods of immense pain and suffering, self-destructive behaviors, and has one of the highest mortality rates of all mental illnesses. [38]
Schizophrenia is chronic and disabling mental illness that is characterized by abnormal behavior , psychotic episodes and inability to recognize between reality and fantasy . Even though schizophrenia can severely handicap its sufferers, there has been a great interest in the relationship of this disorder and intelligence. Interest in the association of intelligence and schizophrenia has been widespread partly stems from the perceived connection between schizophrenia and creativity , and posthumous research of famous intellectuals that have been insinuated to have suffered from the illness. [39] Hollywood played a pivotal role popularizing the myth of the schizophrenic genius with the movie A Beautiful Mind that depicted the life story of Nobel Laureate , John Nash and his struggle with the illness.
Although stories of extremely bright schizophrenic individuals such as that of John Nash do exist, they are the outliers and not the norm. Studies analyzing the association between schizophrenia and intelligence overwhelmingly suggest that schizophrenia is linked to lower intelligence and decreased cognitive functioning . Since the manifestation of schizophrenia is partly characterized by cognitive and motor declines , current research focuses on understanding premorbid IQ patterns of schizophrenia patients. [40]
In the most comprehensive meta-analysis published since the groundbreaking study by Aylward et al. in 1984, [41] researchers at Harvard University found a medium-sized deficit in global cognition prior to the onset of schizophrenia. The mean premorbid IQ estimate for schizophrenia samples was 94.7 or 0.35 standard deviations below the mean , and thus at the lower end of the average IQ range. Additionally, all studies containing reliable premorbid and post-onset IQ estimates of schizophrenia patients found significant decline in IQ scores when comparing premorbid IQ to post-onset IQ. [42] However, while the decline in IQ over the course of the onset of schizophrenia is consistent with theory, [43] some alternative explanations for this decline suggested by the researchers include the clinical state of the patients and/or side effects of antipsychotic medications . [42] [44]
A recent study published in March 2015 edition of the American Journal of Psychiatry suggests that not only there is no correlation between high IQ and schizophrenia, but rather that a high IQ may be protective against the illness. [45] Researchers from the Virginia Commonwealth University analyzed IQ data from over 1.2 million Swedish males born between 1951 and 1975 at ages 18 to 20 years old to investigate future risk of schizophrenia as a function of IQ scores. The researchers created stratified models using pairs of relatives to adjust for family clusters and later applied regression models to examine the interaction between IQ and genetic predisposition to schizophrenia. Results from the study suggest that subjects with low IQ were more sensitive to the effect of genetic liability to schizophrenia than those with high IQ and that the relationship between IQ and schizophrenia is not a consequence of shared genetic or familial-environmental risk factors , but may instead be causal. [45] [46]
The Archive of General Psychiatry published a longitudinal study of a randomly selected sample of 713 study participants (336 boys and 377 girls), from both urban and suburban settings. Of that group, nearly 76 percent had suffered through at least one traumatic event . Those participants were assessed at age 6 years and followed up to age 17 years. In that group of children, those with an IQ above 115 were significantly less likely to have Post-Traumatic Stress Disorder as a result of the trauma, less likely to display behavioral problems, and less likely to experience a trauma. The low incidence of Post-Traumatic Stress Disorder among children with higher IQs was true even if the child grew up in an urban environment (where trauma averaged three times the rate of the suburb), or had behavioral problems. [47]
Post-traumatic stress disorder , severe depression , and schizophrenia are less prevalent in higher IQ bands. Some studies have found that higher IQ persons show a higher prevalence of Obsessive Compulsive Disorder , but a 2017 meta study found the opposite, that people who suffered from OCD had slightly lower average IQs. [48] [47] [49] [42] [50]
Substance abuse is a patterned use of drug consumption in which a person uses substances in amounts or with methods that are harmful to themselves or to others. Substance abuse is commonly associated with a range of maladaptive behaviors that are both detrimental to the individual and to society. Given the terrible consequences that can transpire from abusing substances, recreational experimentation and/or recurrent use of drugs are traditionally thought to be most prevalent among marginalized strands of society. Nevertheless, the very opposite is true; research both in national and individual levels have found that the relationship between IQ and substance abuse indicates positive correlations between superior intelligence, higher alcohol consumption and drug consumption. [51]
A significant positive association between worldwide national alcohol consumption per capita and country level IQ scores has been found. [52]
The relationship between childhood IQ scores and illegal drugs use by adolescence and middle age has been found. High IQ scores at age 10 are positively associated with intake of cannabis , cocaine (only after 30 years of age), ecstasy , amphetamine and polydrug and also highlight a stronger association between high IQ and drug use for women than men. [53] Additionally, these findings are independent of socio-economic status or psychological distress during formative years. A high IQ at age 11 was predictive of increased alcohol dependency later in life and a one standard deviation increase in IQ scores (15-points) was associated with a higher risk of illegal drug use. [54]
The counterintuitive nature of the correlation between high IQ and substance abuse has sparked a fervent debate in the scientific community with some researchers attributing these findings to IQ being an inadequate proxy of intelligence, while others fault employed research methodologies and unrepresentative data. [55] [56] [57] However, with the increased number of studies publishing similar results, overwhelming consensus is that the association between high IQ and substance abuse is real, statistically significant and independent of other variables. [58]
There are several competing theories trying to make sense of this apparent paradox. Doctor James White postulates that people with higher IQs are more critical of information and thus less likely to accept facts at face value. While marketing campaigns against drugs may deter individuals with lower IQs from using drugs with disjoint arguments or over-exaggeration of negative consequences, people with a higher IQ will seek to verify the validity of such claims in their immediate environment. White also eludes to an often-overlooked problem of people with higher IQ, the lack of adequate challenges and intellectual stimulation . [59] White posits that high IQ individuals that are not sufficiently engaged in their lives may choose to forgo good judgment for the sake of stimulation. [60]
The most prominent [ citation needed ] theory attempting to explain the positive relationship between IQ and substance abuse; however, is the Savanna–IQ interaction hypothesis by social psychologist Satoshi Kanazawa . The theory is founded on the assumption that intelligence is a domain-specific adaptation that has evolved as humans moved away from the birthplace of human race, the savanna . Therefore, theory follows that as humans explored beyond the savannas, intelligence rather than instinct dictated survival . Natural selection privileged those who possessed high IQ while simultaneously favoring those with an appetite for evolutionary novel behaviors and experiences. [61] For Kanazawa, this drive to seek evolutionary novel activities and sensations translates to being more open and callous about experimenting with and/or abusing substances in modern culture. For all the attention that the Savanna–IQ interaction hypothesis has garnered with the general public, [62] this theory however, receives equal amounts of praise and criticism in the academic community with key pain points being the fact that humans have continued to evolve after moving away from the savannas and Kanazawa's misattribution of aspects of the openness personality trait to being indicative of superior general intelligence . [63]
A decrease in IQ has also been shown as an early predictor of late-onset Alzheimer's Disease and other forms of dementia .  In a 2004 study, Cervilla and colleagues showed that tests of cognitive ability provide useful predictive information up to a decade before the onset of dementia. [64]
However, when diagnosing individuals with a higher level of cognitive ability, a study of those with IQ's of 120 or more, [65] patients should not be diagnosed from the standard norm but from an adjusted high-IQ norm that measured changes against the individual's higher ability level.
In 2000, Whalley and colleagues published a paper in the journal Neurology , which examined links between childhood mental ability and late-onset dementia.  The study showed that mental ability scores were significantly lower in children who eventually developed late-onset dementia when compared with other  children tested. [66]
The relationship between alcohol consumption and intelligence is not straightforward. In some cohorts higher intelligence has been linked to a reduced risk of binge drinking. In one Scottish study higher intelligence was linked to a lower chance of binge drinking; however, units of alcohol consumed were not measured and alcohol induced hangovers in middle age were used as a proxy for binge drinking. [67] Several studies have found the opposite effect with individuals of higher intelligence being more likely to drink more frequently, consume more units and have a higher risk of developing a drinking problem, especially in women. [68]
In U.S. study the link between drug intake and intelligence suggests that individuals with lower IQ take more drugs. [69] However, in the UK the opposite relationship has been found with higher intelligence being related to greater illegal drug use. [70]
The relationship between intelligence and smoking has changed along with public and government attitudes towards smoking. For people born in 1921 there was no correlation between intelligence and having smoked or not smoked; however, there was a relationship between higher intelligence and quitting smoking by adulthood. [71] In another British study, high childhood IQ was shown to inversely correlate with the chances of starting smoking. [72]
One British study found that high childhood IQ was shown to correlate with one's chance of becoming a vegetarian in adulthood. [73] Those of higher intelligence are also more likely to eat a healthier diet including more fruit and vegetables, fish, poultry and wholemeal bread and to eat less fried food. [74]
Higher intelligence has been linked to exercising.  More intelligent children tend to exercise more as adults and to exercise vigorously. [27] [23] [75]
A study of 11,282 individuals in Scotland who took intelligence tests at ages 7, 9 and 11 in the 1950s and 1960s, found an "inverse linear association" between childhood intelligence and hospital admissions for injuries in adulthood.  The association between childhood IQ and the risk of later injury remained even after accounting for factors such as the child's socioeconomic background. [76]
Practically all indicators of physical health and mental competence favour people of higher socioeconomic status (SES). Social class attainment is important because it can predict health across the lifespan, where people from lower social class have higher morbidity and mortality. [77] SES and health outcomes are general across time, place, disease, and are finely graded up the SES continuum. Gottfredson [78] argues that general intelligence (g) is the fundamental cause for health inequality . The argument is that g is the fundamental cause of social class inequality in health, because it meets six criteria that every candidate for the cause must meet: stable distribution over time, is replicable, is a transportable form of influence, has a general effect on health, is measurable, and is falsifiable.
Stability: Any casual agent has to be persistent and stable across time for its pattern of effects to be general over ages and decades. [78] Large and stable individual differences in g are developed by adolescence and the dispersion of g in population's intelligence present in every generation , no matter what social circumstances are present. Therefore, equalizing socioeconomic environments does very little to reduce the dispersion in IQ. [79] The dispersion of IQ in a society in general is more stable, than its dispersion of socioeconomic status. [78]
Replicability : Siblings who vary in IQ also vary in socioeconomic success which can be comparable with strangers of comparable IQ. [80] Also, g theory [78] predicts that if genetic g is the principal mechanism carrying socioeconomic inequality between generations, then the maximum correlation between the parent and child SES will be near to their genetic correlation for IQ (.50).
Transportability: The performance [81] and functional literacy [82] studies both illustrated how g is transportable across life situations and it represents a set of largely generalizable reasoning and problem-solving skills. [83] G appear to be linearly linked to performance in school, jobs and achievements.
Generality: Studies [84] show that IQ measured at the age of 11 predicted longevity, premature death, lung and stomach cancers, dementia , loss of functional independence, more than 60 years later.  Research has shown that higher IQ at age 11 is significantly related to higher social class in midlife. [85] Therefore, it is safe to assume that higher SES, as well as higher IQ, generally predicts better health.
Measurability: g factor can be extracted from any broad set of mental tests and has provided a common, reliable source for measuring general intelligence in any population. [78]
Falsifiability: theoretically, if g theory [78] would conceive health self-care as a job, as a set of instrumental tasks performed by the individuals, it could predict g to influence the health performance in the same way as it predicts performance in education and job.
Chronic illnesses are the major illnesses in developed countries today, and their major risk factors are health habits and lifestyle. [78] The higher social strata knows the most and the lower social strata knows the least, whether class is assessed by education, occupation or income and even when the information seems to be most useful for the poorest. Higher g promotes more learning, and it increases exposure to learning opportunities. So, the problem is not in the lack of access to health-care, but the patient's failure to use it effectively when delivered. Low literacy [86] has been associated with low use of preventive care, poor comprehension of one's illness – even when care is free. Health self-management is important because literacy provides the ability to acquire new information and complete complex tasks and that limited problem solving abilities make low-literacy patients less likely to change their behaviour on the basis of new information. [78] Chronic lack of good judgement and effective reasoning leads to chronically poor self-management.
There have been many reasons posited for the links between health and intelligence. Although some have argued that the direction is one in which health has an influence on intelligence, most have focused on the influence of intelligence on health. Although health may definitely affect intelligence, most of the cognitive epidemiological studies have looked at intelligence in childhood when ill health is far less frequent and a more unlikely cause of poor intelligence. [3] Thus most explanations have focused on the effects intelligence has on health through its influence on mediating causes.
Various explanations for these findings have been proposed:
"First, ...intelligence is associated with more education, and thereafter with more professional occupations
that might place the person in healthier environments. ...Second, people with higher intelligence might engage in more healthy behaviours. ...Third, mental test scores from early life might act as a record of insults to the brain that have occurred before that date. ...Fourth, mental test scores obtained in youth might be an indicator of a well-put-together system. It is hypothesized that a well-wired body is more able to respond effectively to environmental insults..." [5]
The System integrity hypothesis posits that childhood intelligence is just one aspect of a well wired and well-functioning body and suggests that there is a latent trait that encompasses intelligence, health and many other factors. [87] [88] This trait indexes how well the body is functioning and how well the body can respond to change and return to a normal balance again (allostatic load). According to the system integrity hypothesis lower IQ does not cause mortality but instead poor system integrity causes lower intelligence and poorer health as well as a range of other traits which can be thought of as markers of system integrity. Professor Ian Deary has proposed that fluctuating asymmetry, speed of information processing, physical co-ordination, physical strength, metabolic syndrome and genetic correlation may be further potential markers of system integrity which by definition should explain a large part of or nullify the relationship between intelligence and mortality.
An opposing theory to the system integrity theory is the evolutionary novelty theory which suggests that those with higher intelligence are better equipped to deal with evolutionary novel events. [89] It is proposed that intelligence evolved to tackle evolutionarily novel situations and that those with a higher IQ are better able to process when such a novel situation is dangerous or a health hazard and thus are likely to be in better health. This theory provides a theoretical background for evidence found that supports the idea that intelligence is related to mortality through health behaviours such as wearing a seatbelt or quitting smoking.
Evolutionary novelty theory emphasises the role of behaviour in the link between mortality and intelligence whereas system integrity emphasis the role of genes. Thus system integrity predicts that individuals of higher intelligence will be better protected from diseases that are caused primarily by genetics whereas evolutionary adaptive theory suggests that individuals of higher intelligence will be better protected from diseases that are less heritable and are caused by poor life choices. One study which tested this idea looked at the incidence of heritable and non-heritable cancers in individuals of differing levels of intelligence. They found that those of higher intelligence were less likely to suffer from cancer that was not heritable, that was based on lifestyle, thus supporting the evolutionary novelty theory. [89] However this was only a preliminary study and only included the disease cancer, which has been found in previous studies to have an ambiguous relationship with intelligence.
Having higher intelligence scores may mean that individuals are better at preventing disease and injury. Their cognitive abilities may equip them with a better propensity for understanding the injury and health risks of certain behaviours and actions.
Fatal and non-fatal accidental injury have been associated with lower intelligence. [90] [91] This may be because individuals of higher intelligence are more likely to take precautions such as wearing seat belts, helmets etc. as they are aware of the risks.
Further there is evidence that more intelligent people behave in a healthier way.
People with higher IQ test scores tend to be less likely to smoke or drink alcohol heavily. They also eat better diets, and they are more physically active. So they have a range of better behaviours that may partly explain their lower mortality risk.
Individuals with higher cognitive abilities are also better equipped for dealing with stress, a factor that has been implemented in many health problems ranging from anxiety to cardiovascular disease. It has been suggested that higher intelligence leads to a better sense of control over one's own life and a reduction in feelings of stress. [92] One study found that individuals with lower intelligence experienced a greater number of functional somatic symptoms, symptoms that cannot be explained by organic pathology and are thought to be stress related. However most of the relationship was mediated by work conditions. [93]
There is evidence that higher intelligence is related to better self-care when one has an illness or injury. One study asked participants to take aspirin or a placebo on a daily basis during a study on cardiovascular health.  Participants with higher intelligence persisted with taking the medication for longer than those with lower intelligence indicating that they could care for themselves better. [94] Studies have shown that individuals with lower intelligence have lower health literacy and a study looking at the link between health literacy and actual health found that it was mediated almost entirely by intelligence. [95] It has been claimed that up to a third of medications are not taken correctly and thus jeopardize the patients' health. This is particularly relevant for those with heart problems as the misuse of some heart medications can actually double the risk of death. [96] More intelligent individuals also make use of preventative healthcare more often for example visiting the doctors. Some have argued however that this is an artefact of higher SES; that those with lower intelligence tend to be from a lower social class and have less access to medical facilities. However it has been found that even when access to healthcare is equal, those with lower intelligence still make less use of the services. [78]
A diagnosis of any mental illness, even mild psychological distress is linked to an increased risk of illness and premature death. The majority of psychiatric illness' are also linked to lower intelligence. [97] Thus it has been proposed that psychiatric morbidity may be another pathway through which intelligence and mortality are related. [98] Despite this the direction of causation between Intelligence and mental health issues has been disputed. Some argue that mental health issues such as depression and schizophrenia may cause a decline in mental functioning and thus scores on intelligence tests whilst others believe that it is lower intelligence that effects likelihood of developing a mental health issue. [99] Although evidence for both points of view has been found, most of the cognitive epidemiological studies are carried out using intelligence scores from childhood, when the psychiatric condition was not present, ensuring that it was not the condition which caused the lower scores. This link has been shown to explain part of the relationship between childhood intelligence and mortality, however the amount of variance explained varies from less than 10 percent to about 5 percent.
Although childhood economic status may be seen as a confounder in the relationship between intelligence and mortality, as it is likely to affect intelligence, it is likely that adult SES mediates the relationship. The idea is that intelligent children will find themselves getting a better education, better jobs and will settle in a safer and healthier environment. They will have better access to health resources, good nutrition and will be less likely to experience the hazards and health risks associated with living in poorer neighbourhoods. Several studies have found that there is an association between adult SES and mortality.
Because of the above-mentioned findings, some researchers have proposed a general factor of fitness analogous to the g-factor for general mental ability/intelligence. This factor is supposed to combine fertility factors, health factors, and the g-factor. For instance, one study found a small but significant correlation between three measures of sperm quality and intelligence. [100] [101] [102]
The term cohort effect is used in social science to describe variations in the characteristics of an area of study (such as the incidence of a characteristic or the age at onset) over time among individuals who are defined by some shared temporal experience or common life experience, such as year of birth, or year of exposure to radiation.
Cohort effects are important to epidemiologists searching for patterns in illnesses.  Certain illnesses may be socially affected via the anticipation phenomenon , and cohort effects can be an indicator of this sort of phenomenon.
Cohort effects are important to resource dependency, and economics theorists when these groups affect structures of influence within their larger organizations. Cohorts in organizations are often defined by entry or birth date, and they retain some common characteristic (size, cohesiveness, competition) that can affect the organization. For example, cohort effects are critical issues in school enrollment.
In order to determine whether a cohort effect is present, a researcher may conduct a cohort study .
A cohort study is a particular form of longitudinal study that samples a cohort (a group of people who share a defining characteristic, typically those who experienced a common event in a selected period, such as birth or graduation), performing a cross-section at intervals through time. It is a type of panel study where the individuals in the panel share a common characteristic.
Cohort studies represent one of the fundamental designs of epidemiology which are used in research in the fields of medicine , nursing , psychology , social science , and in any field reliant on 'difficult to reach' answers that are based on evidence ( statistics ). In medicine for instance, while clinical trials are used primarily for assessing the safety of newly developed pharmaceuticals before they are approved for sale, epidemiological analysis on how risk factors affect the incidence of diseases is often used to identify the causes of diseases in the first place, and to help provide pre-clinical justification for the plausibility of protective factors (treatments).
Cohort studies differ from clinical trials in that no intervention, treatment, or exposure is administered to participants in a cohort design; and no control group is defined. Rather, cohort studies are largely about the life histories of segments of populations, and the individual people who constitute these segments. [1] [2] Exposures or protective factors are identified as preexisting characteristics of participants. The study is controlled by including other common characteristics of the cohort in the statistical analysis. Both exposure/treatment and control variables are measured at baseline. Participants are then followed over time to observe the incidence rate of the disease or outcome in question. Regression analysis can then be used to evaluate the extent to which the exposure or treatment variable contributes to the incidence of the disease, while accounting for other variables that may be at play.
Double-blind randomized controlled trials (RCTs) are generally considered superior methodology in the hierarchy of evidence in treatment, because they allow for the most control over other variables that could affect the outcome, and the randomization and blinding processes reduce bias in the study design. This minimizes the chance that results will be influenced by confounding variables, particularly ones that are unknown. However, educated hypotheses based on prior research and background knowledge are used to select variables to be included in the regression model for cohort studies, and statistical methods can be used to identify and account potential confounders from these variables. Bias can also be mitigated in a cohort study when selecting participants for the cohort. It is also important to note that RCTs may not be suitable in all cases; such as when the outcome is a negative health effect and the exposure is hypothesized to be a risk factor for the outcome. Ethical standards, and morality, would prevent the use of risk factors in RCTs. The natural or incidental exposure to these risk factors (e.g. time spent in the sun), or self-administered exposure (e.g. smoking), can be measured without subjecting participants to risk factors outside of their individual lifestyles, habits, and choices.
Cohort studies can be retrospective (looking back in time, thus using existing data such as medical records or claims database) or prospective (requiring the collection of new data). [3] Retrospective cohort studies restrict the investigators ability to reduce confounding and bias because collected information is restricted to data that already exists. There are advantages to this design however, as retrospective studies are much cheaper and faster because the data has already been collected and stored.
A cohort is a group of people who share a common characteristic or experience within a defined period (e.g., are currently living, are exposed to a drug or vaccine or pollutant, or undergo a certain medical procedure). Thus a group of people who were born on a day or in a particular period, say 1948, form a birth cohort. The comparison group may be the general population from which the cohort is drawn, or it may be another cohort of persons thought to have had little or no exposure to the substance under investigation, but otherwise similar. Alternatively, subgroups within the cohort may be compared with each other.
In medicine, a cohort study is often undertaken to obtain evidence to try to refute the existence of a suspected association between cause and effect; failure to refute a hypothesis often strengthens confidence in it. Crucially, the cohort is identified before the appearance of the disease under investigation. The study groups follow a group of people who do not have the disease for a period of time and see who develops the disease (new incidence). The cohort cannot therefore be defined as a group of people who already have the disease. Prospective (longitudinal) cohort studies between exposure and disease strongly aid in studying causal associations, though distinguishing true causality usually requires further corroboration from further experimental trials.
The advantage of prospective cohort study data is that it can help determine risk factors for contracting a new disease because it is a longitudinal observation of the individual through time, and the collection of data at regular intervals, so recall error is reduced. However, cohort studies are expensive to conduct, are sensitive to attrition and take a long follow-up time to generate useful data. Nevertheless, the results that are obtained from long-term cohort studies are of substantially superior quality to those obtained from retrospective/cross-sectional studies. Prospective cohort studies are considered to yield the most reliable results in observational epidemiology. They enable a wide range of exposure-disease associations to be studied.
Some cohort studies track groups of children from their birth, and record a wide range of information (exposures) about them. The value of a cohort study depends on the researchers' capacity to stay in touch with all members of the cohort. Some studies have continued for decades.
In a cohort study, the population under investigation consists of individuals who are at risk of developing a specific disease or health outcome. [4]
An example of an epidemiological question that can be answered using a cohort study is whether exposure to X (say, smoking) associates with outcome Y (say, lung cancer). For example in 1951, the British Doctors Study was started. Using a cohort which included both smokers (the exposed group) and non-smokers (the unexposed group). The study continued through 2001. By 1956, the study provided convincing proof of the association between smoking and the incidence of lung cancer. In a cohort study, the groups are matched in terms of many other variables such as economic status and other health status so that the variable being assessed, the independent variable (in this case, smoking) can be isolated as the cause of the dependent variable (in this case, lung cancer). In this example, a statistically significant increase in the incidence of lung cancer in the smoking group as compared to the non-smoking group is evidence in favor of the hypothesis. However, rare outcomes, such as lung cancer, are generally not studied with the use of a cohort study, but are rather studied with the use of a case-control study.
Shorter term studies are commonly used in medical research as a form of clinical trial , or means to test a particular hypothesis of clinical importance. Such studies typically follow two groups of patients for a period of time and compare an endpoint or outcome measure between the two groups.
Randomized controlled trials , or RCTs, are a superior methodology in the hierarchy of evidence, because they limit the potential for bias by randomly assigning one patient pool to an intervention and another patient pool to non-intervention (or placebo). This minimizes the chance that the incidence of confounding variables will differ between the two groups. Nevertheless, it is sometimes not practical or ethical to perform RCTs to answer a clinical question. To take our example, if we already had reasonable evidence that smoking causes lung cancer then persuading a pool of non-smokers to take up smoking in order to test this hypothesis would generally be considered quite unethical.
Two examples of cohort studies that have been going on for more than 50 years are the Framingham Heart Study and the National Child Development Study (NCDS), the most widely researched of the British birth cohort studies . Key findings of NCDS and a detailed profile of the study appear in the International Journal of Epidemiology . [5]
The Dunedin Longitudinal Study , started in 1975, has been studying the thousand people born in Dunedin , New Zealand, in 1972–1973. The subjects are interviewed regularly, with Phase 45 starting in 2017.
The largest cohort study in women is the Nurses' Health Study . Started in 1976, it is tracking over 120,000 nurses and has been analyzed for many different conditions and outcomes.
The largest cohort study in Africa is the Birth to Twenty Study, which began in 1990 and tracks a cohort of over 3,000 children born in the weeks following Nelson Mandela 's release from prison.
Other famous examples are the Grant Study tracking a number of Harvard graduates from ca. 1950.77, the Whitehall Study tracking 10,308 British civil servants, and the Caerphilly Heart Disease Study , which since 1979 has studied a representative sample of 2,512 men, drawn from the Welsh town of Caerphilly. [6]
The ASPREE-XT study is designed to determine whether there are long-lasting effects of an average of 4–5 years of treatment with daily low-dose aspirin, with outcome measures including cancer mortality. [7] As of September 2018, there were 16,703 ASPREE-XT participants in Australia. [8] It has been proposed that the existing ASPREE-XT study could provide a platform for a future multigenerational research study. [9]
The diagram indicates the starting point and direction of cohort and case-control studies.
In Case-control studies the analysis proceeds from documented disease and investigations are made to arrive at the possible causes of the disease. In cohort studies the assessments starts with the putative cause of disease, and observations are made of the occurrence of disease relative to the hypothesized causal agent . [10]
A current cohort study represents a true prospective study where the data concerning exposure are assembled prior to the occurrence of the fact to be studied, for instance a disease. An example of a current cohort study is the Oxford Family Planning Association Study in the United Kingdom, which aimed to provide a balanced view of the beneficial and harmful effects of different methods of contraception. This study has provided a large amount of information on the efficacy and safety of contraceptive methods, and in particular oral contraceptives (OCs), diaphragms and intrauterine device (IUDs). [11]
In a historical cohort study the data concerning exposure and occurrence of a disease, births, a political attitude or any other categorical variable are collected after the events have taken place, and the subjects (those exposed and unexposed to the agent under study) are assembled from existing records or health care registers.
A " prospective cohort " defines the groups before the study is done, while historical studies, which are sometimes referred to as " retrospective cohort ", defines the grouping after the data is collected. Examples of a retrospective cohort are Long-Term Mortality after Gastric Bypass Surgery [12] and The Lothian Birth Cohort Studies . [13]
Although historical studies are sometimes referred to as retrospective study, it a misnomer as the methodological principles of historical cohort studies and prospective studies are the same. [10]
A nested case-control study is a case control nested inside of a cohort study. The procedure begins like a normal cohort study, however, as participants develop the outcome of interest they are selected as cases. Once the cases are identified, controls are selected and matched to each case. The process for selecting and matching cases is identical to a normal case control study. An example of a nested case-control study is Inflammatory markers and the risk of coronary heart disease in men and women , which was a case control analyses extracted from the Framingham Heart Study cohort. [14]
Nested case-controls have the advantage of reducing the number of participants that require details follow up or diagnostic testing to assess outcome or exposure status. However, this will also reduce the power of the study, when compared to larger cohort the study population is drawn from.
Household panel surveys are an important sub-type of cohort study. These draw representative samples of households and survey them, following all individuals through time on a usually annual basis. Examples include the US Panel Study of Income Dynamics (since 1968), the German Socio-Economic Panel (since 1984), the British Household Panel Survey (since 1991), the Household, Income and Labour Dynamics in Australia Survey (since 2001) and the European Community Household Panel (1994–2001).
For an example in business analysis, see cohort analysis .
Coinfection is the simultaneous infection of a host by multiple pathogen species . In virology , coinfection includes simultaneous infection of a single cell by two or more virus particles. An example is the coinfection of liver cells with hepatitis B virus and hepatitis D virus , which can arise incrementally by initial infection followed by superinfection .
Global prevalence or incidence of coinfection among humans is unknown, but it is thought to be commonplace, [1] sometimes more common than single infection. [2] Coinfection with helminths affects around 800 million people worldwide. [3]
Coinfection is of particular human health importance because pathogen species can interact within the host. The net effect of coinfection on human health is thought to be negative. [4] Interactions can have either positive or negative effects on other parasites. Under positive parasite interactions, disease transmission and progression are enhanced and this is also known as syndemism . Negative parasite interactions include microbial interference when one bacterial species suppresses the virulence or colonisation of other bacteria, such as Pseudomonas aeruginosa suppressing pathogenic Staphylococcus aureus colony formation. [5] The general patterns of ecological interactions between parasite species are unknown, even among common coinfections such as those between sexually transmitted infections . [6] However, network analysis of a food web of coinfection in humans suggests that there is greater potential for interactions via shared food sources than via the immune system . [7]
A globally common coinfection involves tuberculosis and HIV . In some countries, up to 80% of tuberculosis patients are also HIV-positive. [8] The potential for dynamics of these two infectious diseases to be linked has been known for decades. [9] Other common examples of coinfections are AIDS , which involves coinfection of end-stage HIV with opportunistic parasites [10] and polymicrobial infections like Lyme disease with other diseases. [11] Coinfections sometimes can epitomize a zero sum game of bodily resources, and precise viral quantitation demonstrates children co-infected with rhinovirus and respiratory syncytial virus , metapneumovirus or parainfluenza virus have lower nasal viral loads than those with rhinovirus alone. [12]
Poliovirus is a positive single-stranded RNA virus in the family Picornaviridae .    Coinfections appear to be common and several pathways have been identified for transmitting multiple virions to a single host cell. [13] These include transmission by virion aggregates, transmission of viral genomes within membrane vesicles, and transmission by bacteria bound by several viral particles.
Drake demonstrated that poliovirus is able to undergo multiplicity reactivation. [14] That is, when polioviruses were irradiated with UV light and allowed to undergo multiple infections of host cells, viable progeny could be formed even at UV doses that inactivated the virus in single infections. Poliovirus can undergo genetic recombination when at least two viral genomes are present in the same host cell.  Kirkegaard and Baltimore [15] presented evidence that RNA-dependent RNA polymerase (RdRP) catalyzes recombination by a copy choice mechanism in which the RdRP switches between (+)ssRNA templates during negative strand synthesis.  Recombination in RNA viruses appears to be an adaptive mechanism for transmitting an undamaged genome to virus progeny. [16] [17]
In statistics and causal graphs , a variable is a collider when it is causally influenced by two or more variables. The name "collider" reflects the fact that in graphical models , the arrow heads from variables that lead into the collider appear to "collide" on the node that is the collider. [1] They are sometimes also referred to as inverted forks . [2]
The causal variables influencing the collider are themselves not necessarily associated. If they are not adjacent, the collider is unshielded . Otherwise, the collider is shielded and part of a triangle. [3]
The result of having a collider in the path is that the collider blocks the association between the variables that influence it. [4] [5] [6] Thus, the collider does not generate an unconditional association between the variables that determine it.
Conditioning on the collider via regression analysis , stratification, experimental design, or sample selection based on values of the collider create a non-causal association between X and Y ( Berkson's paradox ). In the terminology of causal graphs, conditioning on the collider opens the path between X and Y . This will introduce bias when estimating the causal association between X and Y , potentially introducing associations where there are none. Colliders can therefore undermine attempts to test causal theories.
Colliders are sometimes confused with confounder variables. Unlike colliders, confounder variables should be controlled for when estimating causal associations.
Compartmental models simplify the mathematical modelling of infectious diseases . The population is assigned to compartments with labels – for example, S , I ,  or R , ( S usceptible, I nfectious, or R ecovered). People may progress between compartments. The order of the labels usually shows the flow patterns between the compartments; for example SEIS means susceptible, exposed, infectious, then susceptible again.
The origin of such models is the early 20th century, with important works being that of Ross [1] in 1916, Ross and Hudson in 1917, [2] [3] Kermack and McKendrick in 1927 [4] and Kendall in 1956 [5]
The models are most often run with ordinary differential equations (which are deterministic), but can also be used with a stochastic (random) framework, which is more realistic but much more complicated to analyze.
Models try to predict things such as how a disease spreads, or the total number infected, or the duration of an epidemic, and to estimate various epidemiological parameters such as the reproductive number . Such models can show how different public health interventions may affect the outcome of the epidemic, e.g., what the most efficient technique is for issuing a limited number of vaccines in a given population.
The SIR model [6] [7] [8] [9] is one of the simplest compartmental models, and many models are derivatives of this basic form. The model consists of three compartments:-
This model is reasonably predictive [10] for infectious diseases that are transmitted from human to human, and where recovery confers lasting resistance, such as measles , mumps and rubella .
These variables ( S , I , and R ) represent the number of people in each compartment at a particular time. To represent that the number of susceptible, infectious and removed individuals may vary over time (even if the total population size remains constant), we make the precise numbers a function of t (time): S ( t ), I ( t ) and R ( t ). For a specific disease in a specific population, these functions may be worked out in order to predict possible outbreaks and bring them under control. [10]
As implied by the variable function of t , the model is dynamic in that the numbers in each compartment may fluctuate over time. The importance of this dynamic aspect is most obvious in an endemic disease with a short infectious period, such as measles in the UK prior to the introduction of a vaccine in 1968. Such diseases tend to occur in cycles of outbreaks due to the variation in number of susceptibles (S( t )) over time. During an epidemic , the number of susceptible individuals falls rapidly as more of them are infected and thus enter the infectious and removed compartments. The disease cannot break out again until the number of susceptibles has built back up, e.g. as a result of offspring being born into the susceptible compartment.
Each member of the population typically progresses from susceptible to infectious to recovered. This can be shown as a flow diagram in which the boxes represent the different compartments and the arrows the transition between compartments, i.e.
For the full specification of the model, the arrows should be labeled with the transition rates between compartments. Between S and I , the transition rate is assumed to be d(S/N)/dt = -βSI/N 2 , where N is the total population, β is the average number of contacts per person per time, multiplied by the probability of disease transmission in a contact between a susceptible and an infectious subject, and SI/N 2 is the fraction of those contacts between an infectious and susceptible individual which result in the susceptible person becoming infected. (This is mathematically similar to the law of mass action in chemistry in which random collisions between molecules result in a chemical reaction and the fractional rate is proportional to the concentration of the two reactants).
Between I and R , the transition rate is assumed to be proportional to the number of infectious individuals which is γ I . This is equivalent to assuming that the probability of an infectious individual recovering in any time interval dt is simply γ dt . If an individual is infectious for an average time period D , then γ = 1/ D . This is also equivalent to the assumption that the length of time spent by an individual in the infectious state is a random variable with an exponential distribution . The "classical" SIR model may be modified by using more complex and realistic distributions for the I-R transition rate (e.g. the Erlang distribution [11] ).
For the special case in which there is no removal from the infectious compartment (γ=0), the SIR model reduces to a very simple SI model, which has a logistic solution, in which every individual eventually becomes infected.
The dynamics of an epidemic, for example, the flu , are often much faster than the dynamics of birth and death, therefore, birth and death are often omitted in simple compartmental models.  The SIR system without so-called vital dynamics (birth and death, sometimes called demography) described above can be expressed by the following set of ordinary differential equations : [7] [12]
where S {\displaystyle S} is the stock of susceptible population, I {\displaystyle I} is the stock of infected, R {\displaystyle R} is the stock of removed population (either by death or recovery), and N {\displaystyle N} is the sum of these three.
This model was for the first time proposed by William Ogilvy Kermack and Anderson Gray McKendrick as a special case of what we now call Kermack–McKendrick theory , and followed work McKendrick had done with Ronald Ross .
This system is non-linear , however it is possible to derive its analytic solution in implicit form. [6] Firstly note that from:
it follows that:
expressing in mathematical terms the constancy of population N {\displaystyle N} . Note that the above relationship implies that one need only study the equation for two of the three variables.
Secondly, we note that the dynamics of the infectious class depends on the following ratio:
the so-called basic reproduction number (also called basic reproduction ratio). This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections) from a single infection in a population where all subjects are susceptible. [13] [14] This idea can probably be more readily seen if we say that the typical time between contacts is T c = β − 1 {\displaystyle T_{c}=\beta ^{-1}} , and the typical time until removal is T r = γ − 1 {\displaystyle T_{r}=\gamma ^{-1}} . From here it follows that, on average, the number of contacts by an infectious individual with others before the infectious has been removed is: T r / T c . {\displaystyle T_{r}/T_{c}.}
By dividing the first differential equation by the third, separating the variables and integrating we get
where S ( 0 ) {\displaystyle S(0)} and R ( 0 ) {\displaystyle R(0)} are the initial numbers of, respectively, susceptible and removed subjects. 
Writing s 0 = S ( 0 ) / N {\displaystyle s_{0}=S(0)/N} for the initial proportion of susceptible individuals, and s ∞ = S ( ∞ ) / N {\displaystyle s_{\infty }=S(\infty )/N} and r ∞ = R ( ∞ ) / N {\displaystyle r_{\infty }=R(\infty )/N} for the proportion of susceptible and removed individuals respectively
in the limit t → ∞ , {\displaystyle t\to \infty ,} one has
(note that the infectious compartment empties in this limit).
This transcendental equation has a solution in terms of the Lambert W function , [15] namely
This shows that at the end of an epidemic that conforms to the simple assumptions of the SIR model, unless s 0 = 0 {\displaystyle s_{0}=0} , not all individuals of the population have been removed, so some must remain susceptible. A driving force leading to the end of an epidemic is a decline in the number of infectious individuals. The epidemic does not typically end because of a complete lack of susceptible individuals.
The role of both the basic reproduction number and the initial susceptibility are extremely important. In fact, upon rewriting the equation for infectious individuals as follows:
it yields that if:
then:
i.e., there will be a proper epidemic outbreak with an increase of the number of the  infectious (which can reach a considerable fraction of the population). On the contrary, if
then
i.e., independently from the initial size of the susceptible population the disease can never cause a proper epidemic outbreak. As a consequence, it is clear that both the basic reproduction number and the initial susceptibility are extremely important.
Note that in the above model the function:
models the transition rate from the compartment of susceptible individuals to the compartment of infectious individuals, so that it is called the force of infection . However, for large classes of communicable diseases it is more realistic to consider a force of infection that does not depend on the absolute number of infectious subjects, but on their fraction (with respect to the total constant population N {\displaystyle N} ):
Capasso [16] and, afterwards, other authors have proposed nonlinear forces of infection to model more realistically the contagion process.
In 2014, Harko and coauthors derived an exact so-called analytical solution (involving an integral that can only be calculated numerically) to the SIR model. [6] In the case  without vital dynamics setup, for S ( u ) = S ( t ) {\displaystyle {\mathcal {S}}(u)=S(t)} , etc., it corresponds to the following time parametrization
for
with initial conditions
where u T {\displaystyle u_{T}} satisfies I ( u T ) = 0 {\displaystyle {\mathcal {I}}(u_{T})=0} . By the transcendental equation for R ∞ {\displaystyle R_{\infty }} above, it follows that u T = e − ( R ∞ − R ( 0 ) ) / ρ ( = S ∞ / S ( 0 ) {\displaystyle u_{T}=e^{-(R_{\infty }-R(0))/\rho }(=S_{\infty }/S(0)} , if S ( 0 ) ≠ 0 ) {\displaystyle S(0)\neq 0)} and I ∞ = 0 {\displaystyle I_{\infty }=0} .
An equivalent so-called analytical solution (involving an integral that can only be calculated numerically) found by Miller [17] [18] yields
Here ξ ( t ) {\displaystyle \xi (t)} can be interpreted as the expected number of transmissions an individual has received by time t {\displaystyle t} .  The two solutions are related by e − ξ ( t ) = u {\displaystyle e^{-\xi (t)}=u} .
Effectively the same result can be found in the original work by Kermack and McKendrick. [4]
These solutions may be easily understood by noting that all of the terms on the right-hand sides of the original differential equations are proportional to I {\displaystyle I} . The equations may thus be divided through by I {\displaystyle I} , and the time rescaled so that the differential operator on the left-hand side becomes simply d / d τ {\displaystyle d/d\tau } , where d τ = I d t {\displaystyle d\tau =Idt} , i.e. τ = ∫ I d t {\displaystyle \tau =\int Idt} . The differential equations are now all linear, and the third equation, of the form d R / d τ = {\displaystyle dR/d\tau =} const., shows that τ {\displaystyle \tau } and R {\displaystyle R} (and ξ {\displaystyle \xi } above) are simply linearly related.
A highly accurate analytic approximant of the SIR model as well as exact analytic expressions for the final values S ∞ {\displaystyle S_{\infty }} , I ∞ {\displaystyle I_{\infty }} , and R ∞ {\displaystyle R_{\infty }} were provided by Kröger and Schlickeiser, [8] so that there is no need to perform a numerical integration to solve the SIR model, to obtain its parameters from existing data, or to predict the future dynamics of an epidemics modeled by the SIR model. The approximant involves the Lambert W function which is part of all basic data visualization software such as Microsoft Excel , MATLAB , and Mathematica .
While Kendall [5] considered the so-called all-time SIR model where the initial conditions S ( 0 ) {\displaystyle S(0)} , I ( 0 ) {\displaystyle I(0)} , and R ( 0 ) {\displaystyle R(0)} are coupled through the above relations, Kermack and McKendrick [4] proposed to study the more general semi-time case, for which S ( 0 ) {\displaystyle S(0)} and I ( 0 ) {\displaystyle I(0)} are both arbitrary. This latter version, denoted as semi-time SIR model, [8] makes predictions only for future times t > 0 {\displaystyle t>0} . An analytic approximant and exact expressions for the final values are available for the semi-time SIR model as well. [9]

Consider a population characterized by a death rate μ {\displaystyle \mu } and birth rate Λ {\displaystyle \Lambda } , and where a communicable disease is spreading. [7] The model with mass-action transmission is:
for which the disease-free equilibrium (DFE) is:
In this case, we can derive a basic reproduction number :
which has threshold properties. In fact, independently from biologically meaningful initial values, one can show that:
The point EE is called the Endemic Equilibrium (the disease is not totally eradicated and remains in the population). With heuristic arguments, one may show that R 0 {\displaystyle R_{0}} may be read as the average number of infections caused by a single infectious subject in a wholly susceptible population, the above relationship biologically means that if this number is less than or equal to one the disease goes extinct, whereas if this number is greater than one the disease will remain permanently endemic in the population.
In 1927, W. O. Kermack and A. G. McKendrick created a model in which they considered a fixed population with only three compartments: susceptible, S ( t ) {\displaystyle S(t)} ; infected, I ( t ) {\displaystyle I(t)} ; and recovered, R ( t ) {\displaystyle R(t)} . The compartments used for this model consist of three classes: [4]
The flow of this model may be considered as follows:
Using a fixed population, N = S ( t ) + I ( t ) + R ( t ) {\displaystyle N=S(t)+I(t)+R(t)} in the three functions resolves that the value N {\displaystyle N} should remain constant within the simulation, if a simulation is used to solve the SIR model. Alternatively, the analytic approximant [8] can be used without performing a simulation. The model is started with values of S ( t = 0 ) {\displaystyle S(t=0)} , I ( t = 0 ) {\displaystyle I(t=0)} and R ( t = 0 ) {\displaystyle R(t=0)} . These are the number of people in the susceptible, infected and removed categories at time equals zero. If the SIR model is assumed to hold at all times, these initial conditions are not independent. [8] Subsequently, the flow model updates the three variables for every time point with set values for β {\displaystyle \beta } and γ {\displaystyle \gamma } . The simulation first updates the infected from the susceptible and then the removed category is updated from the infected category for the next time point (t=1). This describes the flow persons between the three categories. During an epidemic the susceptible category is not shifted with this model, β {\displaystyle \beta } changes over the course of the epidemic and so does γ {\displaystyle \gamma } . These variables determine the length of the epidemic and would have to be updated with each cycle.
Several assumptions were made in the formulation of these equations: First, an individual in the population must be considered as having an equal probability as every other individual of contracting the disease with a rate of a {\displaystyle a} and an equal fraction b {\displaystyle b} of people that an individual makes contact with per unit time. Then, let β {\displaystyle \beta } be the multiplication of a {\displaystyle a} and b {\displaystyle b} . This is the transmission probability times the contact rate. Besides, an infected individual makes contact with b {\displaystyle b} persons per unit time whereas only a fraction, S N {\displaystyle {\frac {S}{N}}} of them are susceptible.Thus, we have every infective can infect a b S = β S {\displaystyle abS=\beta S} susceptible persons, and therefore, the whole number of susceptibles infected by infectives per unit time is β S I {\displaystyle \beta SI} . For the second and third equations, consider the population leaving the susceptible class as equal to the number entering the infected class. However, a number equal to the fraction γ {\displaystyle \gamma } (which represents the mean recovery/death rate, or 1 / γ {\displaystyle 1/\gamma } the mean infective period) of infectives are leaving this class per unit time to enter the removed class. These processes which occur simultaneously are referred to as the Law of Mass Action, a widely accepted idea that the rate of contact between two groups in a population is proportional to the size of each of the groups concerned. Finally, it is assumed that the rate of infection and recovery is much faster than the time scale of births and deaths and therefore, these factors are ignored in this model. [19]
The expected duration of susceptibility will be E ⁡ [ min ( T L ∣ T S ) ] {\displaystyle \operatorname {E} [\min(T_{L}\mid T_{S})]} where T L {\displaystyle T_{L}} reflects the time alive (life expectancy) and T S {\displaystyle T_{S}} reflects the time in the susceptible state before becoming infected, which can be simplified [20] to:
such that the number of susceptible persons is the number entering the susceptible compartment μ N {\displaystyle \mu N} times the duration of susceptibility:
Analogously, the steady-state number of infected persons is the number entering the infected state from the susceptible state (number susceptible, times rate of infection λ = β I N , {\displaystyle \lambda ={\tfrac {\beta I}{N}},} times the duration of infectiousness 1 μ + v {\displaystyle {\tfrac {1}{\mu +v}}} :
There are many modifications of the SIR model, including those that include births and deaths, where upon recovery there is no immunity (SIS model), where immunity lasts only for a short period of time (SIRS), where there is a latent period of the disease where the person is not infectious ( SEIS and SEIR ), and where infants can be born with immunity (MSIR).
Some infections, for example, those from the common cold and influenza , do not confer any long-lasting immunity. Such infections do not give immunity upon recovery from infection, and individuals become susceptible again.
We have the model:
Note that denoting with N the total population it holds that:
It follows that:
i.e. the dynamics of infectious is ruled by a logistic function , so that ∀ I ( 0 ) > 0 {\displaystyle \forall I(0)>0} :
It is possible to find an analytical solution to this model (by making a transformation of variables: I = y − 1 {\displaystyle I=y^{-1}} and substituting this into the mean-field equations), [21] such that the basic reproduction rate is greater than unity. The solution is given as
where I ∞ = ( 1 − γ / β ) N {\displaystyle I_{\infty }=(1-\gamma /\beta )N} is the endemic infectious population, χ = β − γ {\displaystyle \chi =\beta -\gamma } , and V = I ∞ / I 0 − 1 {\displaystyle V=I_{\infty }/I_{0}-1} . As the system is assumed to be closed, the susceptible population is then S ( t ) = N − I ( t ) {\displaystyle S(t)=N-I(t)} .
As a special case, one obtains the usual logistic function by assuming γ = 0 {\displaystyle \gamma =0} . This can be also considered in the SIR model with R = 0 {\displaystyle R=0} , i.e. no removal will take place. That is the SI model . [22] The differential equation system using S = N − I {\displaystyle S=N-I} thus reduces to:
In the long run, in the SI model, all individuals will become infected.
For evaluating the epidemic threshold in the SIS model on networks see Parshani et al. [23]
The Susceptible-Infectious-Recovered-Deceased model differentiates between Recovered (meaning specifically individuals having survived the disease and now immune) and Deceased . [13] This model uses the following system of differential equations:
where β , γ , μ {\displaystyle \beta ,\gamma ,\mu } are the rates of infection, recovery, and mortality, respectively. [24]
For many infections, including measles , babies are not born into the susceptible compartment but are immune to the disease for the first few months of life due to protection from maternal antibodies (passed across the placenta and additionally through colostrum ). This is called passive immunity . This added detail can be shown by including an M class (for maternally derived immunity) at the beginning of the model.
To indicate this mathematically, an additional compartment is added, M ( t ) . This results in the following differential equations:
Some people who have had an infectious disease such as tuberculosis never completely recover and continue to carry the infection, whilst not suffering the disease themselves. They may then move back into the infectious compartment and suffer symptoms (as in tuberculosis) or they may continue to infect others in their carrier state, while not suffering symptoms. The most famous example of this is probably Mary Mallon , who infected 22 people with typhoid fever . The carrier compartment is labelled C.

For many important infections, there is a significant latency period during which individuals have been infected but are not yet infectious themselves. During this period the individual is in compartment E (for exposed).
Assuming that the latency period is a random variable with exponential distribution with parameter a {\displaystyle a} (i.e. the average latency period is a − 1 {\displaystyle a^{-1}} ), and also assuming the presence of vital dynamics with birth rate Λ {\displaystyle \Lambda } equal to death rate N μ {\displaystyle N\mu } (so that the total number N {\displaystyle N} is constant), we have the model:
We have S + E + I + R = N , {\displaystyle S+E+I+R=N,} but this is only constant because of the simplifying assumption that birth and death rates are equal; in general N {\displaystyle N} is a variable.
For this model, the basic reproduction number is:
Similarly to the SIR model, also, in this case, we have a Disease-Free-Equilibrium ( N ,0,0,0) and an Endemic Equilibrium EE, and one can show that, independently from biologically meaningful initial conditions
it holds that:
In case of periodically varying contact rate β ( t ) {\displaystyle \beta (t)} the condition for the global attractiveness of DFE is that the following linear system with periodic coefficients:
is stable (i.e. it has its Floquet's eigenvalues inside the unit circle in the complex plane).
The SEIS model is like the SEIR model (above) except that no immunity is acquired at the end.
In this model an infection does not leave any immunity thus individuals that have recovered return to being susceptible, moving back into the S ( t ) compartment.  The following differential equations describe this model:
For the case of a disease, with the factors of passive immunity, and a latency period there is the MSEIR model.
An MSEIRS model is similar to the MSEIR, but the immunity in the R class would be temporary, so that individuals would regain their susceptibility when the temporary immunity ended.
It is well known that the probability of getting a disease is not constant in time. As a pandemic progresses, reactions to the pandemic may change the contact rates which are assumed constant in the simpler models. Counter-measures such as masks, social distancing and lockdown will alter the contact rate in a way to reduce the speed of the pandemic.
In addition, Some diseases are seasonal, such as the common cold viruses, which are more prevalent during winter.  With childhood diseases, such as measles, mumps, and rubella, there is a strong correlation with the school calendar, so that during the school holidays the probability of getting such a disease dramatically decreases. As a consequence, for many classes of diseases, one should consider a force of infection with periodically ('seasonal') varying contact rate
with period T equal to one year.
Thus, our model becomes
(the dynamics of recovered easily follows from R = N − S − I {\displaystyle R=N-S-I} ), i.e. a nonlinear set of differential equations with periodically varying parameters. It is well known that this class of dynamical systems may undergo very interesting and complex phenomena of  nonlinear parametric resonance. It is easy to see that if:
whereas if the integral is greater than one the disease will not die out and there may be such resonances. For example, considering the periodically varying contact rate as the 'input' of the system one has that the output is a periodic function whose period is a multiple of the period of the input.
This allowed to give a contribution to explain the poly-annual (typically biennial) epidemic outbreaks of some infectious diseases as interplay between the period of the contact rate oscillations and the pseudo-period of the damped oscillations near the endemic equilibrium. Remarkably, in some cases, the behavior may also be quasi-periodic or even chaotic.
Spatiotemporal compartmental models describe not the total number, but the density of susceptible/infective/recovered persons. Consequently, they also allow to model the distribution of infected persons in space. In most cases, this is done by combining the SIR model with a diffusion equation
where D S {\displaystyle D_{S}} , D I {\displaystyle D_{I}} and D R {\displaystyle D_{R}} are diffusion constants. Thereby, one obtains a reaction-diffusion equation. (Note that, for dimensional reasons, the parameter β {\displaystyle \beta } has to be changed compared to the simple SIR model.) Early models of this type have been used to model the spread of the black death in Europe. [25] Extensions of this model have been used to incorporate, e.g., effects of nonpharmaceutical interventions such as social distancing. [26]
An SIR community based model to assess the probability for a worldwide spreading and declare pandemic has been recently developed by Valdez et al. [27]
The SIR model can be modified to model vaccination. [28] Typically these introduce an additional compartment to the SIR model, V {\displaystyle V} , for vaccinated individuals. Below are some examples.
In presence of a communicable diseases, one of main tasks is that of eradicating it via  prevention measures and, if possible, via the establishment of a mass vaccination program. Consider a disease for which the newborn are vaccinated (with a vaccine giving lifelong immunity) at a rate P ∈ ( 0 , 1 ) {\displaystyle P\in (0,1)} :
where V {\displaystyle V} is the class of vaccinated subjects. It is immediate to show that:
thus we shall deal with the long term behavior of S {\displaystyle S} and I {\displaystyle I} , for which it holds that:
In other words, if
the vaccination program is not successful in eradicating the disease, on the contrary, it will remain endemic, although at lower levels than the case of absence of vaccinations. This means that the mathematical model suggests that for a disease whose basic reproduction number may be as high as 18 one should vaccinate at least 94.4% of newborns in order to eradicate the disease.
Modern societies are facing the challenge of "rational" exemption, i.e. the family's decision to not vaccinate children as a consequence of a "rational" comparison between the perceived risk from infection and that from getting damages from the vaccine. In order to assess whether this behavior is really rational, i.e. if it can equally lead to the eradication of the disease,  one may simply assume that the vaccination rate is an increasing function of the number of infectious subjects:
In such a case the eradication condition becomes:
i.e. the baseline vaccination rate should be greater than the "mandatory vaccination" threshold, which, in case of exemption, cannot hold. Thus, "rational" exemption might be myopic since it is based only on the current low incidence due to high vaccine coverage, instead taking into account future resurgence of infection due to coverage decline.
In case there also are vaccinations of non newborns at a rate ρ the equation for the susceptible and vaccinated subject has to be modified as follows:
leading to the following eradication condition:
This strategy repeatedly vaccinates a defined age-cohort (such as young children or the elderly) in a susceptible population over time. Using this strategy, the block of susceptible individuals is then immediately removed, making it possible to eliminate an infectious disease, (such as measles), from the entire population. Every T time units a constant fraction p of susceptible subjects is vaccinated in a relatively short (with respect to the dynamics of the disease) time. This leads to the following impulsive differential equations for the susceptible and vaccinated subjects:
It is easy to see that by setting I = 0 one obtains that the dynamics of the susceptible subjects is given by:
and that the eradication condition is:
Age has a deep influence on the disease spread rate in a population, especially the contact rate. This rate summarizes the effectiveness of contacts between susceptible and infectious subjects. Taking into account the ages of the epidemic classes s ( t , a ) , i ( t , a ) , r ( t , a ) {\displaystyle s(t,a),i(t,a),r(t,a)} (to limit ourselves to the susceptible-infectious-removed scheme) such that:
(where a M ≤ + ∞ {\displaystyle a_{M}\leq +\infty } is the maximum admissible age) and their dynamics is not described, as one might think, by "simple" partial differential equations, but by integro-differential equations :
where:
is the force of infection, which, of course, will depend, though the contact kernel k ( a , a 1 ; t ) {\displaystyle k(a,a_{1};t)} on the interactions between the ages.
Complexity is added by the initial conditions for newborns (i.e. for a=0), that are straightforward for infectious and removed:
but that are nonlocal for the density of susceptible newborns:
where φ j ( a ) , j = s , i , r {\displaystyle \varphi _{j}(a),j=s,i,r} are the fertilities of the adults.
Moreover, defining now the density of the total population n ( t , a ) = s ( t , a ) + i ( t , a ) + r ( t , a ) {\displaystyle n(t,a)=s(t,a)+i(t,a)+r(t,a)} one obtains:
In the simplest case of equal fertilities in the three epidemic classes, we have that in order to have demographic equilibrium the following necessary and sufficient condition linking the fertility φ ( . ) {\displaystyle \varphi (.)} with the mortality μ ( a ) {\displaystyle \mu (a)} must hold:
and the demographic equilibrium is
automatically ensuring the existence of the disease-free solution:
A basic reproduction number can be calculated as the spectral radius of an appropriate functional operator.
In the case of some diseases such as AIDS and Hepatitis B, it is possible for the offspring of infected parents to be born infected.  This transmission of the disease down from the mother is called Vertical Transmission.  The influx of additional members into the infected category can be considered within the model by including a fraction of the newborn members in the infected compartment. [29]
Diseases transmitted from human to human indirectly, i.e. malaria spread by way of mosquitoes, are transmitted through a vector.  In these cases, the infection transfers from human to insect and an epidemic model must include both species, generally requiring many more compartments than a model for direct transmission. [29] [30]
Other occurrences which may need to be considered when modeling an epidemic include things such as the following: [29]
It is important to stress that the deterministic models presented here are valid only in case of sufficiently large populations, and as such should be used cautiously. [31]
To be more precise, these models are only valid in the thermodynamic limit , where the population is effectively infinite. In stochastic models, the long-time endemic equilibrium derived above, does not hold, as there is a finite probability that the number of infected individuals drops below one in a system. In a true system then, the pathogen may not propagate, as no host will be infected. But, in deterministic mean-field models, the number of infected can take on real, namely, non-integer values of infected hosts, and the number of hosts in the model can be less than one, but more than zero, thereby allowing the pathogen in the model to propagate. The reliability of compartmental models is limited to compartmental applications.
One of the possible extensions of mean-field models considers the spreading of epidemics on a network based on percolation theory concepts. [32] Stochastic epidemic models have been studied on different networks [33] [34] [35] and more recently applied to the COVID-19 pandemic . [36]
A method for efficient vaccination approach, via vaccinating a small fraction population called acquaintance immunization has been developed by Cohen et al. [37] An alternative method based on identifying and vaccination mainly spreaders has been developed by Liu et al [38]
Complex segregation analysis ( CSA ) is a technique within genetic epidemiology to determine whether there is evidence that a major gene underlies the distribution of a given phenotypic trait.  CSA also provides evidence to whether the implicated trait is inherited in a Mendelian dominant , recessive , or codominant manner.
CSA is often a preliminary step in genetic epidemiology. The purpose of CSA is to provide initial evidence that a single gene has a major effect on a particular phenotypic trait. Only phenotypic information, not genotypic information, is required for CSA.  CSA can provide evidence, but not definitively prove a trait is under the control of a single gene. Evidence from CSA studies can be used to justify which phenotypes might be appropriate for more in-depth studies such as linkage analysis. [1]
CSA requires phenotypic information on family members in a pedigree .  A variety of models with different parameters and assumptions about the nature of the inheritance of the trait are fit to the data.  CSA studies may include non-genetic models which assume the trait has no genetic component and is only determined by environmental factors, models which include environmental components as well as multi-gene heritability components, and models which include environment, multi-gene heritability, and a single major gene to best fit the data. [2] CSA software uses a maximum likelihood estimator to assign the best fitting coefficients to each component in all models.  Nested models are then tested for their goodness of fit starting at the most complex.  If two models are found to fit equally well, the more complex model is rejected in favor of the simpler model.  If the best fitting model includes a single major gene component, there is evidence that the trait of interest is under Mendelian control.
Complex systems biology ( CSB ) is a branch or subfield of mathematical and theoretical biology invented by Robert Rosen concerned with complexity of both structure and function in biological organisms, as well as the emergence and evolution of organisms and species, with emphasis being placed on the interconnectivity of, and within, biological network inference , [1] [2] [3] and on modelling the fundamental relations inherent to life. [4]
According to Baianu et al .  CSB is a field that has only a partial overlap with the more conventional concepts of complex systems theory and systems biology , because CSB is concerned with philosophy and human consciousness. Moreover, mathematics can model a wide range of complex systems , but this is claimed not to be relevant. [5]
There is no satisfying definition of complexity in biology. [6] [7]
According to Rosen, most complex system models are not about biological subjects, such as genomes and organisms, [3] [8] although biology has been successfully modelled since the 19th century. [9] [10]
Two approaches based on information theory and network topology / graph theory have been combined to model human consciousness . [5] [11]
Regarding ontology (the philosophical metaphysics of reality), one is assuming that there is a hierarchy of complexity levels of organization distinct from reality. [5] [12] [13]
Baianu et al . claim that taxonomic ranks such as order , family , genus , species , etc. reflect a hierarchy of complexity levels of organization in biology. [5]
Because of their variability, ability to heal and self-reproduce, and so on, organisms are defined as ' meta-systems ' of simpler systems in CSB. [5] [14] A meta-system is a system of systems. [14] Autopoiesis also models such a biological system of systems, [15] however, Baianu et al . claim that in biology a meta-system is not equivalent to such a system of systems. [5] They also claim it differs from the meta-system defined in a blog by Kent D. Palmer [16] [17] because organisms are different from machines or robots. [5] If, according to Hopcroft et al ., robots or automata can be defined by five qualities: states, startup state, input and output sets/alphabet and transition function, [18] organisms are different. [5]
The following is a list of topics covered in complex systems biology:
Computational epidemiology is a multidisciplinary field that uses techniques from computer science , mathematics , geographic information science and public health to better understand issues central to epidemiology such as the spread of diseases or the effectiveness of a public health intervention.
In contrast with traditional epidemiology , computational epidemiology looks for patterns in unstructured sources of data, such as social media. It can be thought of as the hypothesis-generating antecedent to hypothesis-testing methods such as national surveys and randomized controlled trials.
A mathematical model is developed which describes the observed behavior of the viruses, based on the available data. Then simulations of the model are performed to understand the possible outcomes given the model used. These simulations produce as results projections which can then be used to make predictions or verify the facts and then be used to plan interventions and meters towards the control of the disease's spread.

This medical article is a stub . You can help Wikipedia by expanding it .
The emerging field of conflict epidemiology offers a more accurate method to measure deaths caused during violent conflicts or wars that can generate more reliable numbers than before to guide decision-makers.
In February 2001 the Carter Center and the United States Institute of Peace (USIP), in collaboration with CARE (relief) , Emory University and the Centers for Disease Control and Prevention (CDC), sponsored a meeting on "Violence and Health". The goals of the meeting were to determine the impact of violent conflict on public health and to advise public health training programs on means to enhance the work of public health professionals in working in violent conflicts.
Compiling or estimating the numbers of deaths caused during wars and other violent conflicts is a controversial subject.  Historians often put forward many different estimates of the numbers killed during historic conflicts.  What conflict epidemiology offers is a better methodology to more accurately estimate actual mortality rates during existing wars and conflict.
As war is a leading cause of illness and death, there are those in the field of public health who argue "war epidemiology" should be a more prominent component of the field of public health. [1]
Clarence C Tam et al. provide a conceptual framework [2] for conflict epidemiology, listing the following public health effects stemming from conflicts.
The subject of conflict epidemiology made headline news after a report of a survey was conducted by an American and Iraqi team of public health researchers. Data were collected by local Iraqi doctors and analysed by the faculty of the Johns Hopkins School of Public Health .
The Connecting Organizations for Regional Disease Surveillance (CORDS) is a "regional infectious disease surveillance network that neighboring countries worldwide are organizing to control cross-border outbreaks at their source." [1] [2] In 2012, CORDS was registered as a legal, non-profit international organization in Lyon, France . [1] As of 2021, CORDS was composed of "six regional member networks, working in 28 countries in Africa , Asia , the Middle East and Europe ." [3]
CORDS are "distinct from more formal networks in geographic regions designated by the World Health Organization (WHO)... Some of these regional networks existed before the sudden 2003 outbreak of SARS ," [1] for example:
The CORDS grew out of the 1960s-era Organisations de Coordination et de Cooperation pour la lutte contre les Grandes Endemies (OCCGE) which was an African network, reformed in 1987 to add the West African Health Community (WAHC) and give birth to the West African Health Organisation (WAHO). [1]
The PPHSN was formed in 1996 in order to "streamline" members' "disease reporting and response". In 1997, the PPHSN set up PacNet , in order to "share timely information on disease outbreaks" and "to ensure appropriate action was taken in response to public health threats." [1]
In 2000, the Global Outbreak Alert and Response Network was formalized by the WHO. [1]
In 2001, was formed the Southeastern European Health Network (SEEHN) which grouped the governments of Albania , Bosnia and Herzegovina , Bulgaria , Croatia , Moldova , Montenegro , Romania , and the Former Yugoslav Republic of Macedonia . [1]
In 2003, Israel , Jordan and the Palestinian Authority established the Middle East Consortium on Infectious Disease Surveillance (MECIDS). [1]
The growth of the CORDS can be categorised into several overlapping phases: [1]
In 2005, the International Health Regulations (IHR) mandated official reporting of certain types of disease outbreaks to WHO. [1]
In 2007, the Rockefeller Foundation (RF) used funds from the Nuclear Threat Initiative (NTI) to convene in Bellagio "regional surveillance networks from across the globe to initiate a dialogue about how to harness lessons learned, emerging technologies, and nascent support." [1] In 2009 the RF used funds from NTI to "create a community of practice" named CORDS, [1] [4] which in 2012 was concretized in Lyon France as a legal, non-profit international organization. [1] CORDS convened the 1st Global Conference on Regional Disease Surveillance Networks at the Prince Mahidol Award Conference in 2013. [1]
Contact immunity is the property of some vaccines , where a vaccinated individual can confer immunity upon unimmunized individuals through contact with bodily fluids or excrement.  In other words, if person “A” has been vaccinated for virus X and person “B” has not, person “B” can receive immunity to virus X just by coming into contact with person “A”. The term was coined by Romanian physician Ioan Cantacuzino .
The potential for contact immunity exists primarily in "live" or attenuated vaccines . Vaccination with a live, but attenuated, virus can produce immunity to more dangerous forms of the virus. These attenuated viruses produce little or no illness in most people. However, the live virus multiplies briefly, may be shed in body fluids or excrement , and can be contracted by another person. If this contact produces immunity and carries no notable risk, it benefits an additional person, and further increases the immunity of the group.
The most prominent example of contact immunity was the oral polio vaccine (OPV).  This live, attenuated polio vaccine was widely used in the US between 1960 and 1990; it continues to be used in polio eradication programs in developing countries because of its low cost and ease of administration. It is popular, in part, because it is capable of contact immunity. Recently immunized children "shed" live virus in their feces for a few days after immunization . About 25 percent of people coming into contact with someone immunized with OPV gained protection from polio through this form of contact immunity. [1] Although contact immunity is an advantage of OPV, the risk of vaccine-associated paralytic poliomyelitis —affecting 1 child per 2.4 million OPV doses administered—led the Centers for Disease Control and Prevention (CDC) to cease recommending its use in the US as of January 1, 2010, in favor of inactivated poliovirus vaccine (IPV).  The CDC continues to recommend OPV over IPV for global polio eradication activities. [2]
The main drawback of live virus–based vaccines is that a few people who are vaccinated or exposed to those who have been vaccinated may develop severe disease. Those with defective immune function are the most vulnerable. In the case of OPV, an average of eight to nine adults contracted paralytic polio from contact with a recently immunized child each year. As the risk of catching polio in the Western Hemisphere diminished, the risk of contact infection with the attenuated polio virus outweighed the advantages of OPV, leading the CDC to recommend its discontinuation. [3]
Contact immunity differs from herd immunity , a different type of group protection, in which risk for unimmunized individuals is reduced if they are surrounded by immunized individuals who are unlikely to contract, harbor, or transmit the disease.

In public health , contact tracing is the process of identifying persons who may have come into contact with an infected person ("contacts") and subsequent collection of further information about these contacts. By tracing the contacts of infected individuals, testing them for infection, isolating or treating the infected, and tracing their contacts, public health aims to reduce infections in the population. Diseases for which contact tracing is commonly performed include tuberculosis , vaccine-preventable infections like measles , sexually transmitted infections (including HIV ), blood-borne infections , Ebola , some serious bacterial infections , and novel virus infections (e.g. SARS-CoV , H1N1 , and SARS-CoV-2 ).  The goals of contact tracing are:
Contact tracing has been a pillar of communicable disease control in public health for decades. The eradication of smallpox , for example, was achieved not by universal immunization , but by exhaustive contact tracing to find all infected persons. [1] This was followed by isolation of infected individuals and immunization of the surrounding community and contacts at-risk of contracting smallpox.
In cases of diseases of uncertain infectious potential, contact tracing is also sometimes performed to learn about disease characteristics, including infectiousness. Contact tracing is not always the most efficient method of addressing infectious disease. In areas of high disease prevalence , screening or focused testing may be more cost-effective.
Partner notification , also called partner care, is a subset of contact tracing aimed specifically at informing sexual partners of an infected person and addressing their health needs.
Contact tracing generally involves the following steps:
Although contact tracing can be enhanced by letting patients provide information, medication, and referrals to their contacts, evidence demonstrates that direct public health involvement in notification is most effective. [2]
The types of contacts that are relevant for public health management vary with the communicable disease because of differing modes of transmission . For sexually transmitted infections, sexual contacts of the index case are relevant, as well as any babies born to the index case. For blood-borne infections, blood transfusion recipients, contacts who shared a needle, and anyone else who could have been exposed to the blood of the index case are relevant. For pulmonary tuberculosis, people living in the same household or spending a significant amount of time in the same room as the index case are relevant. [3]
Although contact tracing is most commonly used for control of diseases, it is also a critical tool for investigating new diseases or unusual outbreaks. For example, as was the case with SARS, contact tracing can be used to determine if probable cases are linked to known cases of the disease, and to determine if secondary transmission is taking place in a particular community. [4]
Contact tracing has also been initiated among flight passengers during the containment phase of larger pandemics, such as the 2009 pandemic H1NI influenza. However, there continue to be large challenges in achieving the goals of contact tracing during such chaotic events. [5] Development of better guidelines and strategies for pandemic contact tracing continues. [6]
Contact tracing can help identify the etiology of a disease outbreak. [8] In 1984, contact tracing provided the first direct evidence that AIDS may be spread by an infectious agent during sexual contacts. [9] [7] The infective agent has since been identified as HIV . [10]
Backward (or reverse or retrospective) tracing seeks to establish the source of an infection, by looking for contacts before infection.
Forward tracing is the process of looking for contacts after infection, so as to prevent further disease spread.
Backward contact tracing has been advised as a way of finding people with tuberculosis who are missed by routine health services. [11]
For epidemics with high heterogeneity in infectiousness, it might make sense, depending on local contact tracing capacity, to adopt a hybrid strategy of "regular" forward contact tracing combined with contact tracing backwards in time to find the source of infection of the index case itself. Indeed, due to a statistical effect similar to the friendship paradox , the index case is expected to have a much higher number of infected "offspring".
During the COVID-19 pandemic , the adoption by Japan in early spring 2020 of an approach focusing on backward contact tracing was hailed as successful, [12] [13] [14] [15] and other countries which managed to keep the epidemic under control, such as South Korea [16] and Uruguay , [17] [18] are said to have also used the approach. [19] There is mounting theoretical support for the adoption of this strategy in the context of COVID, if local resources allow. [20] [21] [22] [23] Journalist and author Laurie Garrett pointed out in late October 2020, however, that the amount of the virus in the U.S. is now so large that no health departments has the resources to contact and trace. [24] Additionally, officials overlooked the effect of mistrust in the government and conspiracy theories regarding the virus on thwarting contact tracing efforts in the U.S. [25]
In September 2020, science writer and sociologist Zeynep Tufekci wrote a long piece in The Atlantic on the lack of attention by many public health authorities to the index of dispersion of the SARS-CoV-2 virus and its implications for contact tracing. [26]
In October 2020, German virologist Christian Drosten called on all citizens of Germany to maintain a diary of their close contacts, in order to facilitate backward tracing and thus help contact tracing teams to find disease clusters. [27]
Case management software is often used by contact tracers to maintain records of cases and contact tracing activities. [28] This is typically a cloud database that may have specialized features such as the ability to use SMS or email directly within the software to notify people believed to have been in close contact with someone carrying an infectious disease. [29] Vendors offering contact tracing case management software include Salesforce , Microsoft and Watkyn . [30] [31] [32]
Smartphones can provide proximity information useful for contact tracing using GPS, Bluetooth or Wifi signals. [33] Facebook Labs patented the use of Bluetooth on smartphones for this in 2018. [34] On 10 April 2020, Apple and Google , who account for most of the world's mobile operating systems , announced COVID-19 apps for iOS and Android . [35] Relying on Bluetooth Low Energy (BLE) wireless radio signals for proximity information, [36] the new tools would warn people that they had been in contact with who are infected by SARS-CoV-2 . [35]
Various protocols, such as Pan-European Privacy-Preserving Proximity Tracing (PEPP-PT), [37] Whisper Tracing Protocol , [38] Decentralized Privacy-Preserving Proximity Tracing (DP-PPT/DP-3T), [39] [40] TCN Protocol , Contact Event Numbers (CEN), Privacy Sensitive Protocols And Mechanisms for Mobile Contact Tracing (PACT) [41] and others, are being discussed to preserve user privacy.
The DP-3T and TCN protocols are currently used by Switzerland , Austria , Estonia , Finland , Italy , and the UK . While France , Australia , New Zealand , and Singapore use PEPP-PT or BlueTrace based systems. [42]
Challenges with contact tracing can arise related to issues of medical privacy and confidentiality. Public health practitioners often are mandated reporters , required to act to contain a communicable disease within a broader population and also ethically obliged to warn individuals of their exposure.  Simultaneously, infected individuals have a recognized right to medical confidentiality.  Public health teams typically disclose the minimum amount of information required to achieve the objectives of contact tracing.  For example, contacts are only told that they have been exposed to a particular infection, but not informed of the person who was the source of the exposure. [2]
Some activists and health care providers have expressed concerns that contact tracing may discourage persons from seeking medical treatment for fear of loss of confidentiality and subsequent stigma, discrimination, or abuse.  This has been of particular concern regarding contact tracing for HIV.  Public health officials have recognized that the goals of contact tracing must be balanced with the maintenance of trust with vulnerable populations and sensitivity to individual situations. [2]
A contagious disease (or communicable disease )  is a disease that readily spread (that is, communicated) by transmission of a pathogen from an infected person to another person. [1] Contagious diseases vary in how readily they are communicated. For example, COVID-19 , which spreads by transmission of the SARS-CoV-2 coronavirus from one person to another, is extremely contagious, as evidenced by the global pandemic it caused.
Conversely, a non-contagious disease either cannot be transmitted from one person to another or the probability of transmitting the disease to another person is low. [2]
A disease is often known to be contagious before medical science discovers its causative agent . Koch's postulates , which were published at the end of the 19th century, were the standard for the next 100 years or more, especially with diseases caused by bacteria . Microbial pathogenesis attempts to account for diseases caused by a virus .
Originally, the term referred to a contagion (a derivative of 'contact') or disease transmissible only by direct physical contact. In the modern-day, the term has sometimes been broadened to encompass any communicable or infectious disease. Often the word can only be understood in context, where it is used to emphasise very infectious , easily transmitted, or especially severe communicable disease.
In 1849, John Snow first proposed that cholera was a contagious disease.
Most epidemics are caused by contagious diseases, with occasional exceptions, such as yellow fever . The spread of non-contagious communicable diseases is changed either very little or not at all by medical isolation of ill persons or medical quarantine for exposed persons. Thus, a "contagious disease" is sometimes defined in practical terms, as a disease for which isolation or quarantine are useful public health responses. [3] [ failed verification ]
Some locations are better suited for the research into the contagious pathogens due to the reduced risk of transmission afforded by a remote or isolated location.
Negative room pressure is a technique in health care facilities based on aerobiological designs.
Contingent contagionism was a concept in 19th-century medical writing and epidemiology before the germ theory , used as a qualified way of rejecting the application of the term " contagious disease " for a particular infection. For example, it could be stated that cholera , or typhus , was not contagious in a "healthy atmosphere", but might be contagious in an "impure atmosphere". [1] Contingent contagionism covered a wide range of views between "contagionist", and "anti-contagionist" such as held by supporters of the miasma theory . [2]
A form of contingent contagionism was standard in medieval European medicine. Contagion was not conceptualised as restricted to physical contact. A corruption of air could be transmitted from person to person, at short range. [3]
By the 1840s public health policy , at least in the United Kingdom , had become a battleground between contagionist and anti-contagionist parties. The former, in particular, supported quarantine measures against epidemics (such as the cholera pandemic ). The latter opposed quarantines. Anticontagionists, for example, argued that infection could be at a distance, from a cause that could be sporadic and possible diffused through the air, and taking advantage of "predisposed" individuals. [4] Public health measures quite typically combined contagionist and anti-contagionist aspects. [5] Anti-contagionists, such as Florence Nightingale who was a convinced miasmatist, could collaborate with contingent contagionists on sanitary measures. [6]
Decomposing organic waste, as " filth ", was considered implicated in many diseases, because of the gases it generated. The application of contingent contagionism could be that there was a contagious agent that was spread by filthy conditions. Sanitation as cleaning was therefore directly associated with public health . [7] It has been commented that those involved in public health at this time, successful in bringing down death rates, "often attributed disease causation to levels farther up the causal chain than direct biological mechanisms". [8]
The Medico-chirurgical Review in 1824 wrote that it had "always advocated" the doctrine of contingent contagion in the case of yellow fever "and indeed in most fevers". Having mentioned William Pym (contagionist) and Edward Nathaniel Bancroft (anti-contagionist) as extremists, it went on to say (italics in the original)
That the yellow fever of the West Indies [...] is rarely contagious, under common circumstances of cleanliness and ventilation , is as well ascertained as any fact in medicine. [9]
Which it qualified in terms of overcrowding, and an outbreak in 1823 on the sloop HMS Bann .
The influence of atmosphere on contagion was subject to a distinction: a "pure" atmosphere might effectively block airborne contagion, while an "impure" atmosphere was ineffective for that; or on the other hand "impure" atmosphere, as well as crowding and filth, might mean a disease could "acquire" the property of contagion. [10] A "malignant microenvironment" could be to blame, a hypothesis that had a consensus behind it in the aetiology of the middle of the 19th century. Inadequate ventilation was one factor to which the consensus pointed. [11]
Zymotic theory was an explanation of disease developed by Justus von Liebig and William Farr in the 1840s. A form of contingent contagionism, it began with a hypothesis on decomposition of large complex molecules, depending on collision with other such molecules. It relied on fermentation as an underlying analogy for disease. [12]
Correlates of immunity/protection to a virus or other infectious pathogen are measurable signs that a person (or other potential host) is immune , in the sense of being protected against becoming infected and/or developing disease .
For many viruses, antibodies serve as a correlate of immunity.  So for example, pregnant women  are routinely screened in the UK for rubella antibodies to confirm their immunity to this infection which can cause serious congenital abnormalities.  In contrast for HIV , the simple presence of antibodies is clearly not a correlate of immunity/protection since infected individuals develop antibodies without being protected against disease.
The fact that the correlates of immunity/protection remain unclear is a significant barrier to HIV vaccine research.  There is evidence that some highly exposed individuals can develop resistance to HIV infection, [1] suggesting that immunity and therefore a vaccine is possible.  However, without knowing the correlates of immunity, scientists cannot know exactly what sort of immune response a vaccine would need to stimulate, and the only method of assessing vaccine effectiveness will be through large phase III trials with clinical outcomes (i.e. infection and/or disease, not just laboratory markers).
The Corrupted Blood incident (or World of Warcraft pandemic [1] [2] ) was a virtual pandemic in the MMORPG World of Warcraft , which began on September 13, 2005, and lasted for one week. The epidemic began with the introduction of the new raid Zul'Gurub and its end boss Hakkar the Soulflayer. When confronted and attacked, Hakkar would cast a hit point -draining and highly contagious debuff spell called "Corrupted Blood" on players.
The spell, intended to last only seconds and function only within the new area of Zul'Gurub, soon spread across the virtual world by way of an oversight that allowed pets and minions to take the affliction out of its intended confines. By both accidental and then purposeful intent, a pandemic ensued that quickly killed lower-level characters and drastically changed normal gameplay, as players did what they could do to avoid infection. Despite measures such as programmer-imposed quarantines, and the players' abandoning of densely populated cities (or even just not playing the game), it lasted until a combination of patches and resets of the virtual world finally controlled the spread.
The conditions and reactions of the event attracted the attention of epidemiologists for its implications of how human populations could react to a real-world epidemic.
The epidemic began on September 13, 2005, when Blizzard introduced a new raid called Zul'Gurub into the game as part of a new update. Its end boss, Hakkar the Soulflayer, could affect a set number of players by applying a contagious debuff called Corrupted Blood, which was countered by players spreading out around the area, away from each other. However, Corrupted Blood could be passed on between any nearby characters, and would kill characters of lower levels in a few seconds, while higher level characters could keep themselves alive. It would disappear as time passed or when the character died.
Due to a programming oversight, when hunters or warlocks dismissed their pets, those pets would keep any active debuffs when summoned again. Non-player characters could contract the debuff, and could not be killed by it but could still spread it to players; in effect, this turned them into asymptomatic carriers and a form of vector for the debuff. [3] At least three of the game's servers were affected. The difficulty in killing Hakkar may have limited the spread of the disease. Discussion forum posters described seeing hundreds of bodies lying in the streets of the towns and cities. Deaths in World of Warcraft are not permanent, as characters are resurrected shortly afterward. [4] However, dying in such a way is disadvantageous to the player's character and incurs inconvenience. [5]
During the epidemic, normal gameplay was disrupted. The major towns and cities were abandoned by the population as panic set in and players rushed to evacuate to the relative safety of the countryside, leaving urban areas filled with dead player characters. [6]
Player responses varied but resembled real-world behaviors. Some characters with healing abilities volunteered their services, some lower-level characters who could not help would direct people away from infected areas, some characters would flee to uninfected areas, and some characters attempted to spread the disease to others. [3] Players in the game reacted to the disease as if there were real risk to their well-being. [7] Blizzard Entertainment attempted to institute a voluntary quarantine to stem the disease, but it failed, as some players didn't take it seriously, while others took advantage of the pandemonium. [3] Despite certain security measures, players overcame them by giving the disease to summonable pets. [8]
Blizzard was forced to fix the problem by instituting hard resets of the servers and applying quick fixes. [4] The plague ended on October 8, 2005, when Blizzard made pets unable to be affected by Corrupted Blood, thereby rendering it unable to exist outside of Zul'Gurub.
At the time, World of Warcraft had more than two million players all over the world. [9] Before Blizzard Entertainment commented on the outbreak, there was debate whether it was intentional or a glitch. [4] On Blizzard's forums, posters were commenting about how it was a fantastic world event, and calling it "the day the plague wiped out Ironforge." [10] An editor of a World of Warcraft fan site described it as the first proper world event. [4] After the incident began, Blizzard received calls from angry customers complaining about how they just died. [10] Some players abandoned the game altogether until the problem was fixed. [11] The hard resets were described as a "blunt ending" by Gamasutra . [3]
The people who spread the disease out of malice were described by Security Focus editor Robert Lemos as terrorists of World of Warcraft . [12]
Jeffrey Kaplan —a game designer for World of Warcraft —stated that it gave them ideas for possible real events in the future. [10] Brian Martin—independent security consultant for World of Warcraft —commented that it presented an in-game dynamic that was not expected by players or Blizzard developers and that it reminds people that even in controlled online atmospheres, unexpected consequences can occur. He also compared it to a computer virus , stating that while it is not as serious, it also reminds people of the impact computer code can have on them, and they're not always safe, regardless of the precautions they take. [12]
During one week of October 2008, a zombie plague was spread to promote the second World of Warcraft expansion, Wrath of the Lich King , before its release. Unlike Corrupted Blood, this plague was intentional and was dubbed by an authorized representative of Blizzard Entertainment as the "Great Zombie Plague of '08". [13] It was compared to Corrupted Blood by The Sunday Times , which described the zombie plague as being more true-to-life. The plague was contagious, but in contrast to Corrupted Blood, which had 100% transmission to nearby characters, being in the vicinity of a character infected with the zombie plague represented only a small risk of transmission. This meant that encountering a lone zombie was not as dangerous as encountering a large mass of infected individuals. [14] The event—which Blizzard ended on 28 October—earned the company both praise and criticism from its fans. [15]
On 12 January 2017, lasting for a day until a hotfix was released by Blizzard, [16] a plague involving a debuff called Burn, that increasingly damaged players over time, started in a boss area. The debuff could be passed between players of the same faction and could kill low-level players in a few seconds. [17] It's believed that the debuff had been taken out of the boss area by a pet. During the incident Blizzard gamemasters "disinfected" players and kept the plague under control. [18]
In March 2007, Ran D. Balicer, an epidemiologist physician at the Ben-Gurion University of the Negev in Beersheba , Israel , published an article in the journal Epidemiology that described the similarities between this outbreak and the then-recent SARS and avian influenza outbreaks. Dr. Balicer suggested that role-playing games could serve as an advanced platform for modeling the dissemination of infectious diseases. [19] In a follow-up article in the journal Science , the game Second Life was suggested as another possible platform for these studies. [20] The Centers for Disease Control and Prevention contacted Blizzard Entertainment and requested statistics on this event for research on epidemics, but was told that it was a glitch. [10]
The Corrupted Blood incident was described as a fascinating yet accidental case study of modeling disease origins and control at the Games for Health conference in Baltimore , Maryland by Gamasutra . [3] They compared it to a real-life epidemic, in that it originated in a remote, uninhabited region and was carried by travelers to larger regions; hosts were both human and animal, comparing it to the avian flu ; was passed through close contact; and there were people, in this case non-playable characters, who could contract it but were asymptomatic . [3] However, there were elements that differed from a real-world epidemic, including an indicator for carriers that they have the disease and how much risk they are at, which cannot be done in the real world. [3] One aspect of the epidemic that was not considered by epidemiologists in their models was curiosity, describing how players would rush into infected areas to witness the infection and then rush out. This was paralleled to real-world behavior, specifically with how journalists would cover an incident, and then leave the area. [3]
In August 2007, Nina Fefferman—a Tufts University assistant research professor of public health and family medicine—called for research on this incident, citing the resemblances with biological plagues. Some scientists want to study how people would react to environmental pathogens , by using the virtual counterpart as a point of reference. [21] [22] Subsequently, she co-authored a paper in Lancet Infectious Disease discussing the epidemiological and disease modeling implications of the outbreak, along with Eric Lofgren, a University of North Carolina graduate student. [23] She spoke at the 2008 Games for Health conference in Baltimore , Maryland and the 2011 Game Developers Conference about the incident and how massively multiplayer online populations could solve the problems inherent with more traditional models of epidemics. [3] [24]
Fefferman added that the three base models have their strengths and weaknesses, but make significant behavioral assumptions. She also compared Corrupted Blood to a drug trial with mice—"a real good first step."  She stated, "These are my mice [and] I want this to be my new experiment setup."  She expressed an interest in designing new diseases, perhaps non-fatal ones, to be introduced to the game so she could study how risk is viewed, how rumors would spread, and how public health notices are handled.  She added that Blizzard made such notices in the original outbreak, but kept changing its position as it could not effectively deal with the problem.  She commented that she did not believe it would ruin gameplay, as World of Warcraft dealt with health challenges in combat, and that games set in medieval times had such health risk.  She argued that if researchers and developers worked together, it could be fun.  While Blizzard was initially excited about the proposition, it became less outwardly excited over time, though never rejected it.  She has been in contact with other developers, hoping to conduct the simulation in games similar to World of Warcraft . [3]
Dr. Gary Smith, professor of population biology and epidemiology at the University of Pennsylvania , commented that very few mathematical models of disease transmission take host behavior into account, but also questioned how representative of real life a virtual model could be.  He stated that while the characteristics of the disease could be defined beforehand, the study is just as observational as one conducted on a real-life disease outbreak.  However, he added that one could argue that the proposal could give an opportunity for a study that epidemiologists may never have. [7] Neil Ferguson , director of the MRC Centre for Global Infectious Disease Analysis at Imperial College , London, felt skeptical of the idea, commenting that such a study could not properly mimic genuine behavior.  Using the zombie plague used to promote World of Warcraft: Wrath of the Lich King before its release as an example, players would intentionally become infected to gain zombie powers.  He added that characters could also regenerate, meaning there was low risk in becoming infected.  He felt that while online games such as World of Warcraft could be set up to help scientists study epidemics, it will always be limited as their primary use is for entertainment. [15]
In an analysis of the Corrupted Blood incident, Charles Blair , deputy director of the Center of Terrorism and Intelligence Studies, said that World of Warcraft could provide a powerful new way to study how terrorist cells form and operate. While his organization already uses computer models to study terrorists' tactics, Blair explained that because World of Warcraft involves real people making real decisions in a world with controllable bounds, it could provide a more realistic models for military intelligence analysts. [8]
Yale University terrorism expert Stuart Gottlieb admitted that while the outbreak was interesting and relevant to the times, he would not base a counter-terrorism strategy on a video game. Gottlieb expressed skepticism that analyzing the incident could shed light on the complex underlying causes of terrorism in the real world, as the stakes for both terrorists and civilians are lowered in a virtual setting. However, as commented by the editor of the article, "the biggest weakness for using a game as an analytical tool is that death in World of Warcraft is a nuisance at most". [8]
Blizzard has maintained a position that World of Warcraft is first and foremost a game, and that it was never designed to mirror reality or anything in the real world. [8]
The Corrupted Blood incident has been compared to the COVID-19 pandemic , and epidemiologists who studied the Corrupted Blood outbreak are using the research from the incident to better understand coronavirus's spread - primarily its sociological factors. [25] [26] [27] [28] [29] Dr. Eric Lofgren, an epidemiologist and co-author of a research paper about Corrupted Blood, stated in an interview with PC Gamer that  "When people react to public health emergencies, how those reactions really shape the course of things. We often view epidemics as these things that sort of happen to people. There's a virus and it's doing things. But really it's a virus that's spreading between people, and how people interact and behave and comply with authority figures, or don't, those are all very important things. And also that these things are very chaotic. You can't really predict 'oh yeah, everyone will quarantine. It'll be fine.' No, they won't." [25] [26] [27] [28] Griefing , such as players intentionally spreading Corrupted Blood to others, was one of the aspects of the Corrupted Blood study that has been criticized as lacking a real-world basis; Dr. Lofgren expressed in the wake of the coronavirus epidemic that "one of the critiques we got from a lot of people, both gamers and scientists, was over this idea of griefing, ... How griefing isn't really analogous to anything that takes place in the real world. People aren't intentionally getting people sick. And they might not be intentionally getting people sick, but wilfully ignoring your potential to get people sick is pretty close to that. You start to see people like 'oh this isn't a big deal, I'm not going to change my behavior.' ... Epidemics are a social problem... Minimizing the seriousness of something is sort of real-world griefing." [25]
The Corrupted Blood incident was abrupt and far-reaching; Dr. Lofgren expressed that this abruptness is a property of a real pandemic, stating "Corrupted Blood was this unexpected black swan event. We treat this [coronavirus] as if it's unexpected, but nature is really good at getting people sick". [25] [28] Dr. Nina Fefferman, a co-author of the Corrupted Blood study, expressed that the incident particularly exemplified "how people perceive threats and how differences in that perception can change how they behave", and how people discuss a threat on social media, stating that "A lot of my work since then has been in trying to build models of the social construction of risk perception and I don't think I would have come to that as easily if I hadn't spent time thinking about the discussions WoW players had in real time about Corrupted Blood and how to act in the game based on the understanding they built from those discussions." [25] [26] [28]
Pandemic fatigue is the state of being worn out by recommended precautions and restrictions relating to a pandemic , often due to the length of the restrictions and lack of activities for one to engage in, resulting in boredom , depression , and other issues, thereby leading one to abandoning these precautions and risk catching the disease. [1] Pandemic fatigue can be responsible for an increased number of cases. [2]
Social norms can have an effect on pandemic fatigue. [3]
Political distrust can have an effect on pandemic fatigue as well. "Crisis fatigue" is the idea the public has  become immune to warnings from politicians and distrustful of their claims. [4] The public has been exposed to several crises in the past two decades, including SARS in 2003, bird flu in 2005, swine flu in 2009, MERS in 2012, Ebola in 2014 and currently COVID-19 in 2020-2021. [5] Because of this, some people find it hard to trust political officials and their suggestions on how to treat and manage COVID-19 . [6]
Epidemiologist Julia Marcus wrote that indefinite abstention from all social contact was not a sustainable way to contain a pandemic. Drawing from lessons in HIV prevention , she advised a principle of harm reduction rather than an "all-or-nothing approach" in controlling the COVID-19 pandemic . [7]
With many countries having a rise in new cases from Variants of SARS-CoV-2 , more waves of lockdowns have been put in effect. Countries like the UK have been put back into COVID-19 lockdowns and due to this, many citizens there have been in this state of fatigue and exhaustion. Studies show that people are finding it harder to stay positive, with 60% of citizens in the UK saying they are finding it harder to stay positive daily compared to before the pandemic – an 8-point increase. [8]
One of the major ways with coping with pandemic fatigue is limiting the amount of time you spend on your device. Justin Ross, a psychologist who studies the effects of pandemic fatigue, states that " Doomscrolling , or purposely tuning in to negative stories on TV or on social media, fuels increased dread, uncertainty, anxiety, and fatigue." [9] Another method he found to be very useful in his studies was being active. "If you make movement a priority, you will find a way to make it happen. Prioritizing time to exercise and meditate by putting it in your schedule and protecting that time is going to make a huge difference in your mental health". Other forms of coping include meditation and finding time for yourself to reflect.
2020
2021
By conveyance
Vaccines
By industry
COVID fatigue is the state of being worn out about the precautionary measures and the threat of COVID-19 . Anxiety from the threat of losing economic security and catching the disease both play a part in the feeling of fatigue in people. COVID fatigue has caused people to not follow precautionary guidelines, increasing their risk of catching the virus. [10] Many people are tired of the lockdowns, and not having a normal routine. [11] [12] Higher levels of alcohol and drug use also contribute to the feeling of tiredness. [13]
As lockdowns were lifted in many parts of the world, a lot of people started to ignore stay-at-home orders. People went to bars and restaurants, ultimately causing the disease to spread faster. [14]
Zoom fatigue is described as tiredness, anxiety, or worry resulting from overusing virtual videoconferencing platforms. [15] Evidence suggests that being on Zoom calls limits the amount of nonverbal cues our brains pick up in face-to-face interactions. The lack of these cues causes our brains to subconsciously exert more energy, making us feel more irritable and exhausted after video calls are over. Other issues of Zoom include the fact that we are staring at a screen with peoples faces a couple feet away. This leads to a sense of danger and although our body knows we are in a safe place, our mind is on high alert. [15] Treatment for Zoom fatigue is fairly easy. Being able to connect with friends and family over technology that allows for these nonverbal cues (such as VR) works wonders. VR allows for "avatars" to interact with each other and gives the user the sensation that they are actually there, while still maintaining safe distances during lockdowns. [ citation needed ]
The COVID-19 Immunity Task Force (CITF) is one of the Government of Canada 's early efforts to track the 2020 coronavirus pandemic . [1] An external, dedicated secretariat will help maximize the efficiency of the CITF's work. [2]
The CITF Board is composed of doctors, infectious disease experts, and policy makers. Its charter members were five: [1]
The CCITF board expanded on 2 May 2020. Its additional members are: [3]
The CITF was to use a serology "to survey representative samples of the population for the presence of antibodies to the virus". [4] Trudeau's press release on 23 April 2020, on the initiation of the CCITF listed several goals it would help to achieve notably that it would: [2]

establish priorities and oversee the coordination of a series of country-wide blood test surveys that will tell us how widely the virus has spread in Canada and provide reliable estimates of potential immunity and vulnerabilities in Canadian populations.

This article about the COVID-19 pandemic is a stub . You can help Wikipedia by expanding it .
This article about an organization in Canada is a stub . You can help Wikipedia by expanding it .
The critical community size (CCS) is the minimum size of a closed population within which a human-to-human, non-zoonotic pathogen can persist indefinitely. [1]
When the size of the closed population falls below the critical community size level, the low density of infected hosts causes extinction of the pathogen. [2] This epidemiologic phenomenon was first identified during measles outbreaks in the 1950s. [1]
The critical community size depends on:

This virus -related article is a stub . You can help Wikipedia by expanding it .
In medical research , social science , and biology , a cross-sectional study (also known as a cross-sectional analysis , transverse study , prevalence study ) is a type of observational study that analyzes data from a population, or a representative subset, at a specific point in time —that is, cross-sectional data .
In economics , cross-sectional studies typically involve the use of cross-sectional regression , in order to sort out the existence and magnitude of causal effects of one independent variable upon a dependent variable of interest at a given point in time. They differ from time series analysis , in which the behavior of one or more economic aggregates is traced through time.
In medical research, cross-sectional studies differ from case-control studies in that they aim to provide data on the entire population under study, whereas case-control studies typically include only individuals who have developed a specific condition and compare them with a matched sample, often a tiny minority, of the rest of the population. Cross-sectional studies are descriptive studies (neither longitudinal nor experimental). Unlike case-control studies, they can be used to describe, not only the odds ratio , but also absolute risks and relative risks from prevalences (sometimes called prevalence risk ratio , or PRR). [1] [2] They may be used to describe some feature of the population, such as prevalence of an illness, but cannot prove cause and effect. Longitudinal studies differ from both in making a series of observations more than once on members of the study population over a period of time.
Cross-sectional studies involve data collected at a defined time.. They are often used to assess the prevalence of acute or chronic conditions, but cannot be used to answer questions about the causes of disease or the results of intervention. Cross-sectional data cannot be used to infer causality because temporality is not known. They may also be described as censuses . Cross-sectional studies may involve special data collection, including questions about the past, but they often rely on data originally collected for other purposes. They are moderately expensive, and are not suitable for the study of rare diseases. Difficulty in recalling past events may also contribute bias.
The use of routinely collected data allows large cross-sectional studies to be made at little or no expense. This is a major advantage over other forms of epidemiological study. A natural progression has been suggested from cheap cross-sectional studies of routinely collected data which suggest hypotheses, to case-control studies testing them more specifically, then to cohort studies and trials which cost much more and take much longer, but may give stronger evidence. In a cross-sectional survey, a specific group is looked at to see if an activity, say alcohol consumption, is related to the health effect being investigated, say cirrhosis of the liver . If alcohol use is correlated with cirrhosis of the liver, this would support the hypothesis that alcohol use may be associated with cirrhosis.
Routine data may not be designed to answer the specific question.
Routinely collected data does not normally describe which variable is the cause and which the effect. Cross-sectional studies using data originally collected for other purposes are often unable to include data on confounding factors , other variables that affect the relationship between the putative cause and effect. For example, data only on present alcohol consumption and cirrhosis would not allow the role of past alcohol use, or of other causes, to be explored. Cross-sectional studies are very susceptible to recall bias .
Most case-control studies collect specifically designed data on all participants, including data fields designed to allow the hypothesis of interest to be tested. However, in issues where strong personal feelings may be involved, specific questions may be a source of bias. For example, past alcohol consumption may be incorrectly reported by an individual wishing to reduce their personal feelings of guilt. Such bias may be less in routinely collected statistics, or effectively eliminated if the observations are made by third parties, for example taxation records of alcohol by area.
Cross-sectional studies can contain individual-level data (one record per individual, for example, in national health surveys). However, in modern epidemiology it may be impossible to survey the entire population of interest, so cross-sectional studies often involve secondary analysis of data collected for another purpose. In many such cases, no individual records are available to the researcher, and group-level information must be used. Major sources of such data are often large institutions like the Census Bureau or the Centers for Disease Control in the United States. Recent census data is not provided on individuals, for example in the UK individual census data is released only after a century. Instead data is aggregated, usually by administrative area. Inferences about individuals based on aggregate data are weakened by the ecological fallacy .  Also consider the potential for committing the "atomistic fallacy" where assumptions about aggregated counts are made based on the aggregation of individual level data (such as averaging census tracts to calculate a county average).  For example, it might be true that there is no correlation between infant mortality and family income at the city level, while still being true that there is a strong relationship between infant mortality and family income at the individual level. All aggregate statistics are subject to compositional effects, so that what matters is not only the individual-level relationship between income and infant mortality, but also the proportions of low, middle, and high income individuals in each city. Because case-control studies are usually based on individual-level data, they do not have this problem.
In economics, cross-sectional analysis has the advantage of avoiding various complicating aspects of the use of data drawn from various points in time, such as serial correlation of residuals. It also has the advantage that the data analysis itself does not need an assumption that the nature of the relationships between variables is stable over time, though this comes at the cost of requiring caution if the results for one time period are to be assumed valid at some different point in time.
An example of cross-sectional analysis in economics is the regression of money demand —the amounts that various people hold in highly liquid financial assets—at a particular time upon their income, total financial wealth, and various demographic factors . Each data point is for a particular individual or family, and the regression is conducted on a statistical sample drawn at one point in time from the entire population of individuals or families. In contrast, an intertemporal analysis of money demand would use data on an entire country's holdings of money at each of various points in time, and would regress that on contemporaneous (or near-contemporaneous) income, total financial wealth, and some measure of interest rates. The cross-sectional study has the advantage that it can investigate the effects of various demographic factors (age, for example) on individual differences; but it has the disadvantage that it cannot find the effect of interest rates on money demand, because in the cross-sectional study at a particular point in time all observed units are faced with the same current level of interest rates.
Cross-species transmission ( CST ), also called interspecies transmission , host jump , or spillover , is the transmission of an infectious pathogen , such as a virus , between hosts belonging to different species . Once introduced into an individual of a new host species, the pathogen may cause disease for the new host and/or acquire the ability to infect other individuals of the same species, allowing it to spread through the new host population. [1] The phenomenon is most commonly studied in virology , but cross-species transmission may also occur with bacterial pathogens or other types of microorganisms. [2]
Steps involved in the transfer of pathogens to new hosts include contact between the pathogen and the host; the successful infection of an initial individual host, which may lead to amplification and an outbreak ; and the adaptation of the pathogen, within either the original or new host, which may render it capable of spreading efficiently between individuals in populations of the new host. [3] The concept is important in understanding and controlling emerging infectious diseases in humans, especially those caused by viruses. Most viral diseases of humans are zoonotic in origin, having been historically transmitted to human populations from various animal species; examples include SARS , Ebola , swine flu , rabies , and avian influenza . [4]
The exact mechanisms which facilitate cross-species transmission vary by pathogen, and even for common diseases are often poorly understood. It is believed that viruses with high mutation rates are able to rapidly adapt to new hosts and thereby overcome host-specific immunological defenses , allowing their continued transmission. A host shifting event occurs when a strain that was previously zoonotic begins to circulate exclusively among the new host species. [5]
Pathogen transfer is most likely to occur between species which are frequently in close contact with each other. It can also occur indirectly between species with less frequent contact if facilitated by an intermediary species; for example, a reservoir species may transfer the virus to a vector species, which in turn transfers the virus to humans. [6] [7] The degree of phylogenetic relatedness between host species also influences the likelihood that a pathogen is transmitted between them, likely because of the similarity of the hosts' immunological defenses; for example, most human zoonotic transmissions come from other species of mammals. Pathogens of more distantly related species, on the other hand, such as plant viruses , may not be capable of infecting humans at all. Other factors influencing transmission rates include geographic proximity and intraspecies behaviors. [3]
Cross-species transmission is the most significant cause of disease emergence in humans and other species. Wildlife zoonotic diseases of microbial origin are also the most common group of human emerging diseases, and CST between wildlife and livestock has appreciable economic impacts in agriculture by reducing livestock productivity and imposing export restrictions. [2] This makes CST of major concern for public health , agriculture , and wildlife management .
The authors of a study on the bubonic plague in Oran stress that the disease "is primarily a bacterial zoonosis affecting rodents . It is caused by Yersinia pestis and is transmitted from animal to animal by fleas . Humans usually become infected through the bite of an infected rodent flea ." The sanitary control measure instituted by the public health authority was chemical in nature: "Intra- and peridomestic spraying with permethrin was conducted. Deltamethrin was dusted on the tracks and around the burrows of rodents located in a radius of 10 km around the dwelling of the patients. Uncontrolled killing of rats was prohibited." [8]
A large proportion of viral pathogens that have emerged recently in humans are considered to have originated from various animal species. This is shown by several recent epidemics such as, avian flu , Ebola , monkey pox , and Hanta viruses . [9] There is evidence to suggest that some diseases can potentially be re-introduced to human populations through animal hosts after they have been eradicated in humans. [10] There is a risk of this phenomenon occurring with morbilliviruses as they can readily cross species barriers. [10] CST can also have a significant effect on produce industries. Genotype VI- Avian paramyxovirus serotype 1 (GVI-PMV1) is a virus that arose through cross-species transmission events from Galliformes (i.e. chicken ) to Columbiformes , and has become prevalent in the poultry industry . [11]
CST of rabies virus variants between many different species populations is a major concern of wildlife management . Introduction of these variants into non-reservoir animals increases the risk of human exposures and threatens current advances toward rabies control. [12]
Many pathogens are thought to have host specialization, which explains the maintenance of distinct strains in host species. [5] Pathogens would have to overcome their host specificity to cross to a new host species.  Some studies have argued that host specializations may be exaggerated, and pathogens are more likely to exhibit CST than previously thought. [5] Original hosts usually have low death rates when infected with a pathogen, with fatality rates tending to be much higher in new hosts [13]
Due to the close relation of nonhuman primates (NHP) and humans , disease transmission between NHP and humans is relatively common and can become a major public health concern. Diseases such as HIV and human adenoviruses have been associated with NHP interactions. [14] [15]
In places where contact between humans and NHPs is frequent, precautions are often taken to prevent disease transmission. Simian foamy viruses (SFV) is an enzootic retrovirus that has high rates of cross-species transmission and has been known to affect humans bitten by infected NHPs. [16] It has caused health concerns in places like Indonesia where visitors at monkey temples can contract SFV from temple macaques ( Macaca fascicularis ). [17] TMAdV ( titi monkey adenovirus ) is a highly divergent, sharing <57% pairwise nucleotide identity with other adenoviruses, NHP virus that had a high fatality rate (83%) in monkeys and is capable of spreading through human hosts. [13]
Prediction and monitoring are important for the study of CSTs and their effects. However, factors that determine the origin and fate of cross-species transmission events remain unclear for the majority of human pathogens. [4] This has resulted in the use of different statistical models for the analysis of CST. Some of these include risk-analysis models, [18] single rate dated tip (SRDT) models, [15] and phylogenetic diffusion models. [4] The study of the genomes of pathogens involved in CST events is very useful in determining their origin and fate. [4] This is because a pathogens genetic diversity and mutation rate are key factors in determining if it can transmit across multiple hosts.  This makes it important for the genomes of transmission species to be partially or completely sequenced. [13] A change in genomic structure could cause a pathogen that has a narrow host range to become capable of exploiting a wider host range. [5] Genetic distance between different species, geographical range, and other interaction barriers will also influence cross-species transmission. [4]
One approach to risk assessment analysis of CST is to develop risk-analysis models that break the ‘‘process’’ of disease transmission into parts. Processes and interactions that could lead to cross-species disease transmission are explicitly described as a hypothetical infection chain. Data from laboratory and field experiments are used to estimate the probability of each component, expected natural variation, and margins of error. [17]
Different types of CST research would require different analysis pathways to meet their needs. A study on identification of viruses in bats that could spread to other mammals used the workflow:  sequencing of genomic samples → “cleaning” of raw reads → elimination of host reads and eukaryotic contaminants → de novo assembly of the remaining reads → annotation of viral contigs → molecular detection of specific viruses → phylogenetic analysis → interpretation of data. [19]
Detecting CST and estimating its rate based on prevalence data is challenging. [2] Due to these difficulties, computational methods are used to analyse CST events and the pathogens associated with them. The explosive development of molecular techniques has opened new possibilities for using phylogenetic analysis of pathogen genetics to infer epidemiological parameters. [2] This provides some insight into the origins of these events and how they could be addressed. Methods of CST prevention are currently using both biological and computational data. An example of this is using both cellular assays and phylogenetic comparisons to support a role for TRIM5α, the product of the TRIM5 gene, in suppressing interspecies transmission and emergence of retroviruses in nature. [20]
The comparison of genomic data is very important for the study of cross-species transmission. Phylogenetic analysis is used to compare genetic variation in both pathogens associated with CST and the host species that they infect. Taken together, it is possible to infer what allowed a pathogen to crossover to a new host (i.e. mutation in a pathogen, change in host susceptibility) and how this can be prevented in the future. If the mechanisms a pathogens uses to initially enter a new species are well characterized and understood a certain level of risk control and prevention can be obtained. In contact, a poor understanding of pathogens, and their associated diseases, makes it harder for preventive measures to be taken [18]
Alternative hosts can also potentially have a critical role in the evolution and diffusion of a pathogen. [21] When a pathogen crosses species it often acquires new characteristics that allow it to breach host barriers. [18] Different pathogen variants can have very different effects on host species. [21] Thus it can be beneficial to CST analysis to compare the same pathogens occurring in different host species. Phylogenetic analysis can be used to track a pathogens history through different species populations. Even if a pathogen is new and highly divergent, phylogenetic comparison can be very insightful. [13] A useful strategy for investigating the history of epidemics caused by pathogen transmission combines molecular clock analysis, to estimate the timescale of the epidemic, and coalescent theory , to infer the demographic history of the pathogen. [15] When constructing phylogenies, computer databases and tools are often used.  Programs, such as BLAST , are used to annotate pathogen sequences, while databases like GenBank provide information about functions based on the pathogens genomic structure. Trees are constructed using computational methods such as MPR or Bayesian Inference, and models are created depending on the needs of the study. [22] Single rate dated tip (SRDT) models, for example, allows for estimates of timescale under a phylogenetic tree. [15] Models for CST prediction will vary depending on what parameters need to be accounted for when constructing the model.
Parsimony is the principle in which one chooses the simplest scientific explanation that fits the evidence. In terms of building phylogenetic trees, the best hypothesis is the one that requires the fewest evolutionary changes. Using parsimony to reconstruct ancestral character states on a phylogenetic tree is a method for testing ecological and evolutionary hypotheses. [23] This method can be used in CST studies to estimate the number of character changes that exist between pathogens in relation to their host. [2] This makes MPR useful for tracking a CST pathogen to its origins. MPR can also be used to the compare traits of host species populations. Traits and behaviours within a population could make them more susceptible to CST. For example, species which migrate regionally are important for spreading viruses through population networks. [24]
Despite the success of parsimony reconstructions, research suggests they are often sensitive and can sometimes be prone to bias in complex models. [23] This can cause problems for CST models that have to consider many variables.  Alternatives methods, such as maximum likelihood, have been developed as an alternative to parsimony reconstruction. [23]
Two methods of measuring genetic variation, variable number tandem repeats (VNTRs) and single nucleotide polymorphisms (SNPs), have been very beneficial to the study of bacterial transmission. [2] VNTRs, due to the low cost and high mutation rates, make them particularly useful to detect genetic differences in recent outbreaks , and while SNPs have a lower mutation rate per locus than VNTRs, they deliver more stable and reliable genetic relationships between isolates. Both methods are used to construct phylogenies for genetic analysis, however, SNPs are more suitable for studies on phylogenies contraction. [2] However, it can be difficult for these methods to accurately simulate CSTs everts.  Estimates of CST based on phylogenies made using the VNTR marker can be biased towards detecting CST events across a wide range of the parameters. SNPs tend to be less biased and variable in estimates of CST when estimations of CST rates are low and a low number of SNPs is used. In general, CST rate estimates using these methods are most reliable in systems with more mutations, more markers, and high genetic differences between introduced strains. [2] CST is very complex and models need to account for a lot of parameters to accurately represent the phenomena. Models that oversimplify reality can result in biased data. Multiple parameters such as number of mutations accumulated since introduction, stochasticity , the genetic difference of strains introduced, and the sampling effort can make unbiased estimates of CST difficult even with whole-genome sequences, especially if sampling is limited, mutation rates are low, or if pathogens were recently introduced. [2] More information on the factors that influence CST rates is needed for the contraction of more appropriate models to study these events.
The process of using genetic markers to estimate CST rates should take into account several important factors to reduce bias. One is that the phylogenetic tree constructed in the analysis needs to capture the underlying epidemiological process generating the tree. [2] The models need to account for how the genetic variability of a pathogen influences a disease in a species, not just general differences in genomic structure. Two, the strength of the analysis will depend on the amount of mutation accumulated since the pathogen was introduced in the system. [2] This is due to many models using the number of mutations as an indicator of CST frequency. Therefore, efforts are focused on estimating either time since the introduction or the substitution rate of the marker (from laboratory experiments or genomic comparative analysis). This is important not only when using the MPR method but also for Likelihood approaches that require an estimation of the mutation rate. [2] Three, CST will also affect disease prevalence in the potential host, so combining both epidemiological time series data with genetic data may be an excellent approach to CST study [2]
Bayesian frameworks are a form of maximum likelihood-based analyses and can be very effective in cross-species transmission studies. Bayesian inference of character evolution methods can account for phylogenetic tree uncertainty and more complex scenarios, with models such as the character diffusion model currently being developed for the study of CST in RNA viruses . [2] A Bayesian statistical approach presents advantages over other analyses for tracking CST origins. Computational techniques allow integration over an unknown phylogeny, which cannot be directly observed, and unknown migration process, which is usually poorly understood. [25]
The Bayesian frameworks are also well suited to bring together different kinds of information. The BEAST software, which has a strong focus on calibrated phylogenies and genealogies, illustrates this by offering a large number of complementary evolutionary models including substitution models, demographic and relaxed clock models that can be combined into a full probabilistic model. By adding spatial reconstruction, these models create the probability of biogeographical history reconstruction from genetic data. [25] This could be useful for determining the origins of cross-species transmissions. 
The high effectiveness of Bayesian statistical methods has made them instrumental in evolutionary studies. [26] Bayesian ancestral host reconstruction under discrete diffusion models can be used to infer the origin and effects of pathogens associated with CST.  One study on Human adenoviruses using Bayesian supported a gorilla and chimpanzee origin for the viral species, aiding prevention efforts. [14] Despite presumably rare direct contact between sympatric populations of the two species, CST events can occur between them. The study also determined that two independent HAdV-B transmission events to humans occurred and that the HAdV-Bs circulating in humans are of zoonotic origin and have probably affected global health for most of our species lifetime. [14]
Phylogenetic diffusion models are frequently used for phylogeographic analyses, with the inference of host jumping becoming of increasing interest. [4] The Bayesian inference approach enables model averaging over several potential diffusion predictors and estimates the support and contribution of each predictor while marginalizing over phylogenetic history. [4] For studying viral CST, the flexibility of the Bayesian statistical framework allows for the reconstruction of virus transmission between different host species while simultaneously testing and quantifying the contribution of multiple ecological and evolutionary influences of both CST spillover and host shifting. [4] One study on rabies in bats showed geographical range overlap is a modest predictor for CST, but not for host shifts. [4] This highlights how Bayesian inferences in models can be used for CST analysis.
In statistics , the Cuzick–Edwards test [1] is a significance test whose aim is to detect the possible clustering of sub-populations within a clustered or non-uniformly-spread overall population. Possible applications of the test include examining the spatial clustering of childhood leukemia and lymphoma within the general population, given that the general population is spatially clustered.
The test is based on:
An example application of this test was to spatial clustering of leukaemias and lymphomas among young people in New Zealand. [2]
In epidemiology , data or facts about a population is called denominator data . Denominator data are independent of any specific disease or condition.  This name is given because in mathematical models of disease , disease-specific data such as the incidence of disease in a population, the susceptibility of the population to a specific condition, the disease resistance, etc. disease-specific variables are expressed as their proportion of some attribute of the general population, and hence appear as the numerator of the fraction or percentage being calculated, general data about the population typically appearing in the denominator ; hence the term "denominator data."
In an epidemiological compartment model , for example, variables are often scaled to total population. The susceptible fraction of a population is obtained by taking the ratio of the number of people susceptible to the total population. Susceptibility to a disease may depend on other factors such as age or sex. Data about a population including age distribution, male/female ratios, and other demographic factors may be relevant as denominator data. Denominator data is not only limited to data describing human populations but also includes information about wild and domestic animal populations.
In population ecology , density-dependent processes occur when population growth rates are regulated by the density of a population . [1] This article will focus on density-dependence in the context of macroparasite life cycles.
Positive density-dependence , density-dependent facilitation , or the Allee effect describes a situation in which population growth is facilitated by increased population density.
For dioecious (separate sex) obligatory parasites, mated female worms are required to complete a transmission cycle. At low parasite densities, the probability of a female worm encountering a male worm and forming a mating pair can become so low that reproduction is restricted due to single sex infections. At higher parasite densities, the probability of mating pairs forming and successful reproduction increases. This has been observed in the population dynamics of Schistosomes . [2]
Positive density-dependence processes occur in macroparasite life cycles that rely on vectors with a cibarial armature, such as Anopheles or Culex mosquitoes. For Wuchereria bancrofti , a filarial nematode, well-developed cibarial armatures in vectors can damage ingested microfilariae and impede the development of infective L3 larvae . At low microfilariae densities, most microfilariae can be ruptured by teeth, preventing successful development of infective L3 larvae. As more larvae are ingested, the ones that become entangled in the teeth may protect the remaining larvae, which are then left undamaged during ingestion. [3]
Positive density-dependence processes may also occur in macroparasite infections that lead to immunosuppression . Onchocerca volvulus infection promotes immunosuppressive processes within the human host that suppress immunity against incoming infective L3 larvae. This suppression of anti-parasite immunity causes parasite establishment rates to increase with higher parasite burden. [4]
Negative density-dependence , or density-dependent restriction , describes a situation in which population growth is curtailed by crowding, predators and competition. In cell biology , it describes the reduction in cell division. When a cell population reaches a certain density, the amount of required growth factors and nutrients available to each cell becomes insufficient to allow continued cell growth .
This is also true for other organisms because an increased density means an increase in intraspecific competition . Greater competition means an individual has a decreased contribution to the next generation i.e. offspring. 
Density-dependent mortality can be overcompensating, undercompensating or exactly compensating.
There also exists density-independent inhibition , where other factors such as weather or environmental conditions and disturbances may affect a population's carrying capacity .
An example of a density-dependent variable is crowding and competition.
Density-dependent fecundity exists, where the birth rate falls as competition increases. In the context of gastrointestinal nematodes, the weight of female Ascaris lumbricoides and its rates of egg production decrease as host infection intensity increases. Thus, the per-capita contribution of each worm to transmission decreases as a function of infection intensity. [5]
Parasite-induced vector mortality is a form of negative density-dependence. The Onchocerciasis life cycle involves transmission via a black fly vector. In this life-cycle, the life expectancy of the black fly vector decreases as the worm load ingested by the vector increases. Because O. volvulus microfilariae require at least seven days to mature into infective L3 larvae in the black fly, the worm load is restricted to levels that allow the black fly to survive for long enough to pass infective L3 larvae onto humans. [6]
In macroparasite life cycles, density-dependent processes can influence parasite fecundity, survival, and establishment. Density-dependent processes can act across multiple points of the macroparasite life cycle. For filarial worms , density-dependent processes can act at the host/vector interface or within the host/vector life-cycle stages. At the host/vector interface, density-dependence may influence the input of L3 larvae into the host's skin and the ingestion of microfilariae by the vector. Within the life-cycle stages taking place in the vector, density-dependence may influence the development of L3 larvae in vectors and vector life expectancy. Within the life-cycle stages taking place in the host, density-dependence may influence the development of microfilariae and host life expectancy. [7]
In reality, combinations of negative (restriction) and positive (facilitation) density-dependent processes occur in the life cycles of parasites. However, the extent to which one process predominates over the other vary widely according to the parasite, vector, and host involved. This is illustrated by the W. bancrofti life cycle. In Culex mosquitoes, which lack a well-developed cibarial armature, restriction processes predominate. Thus, the number of L3 larvae per mosquito declines as the number of ingested microfilariae increases. Conversely, in Aedes and Anopheles mosquitoes, which have well-developed cibarial armatures, facilitation processes predominate. Consequently, the number of L3 larvae per mosquito increases as the number of ingested microfilariae increases. [3]
Negative density-dependent (restriction) processes contribute to the resilience of macroparasite populations. At high parasite populations, restriction processes tend to restrict population growth rates and contribute to the stability of these populations. Interventions that lead to a reduction in parasite populations will cause a relaxation of density-dependent restrictions , increasing per-capita rates of reproduction or survival, thereby contributing to population persistence and resilience. [7]
Contrariwise, positive density-dependent or facilitation processes make elimination of a parasite population more likely. Facilitation processes cause the reproductive success of the parasite to decrease with lower worm burden.  Thus, control measures that reduce parasite burden will automatically reduce per-capita reproductive success and increase the likelihood of elimination when facilitation processes predominate. [8]
The extinction threshold refers to minimum parasite density level for the parasite to persist in a population. Interventions that reduce parasite density to a level below this threshold will ultimately lead to the extinction of that parasite in that population. Facilitation processes increase the extinction threshold, making it easier to achieve using parasite control interventions. Conversely, restriction processes complicates control measures by decreasing the extinction threshold. [8]
Anderson and Gordon (1982) propose that the distribution of macroparasites in a host population is regulated by a combination of positive and negative density-dependent processes. In overdispersed distributions, a small proportion of hosts harbour most of the parasite population. Positive density-dependent processes contribute to overdispersion of parasite populations, whereas negative density-dependent processes contribute to underdispersion of parasite populations. As mean parasite burden increases, negative density-dependent processes become more prominent and the distribution of the parasite population tends to become less overdispersed. [9]
Consequently, interventions that lead to a reduction in parasite burden will tend to cause the parasite distribution to become overdispersed. For instance, time-series data for Onchocerciasis infection demonstrates that 10 years of vector control lead to reduced parasite burden with a more overdispersed distribution. [10]
Globally, an estimated 463 million adults are living with diabetes , according to the latest 2019 data from the International Diabetes Federation . [1] Diabetes prevalence is increasing rapidly; previous 2017 estimates put the number at 425 million people living with diabetes. [2] The number is projected to almost double by 2030. [1] Type 2 diabetes makes up about 85-90% of all cases. [3] [4] Increases in the overall diabetes prevalence rates largely reflect an increase in risk factors for type 2, notably greater longevity and being overweight or obese . [5]
Diabetes mellitus occurs throughout the world, but is more common (especially type 2) in the more developed countries. The greatest increase in prevalence is, however, occurring in low- and middle-income countries [5] including in Asia and Africa, where most patients will probably be found by 2030. [6] The increase in incidence in developing countries follows the trend of urbanization and lifestyle changes, including increasingly sedentary lifestyles, less physically demanding work and the global nutrition transition, marked by increased intake of foods that are high energy-dense but nutrient-poor (often high in sugar and saturated fats, sometimes referred to as the Western pattern diet ). [5] [6] The risk of getting type 2 diabetes has been widely found to be associated with lower socio-economic position across countries. [7]
The WHO estimates that diabetes resulted in 1.5 million deaths in 2012, making it the 8th leading cause of death. [5] However another 2.2 million deaths worldwide were attributable to high blood glucose and the increased risks of associated complications (e.g. heart disease, stroke, kidney failure), which often result in premature death and are often listed as the underlying cause on death certificates rather than diabetes. [5] [8]
Almost one Chinese adult in ten has diabetes. A 2010 study estimated that more than 92 million Chinese adults have the disease, with another 150 million showing early symptoms. [9] The incidence of the disease is increasing rapidly: a reported 30% increase in 7 years. [10] Indigenous nomadic peoples like Tibetans and Mongols are at much higher susceptibility than Han Chinese .
Until recently, India had more diabetics than any other country in the world, according to the International Diabetes Foundation, [11] although the country has now been surpassed in the top spot by China. [9] Diabetes currently affects more than 62 million Indians, which is more than 7.2% of the adult population. [12] Among young and middle aged adults the prevalence of diabetes is 6.7% and prediabetes is 5.6% according to the National Family Health Survey-4. [13] The average age on onset is 42.5 years. [11] Nearly 1 million Indians die due to diabetes every year. [11]
According to the Indian Heart Association , India is projected to be home to 109 million individuals with diabetes by 2035. [14] A study by the American Diabetes Association reports that India will see the greatest increase in people diagnosed with diabetes by 2030. [15] The high incidence is attributed to a combination of genetic susceptibility plus adoption of a high-calorie, low-activity lifestyle by India's growing middle class. [16]
About 3.8 million people in the United Kingdom have diabetes mellitus, but the charity Diabetes U.K. have made predictions that could become high as 6.2 million by 2035/2036. The NHS spent a daily average of £2.2m (€2.6m; $3.7m) in 2013 on prescriptions for managing diabetes in primary care, and about 10% of the primary care prescribing budget is spent on treating diabetes. [17] Diabetes U.K. have also predicted that the National Health Service could be spending as much as 16.9 billion pounds on diabetes mellitus by 2035, a figure that means the NHS could be spending as much as 17% of its budget on diabetes treatment by 2035. [18] [19] However, since the United Kingdom practices a national health care system with universal access, there are far fewer cases of diagnosed diabetes compared to the United States. [20]
Almost 2.4 million Canadians (6.8%) have been diagnosed with type 1 or type 2 diabetes, based on 2009 cronic disease surveillance data. Prevalence is higher among males (7.2%) than females (6.4%). [8] However these numbers are likely an underestimate, as data obtained from blood samples indicate about 20% of diabetes cases remain undiagnosed. [21]
Accounting for the younger age structure in Aboriginal populations, the prevalence of diabetes is 2-3 times higher among First Nations and Métis , compared to the non-Aboriginal population. [8]
The prevalence of diagnosed diabetes among Canadians increased by 70% over the decade from 1999 to 2009. [8] The greatest relative increase in prevalence was seen younger adults (35 to 44 years), attributable in part to increasing rates of overweight and obesity. The Public Health Agency of Canada estimates that if current trends in diabetes continue, the number of Canadians living with diabetes will reach 3.7 million by 2019. [8]
Diabetes rates in the United States, like across North America and around the world, have been increasing substantially. According to the 2014 Statistics Report done by the CDC it was found that, “Diabetes Mellitus affects an estimated 29.1 million people in the United States and is the 7th leading cause of death. It also increases the chances of mortality, as well as the risk for heart attack, kidney failure, and blindness” [22] While the number of people with diabetes in the US continues to grow, the number of new cases has been declining since 2009, after decades of increases in new cases. [23] In 2014, more than 29 million people had diabetes in the United States, of whom 7 million people remain undiagnosed. [24] As of 2012 another 57 million people were estimated to have prediabetes . [25] [26] There were approximately 12.1 million diabetes-related emergency department (ED) visits in 2010 for adults aged 18 years or older (515 per 10,000 U.S. population), accounting for 9.4 percent of all ED visits. [27]
The Centers for Disease Control and Prevention (CDC) has called the change an epidemic . [28] Geographically, there is a U.S. diabetes belt with high diabetes prevalence estimates, which includes Mississippi and parts of Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, North Carolina, Ohio, Pennsylvania, South Carolina, Tennessee, Texas, Virginia, and West Virginia. [29] The National Diabetes Information Clearinghouse estimates diabetes costs $132 billion in the United States alone every year. About 5%–10% of diabetes cases in North America are type 1, with the rest being type 2. The fraction of type 1 in other parts of the world differs. Most of this difference is not currently understood. The American Diabetes Association (ADA) cites the 2003 assessment of the National Center for Chronic Disease Prevention and Health Promotion (Centers for Disease Control and Prevention) that one in three Americans born after 2000 will develop diabetes in their lifetimes. [30] [31]
Diabetes is also more prominent in minority groups. For example according to the American Diabetes Association the rates of diagnosed diabetes are 12.8% of Hispanics, 13.2% of Non-Hispanic blacks, 15.9% of American Indians/Alaskan Natives. While Non-Hispanic whites are 7.6% and only 9% of Asian Americans have diagnosed diabetes. [32] 4.9% of American adults had diabetes in 1990. By 1998, that number rose by a third to 6.5%. The prevalence of diabetes increased for both sexes and every racial group. American women have suffered from diabetes at a higher rate than men, with 7.4% of women being diabetic in 1998, as opposed to only 5.5% of men. The increase in diabetes coincides with an increase in average weight across both genders. In the same time frame, average weight in both men and women increased by nearly 4 kilograms. This relates to the fact that the most common form of diabetes, type 2, is strongly associated with unhealthy weight. Older Americans have suffered from diabetes at a much higher rate than younger people, with over 12% of those in their 60s and 70s being diabetic in 1998. In the same year, less than 2% of those under 30 suffered from diabetes. Weight is also a strong factor in one's likelihood of becoming diabetic, with 13.5% of obese Americans in 1998 being diabetic. In the same year, only 3.5% of people at a healthy weight had the disease. [33]
As of 2006, about 18.3% (8.6 million) of Americans age 60 and older had diabetes, according to the ADA. [34] Diabetes mellitus prevalence increases with age, and the numbers of older persons with diabetes are expected to grow as the elderly population increases in number. The National Health and Nutrition Examination Survey (NHANES III) from 1988–1994 demonstrated, in the population over 65 years old, 18% to 20% had diabetes, with 40% having either diabetes or its precursor form of impaired glucose tolerance . [35] Older individuals are also more likely to be seen in the emergency department (ED) for diabetes.  A study by the Agency for Healthcare Research and Quality (AHRQ) found that in 2010, diabetes-related ED visit rates were highest for patients aged 65 and older (1,307 per 10,000 population), compared with 45- to 64-year-olds (584 per 10,000 population) and 18- to 44-year-olds (183 per 10,000 population). [27]
A second study by AHRQ found that diabetes with complications was one of the twenty most expensive conditions seen in U.S. inpatient hospitalizations in 2011, with an aggregate cost of nearly $5.4 billion for 561,000 stays.  It was among the top five most expensive conditions for uninsured patients, at an aggregate cost of $440 million for 62,000 hospitalizations. [36]
An estimated 700,000 Australians have diabetes. [4] Indigenous populations in developed countries generally have higher prevalence and incidence of diabetes than their corresponding nonindigenous populations. In Australia, the age-standardised prevalence of self-reported diabetes in indigenous Australians is almost four times that of nonindigenous Australians. [37] Reasons include higher rates of obesity, physical inactivity, and living in poor housing and environments among Indigenous peoples. [4] Preventative community health programs are showing some success in tackling this problem.
The International Diabetes Federation (IDF) estimates that 14.2 million are living with diabetes in Africa. [38] The region of Africa has the highest percentage of undiagnosed diabetes cases reaching 66.7%, the highest proportion of diabetes mellitus related mortality and the lowest health expenditure spent on diabetes. [38]
In medical testing with binary classification , the diagnostic odds ratio ( DOR ) is a measure of the effectiveness of a diagnostic test . [1] It is defined as the ratio of the odds of the test being positive if the subject has a disease relative to the odds of the test being positive if the subject does not have the disease.
The rationale for the diagnostic odds ratio is that it is a single indicator of test performance (like accuracy and Youden's J statistic ) but which is independent of prevalence (unlike accuracy) and is presented as an odds ratio , which is familiar to medical practitioners.
The diagnostic odds ratio is defined mathematically as:
where T P {\displaystyle TP} , F N {\displaystyle FN} , F P {\displaystyle FP} and T N {\displaystyle TN} are the number of true positives, false negatives, false positives and true negatives respectively. [1]
As an odds ratio , the logarithm of the diagnostic odds ratio is approximately normally distributed . [ clarification needed ] The standard error of the log diagnostic odds ratio is approximately:
From this an approximate 95% confidence interval can be calculated for the log diagnostic odds ratio:
Exponentiation of the approximate confidence interval for the log diagnostic odds ratio gives the approximate confidence interval for the diagnostic odds ratio. [1]
The diagnostic odds ratio ranges from zero to infinity, although for useful tests it is greater than one, and higher diagnostic odds ratios are indicative of better test performance. [1] Diagnostic odds ratios less than one indicate that the test can be improved by simply inverting the outcome of the test – the test is in the wrong direction, while a diagnostic odds ratio of exactly one means that the test is equally likely to predict a positive outcome whatever the true condition – the test gives no information.
The diagnostic odds ratio may be expressed in terms of the sensitivity and specificity of the test: [1]
It may also be expressed in terms of the Positive predictive value (PPV) and Negative predictive value (NPV): [1]
It is also related to the likelihood ratios , L R + {\displaystyle LR+} and L R − {\displaystyle LR-} : [1]
The log diagnostic odds ratio is sometimes used in meta-analyses of diagnostic test accuracy studies due to its simplicity (being approximately normally distributed). [4]
Traditional meta-analytic techniques such as inverse-variance weighting can be used to combine log diagnostic odds ratios computed from a number of data sources to produce an overall diagnostic odds ratio for the test in question.
The log diagnostic odds ratio can also be used to study the trade-off between sensitivity and specificity [5] [6] by expressing the log diagnostic odds ratio in terms of the logit of the true positive rate (sensitivity) and false positive rate (1 − specificity), and by additionally constructing a measure, S {\displaystyle S} :
It is then possible to fit a straight line, D = a + b S {\displaystyle D=a+bS} . If b ≠ 0 then there is a trend in diagnostic performance with threshold beyond the simple trade-off of sensitivity and specificity. The value a can be used to plot a summary ROC (SROC) curve. [5] [6]
Consider a test with the following 2×2 confusion matrix :
We calculate the diagnostic odds ratio as:
This diagnostic odds ratio is greater than one, so we know that the test is discriminating correctly. We compute the confidence interval for the diagnostic odds ratio of this test as [9, 134].
The diagnostic odds ratio is undefined when the number of false negatives or false positives is zero – if both false negatives and false positives are zero, then the test is perfect, but if only one is, this ratio does not give a usable measure. The typical response to such a scenario is to add 0.5 to all cells in the contingency table, [1] [7] although this should not be seen as a correction as it introduces a bias to results. [5] It is suggested that the adjustment is made to all contingency tables, even if there are no cells with zero entries. [5]

The Dirty Money Project is a scientific research project of New York University , a comprehensive study of the DNA on banknotes which aims to understand the role of banknotes in spreading diseases among humans , especially on those who live in an urban region.
NYU's Dirty Money Project is part of a larger project looking into New York City 's "MetaGenome", which seeks to examine the " microbes all around us".  According to experts, the project may be able to identify potential health threats, fight flu epidemics, and even chart the environmental impact of major storms . The studies conducted so far have revealed that a banknote is a medium of exchange for hundreds of different kinds of bacteria as banknotes pass from hand to hand. [1]
Scientists have also found that each banknote carries about 3,000 types of bacteria on its surface as well as DNA from drug-resistant microbes. [2] [3] And, since a banknote is home to thousands of microbes – bacteria , fungi and pathogens , this situation can cause such illnesses as skin infections , stomach ulcers and food poisoning etc., scientists believe.
One of the goals of the study is to provide information that could help health workers to prevent disease outbreaks in urban environments like New York City before they spread very far through mediums like banknotes.
Disease diffusion occurs when a disease is transmitted to a new location. [1] It implies that a disease spreads, or pours out, from a central source. [2] The idea of showing the spread of disease using a diffusion pattern is relatively modern, compared to earlier methods of mapping disease, which are still used today. [3] According to Rytokonen, the goals of disease mapping are:  1) to describe the spatial variation in disease incidence to formulate an etiological hypothesis; 2) to identify areas of high risk in order to increase prevention; and 3) to provide a map of disease risk for a region for better risk preparedness. [4]
Torsten Hägerstrand ’s early work on “waves of innovation ” is the basis that many medical cartographers and geographers use for mapping spatial diffusion (1968). [5] The diffusion of disease can be described in four patterns:  expansion diffusion, contagious diffusion, hierarchal diffusion and relocation diffusion. [6] Cromley and McLafferty also mention network diffusion and mixed diffusion. [1]
The diffusion of infectious disease tends to occur in a ‘wave’ fashion, spreading from a central source.  Pyle mentions barriers that pose a resistance towards a wave of diffusion, which include but are not limited to:  physiographic features (i.e. mountains, water bodies), political boundaries, linguistic barriers , and with diseases, a barrier could be differing control programs. [7] The diffusion of disease can be identified as a normal distribution over time and translated into an S-shaped curve to show the phases of disease diffusion.  The phases are: Infusion (25th percentile), Inflection (50th percentile), Saturation (75th percentile), and Waning to the upper limits. [8]
The value of mapping and Geographic Information Systems (GIS) is becoming better known to public health professionals to help link disease control to prevention efforts, which can aid in developing better immunization programs. [11] GIS is an excellent tool used to identify spatial patterns and core areas of disease transmission.  Disease maps can distinguish the low and high risk areas, as well as highlight “physical and/or socio-cultural” factors that contribute to the causation of disease. [12] Understanding how a disease spreads gives health officials a better understanding of how to better serve the public.
4. Rytkönen, Mika JP. “Not All Maps are Equal: GIS and Spatial Analysis in Epidemiology.” International Journal of Circumpolar Health 63:1, 2004: pp. 11 Available: http://www.circumpolarhealthjournal.net/index.php/ijch/article/viewFile/17642/20108
In epidemiology , an outbreak is a sudden increase in occurrences of a disease in a particular time and place. It may affect a small and localized group or impact upon thousands of people across an entire continent.  Four linked cases of a rare infectious disease may be sufficient to constitute an outbreak. Outbreaks include epidemics , which term is normally only used for infectious diseases , as well as diseases with an environmental origin, such as a water or foodborne disease .  They may affect a region in a country or a group of countries. Pandemics are near-global disease outbreaks when multiple countries across the world are infected .
The terms "outbreak" and "epidemic" have often been used interchangeably. Researchers Manfred S. Green and colleagues propose that the latter term be restricted to larger events, pointing out that Chambers Concise Dictionary and Stedman's Medical Dictionary acknowledge this distinction. [1]
When investigating disease outbreaks, the epidemiology profession has developed a number of widely accepted steps. As described by the United States Centers for Disease Control and Prevention , these include the following: [2]
The order of the above steps and relative amount of effort and resources used in each varies from outbreak to outbreak. For example, prevention and control measures are usually implemented very early in the investigation, often before the causative agent is known. In many situations, promoting good hygiene and hand-washing is one of the first things recommended. Other interventions may be added as the investigation moves forward and more information is obtained.  Waiting until the end of an investigation to implement prevention and control measures is a sure way to lose ones job. In outbreaks identified through notifiable disease surveillance, reports are often linked to laboratory results and verifying the diagnosis is straight forward.  In outbreaks of unknown etiology, determining and verifying the diagnosis can be a significant part of the investigation with respect to time and resources. Several steps are usually going on at any point in time during the investigation. Steps may be repeated. For example, initial case definitions are often established to be intentionally broad but later refined as more is learned about the outbreak. The above list has 9 steps, others have more. Implementing active surveillance to identify additional cases is often added. [3]
Outbreak debriefing and review has also been recognized as an additional final step and iterative process by the Public Health Agency of Canada. [4]
There are several outbreak patterns, which can be useful in identifying the transmission method or source, and predicting the future rate of infection.  Each has a distinctive epidemic curve , or histogram of case infections and deaths. [5]
Outbreaks can also be:
Patterns of occurrence are:
By convention, a communicable disease outbreak is declared over when a period of twice the incubation period of the infectious disease has elapsed without identification of any new case, however, for organisms with a short incubation period (e.g. fewer than ten days), a period of three times the incubation period is preferred. [10]
Outbreak legislation is still in its infancy and not many countries have had a direct and complete set of the provisions. [11] [12] However, some countries do manage the outbreaks using relevant acts, such as public health law . [13]
Disease surveillance is an epidemiological practice by which the spread of disease is monitored in order to establish patterns of progression. The main role of disease surveillance is to predict, observe, and minimize the harm caused by outbreak , epidemic , and pandemic situations, as well as increase knowledge about which factors contribute to such circumstances. A key part of modern disease surveillance is the practice of disease case reporting . [1]
In modern times, reporting incidences of disease outbreaks has been transformed from manual record keeping, to instant worldwide internet communication.
The number of cases could be gathered from hospitals – which would be expected to see most of the occurrences – collated, and eventually made public. With the advent of modern communication technology , this has changed dramatically. Organizations like the World Health Organization (WHO) and the Centers for Disease Control and Prevention (CDC) now can report cases and deaths from significant diseases within days – sometimes within hours – of the occurrence. Further, there is considerable public pressure to make this information available quickly and accurately. [2] [ failed verification ]
Formal reporting of notifiable infectious diseases is a requirement placed upon health care providers by many regional and national governments, and upon national governments by the World Health Organization to monitor spread as a result of the transmission of infectious agents. Since 1969, WHO has required that all cases of the following diseases be reported to the organization: cholera , plague , yellow fever , smallpox , relapsing fever and typhus . In 2005, the list was extended to include polio and SARS .  Regional and national governments typically monitor a larger set of (around 80 in the U.S.) communicable diseases that can potentially threaten the general population. Tuberculosis , HIV , botulism , hantavirus , anthrax , and rabies are examples of such diseases. The incidence counts of diseases are often used as health indicators to describe the overall health of a population. [ citation needed ]
The World Health Organization (WHO) is the lead agency for coordinating global response to major diseases. The WHO maintains Web sites for a number of diseases, and has active teams in many countries where these diseases occur.
During the SARS outbreak in early 2004, for example, the Beijing staff of the WHO produced updates every few days for the duration of the outbreak. [2] Beginning in January 2004, the WHO has produced similar updates for H5N1 . [3] These results are widely reported and closely watched.
WHO's Epidemic and Pandemic Alert and Response (EPR) to detect, verify rapidly and respond appropriately to epidemic-prone and emerging disease threats covers the following diseases: [4] [ dead link ]
As the lead organization in global public health, the WHO occupies a delicate role in global politics . It must maintain good relationships with each of the many countries in which it is active. As a result, it may only report results within a particular country with the agreement of the country's government. Because some governments regard the release of any information on disease outbreaks as a state secret, this can place the WHO in a difficult position.
The WHO coordinated International Outbreak Alert and Response is designed to ensure "outbreaks of potential international importance are rapidly verified and information is quickly shared within the Network" but not necessarily by the public; integrate and coordinate "activities to support national efforts" rather than challenge national authority within that nation in order to "respect the independence and objectivity of all partners". The commitment that "All Network responses will proceed with full respect for ethical standards, human rights, national and local laws, cultural sensitivities and tradition" ensures each nation that its security, financial, and other interests will be given full weight. [5]
Testing for a disease can be expensive, and distinguishing between two diseases can be prohibitively difficult in many countries. One standard means of determining if a person has had a particular disease is to test for the presence of antibodies that are particular to this disease. In the case of H5N1, for example, there is a low pathogenic H5N1 strain in wild birds in North America that a human could conceivably have antibodies against. It would be extremely difficult to distinguish between antibodies produced by this strain, and antibodies produced by Asian lineage HPAI A(H5N1) . Similar difficulties are common, and make it difficult to determine how widely a disease may have spread.
There is currently little available data on the spread of H5N1 in wild birds in Africa and Asia. Without such data, predicting how the disease might spread in the future is difficult. Information that scientists and decision makers need to make useful medical products and informed decisions for health care, but currently lack include:
Surveillance of H5N1 in humans, poultry, wild birds, cats and other animals remains very weak in many parts of Asia and Africa. Much remains unknown about the exact extent of its spread.
H5N1 in China is less than fully reported. Blogs have described many discrepancies between official China government announcements concerning H5N1 and what people in China see with their own eyes. Many reports of total H5N1 cases have excluded China due to widespread disbelief in China's official numbers. [6] [7] [8] [9] (See Disease surveillance in China .)
"Only half the world's human bird flu cases are being reported to the World Health Organization within two weeks of being detected, a response time that must be improved to avert a pandemic, a senior WHO official said Saturday. Shigeru Omi , WHO's regional director for the Western Pacific, said it is estimated that countries would have only two to three weeks to stamp out, or at least slow, a pandemic flu strain after it began spreading in humans." [10]
David Nabarro , chief avian flu coordinator for the United Nations , says avian flu has too many unanswered questions. [11] [12]
CIDRAP reported on 25 August 2006 on a new US government Web site [13] that allows the public to view current information about testing of wild birds for H5N1 avian influenza which is part of a national wild-bird surveillance plan that "includes five strategies for early detection of highly pathogenic avian influenza. Sample numbers from three of these will be available on HEDDS : live wild birds, subsistence hunter-killed birds, and investigations of sick and dead wild birds. The other two strategies involve domestic bird testing and environmental sampling of water and wild-bird droppings. […] A map on the new USGS site shows that 9327 birds from Alaska have been tested so far this year, with only a few from most other states.  Last year officials tested just 721 birds from Alaska and none from most other states, another map shows. The goal of the surveillance program for 2006 is to collect 75 000 to 100 000 samples from wild birds and 50 000 environmental samples, officials have said". [14]
Diseases of affluence , previously called diseases of rich people , is a term sometimes given to selected diseases and other health conditions which are commonly thought to be a result of increasing wealth in a society. [1] Also referred to as the "Western disease" paradigm, these diseases are in contrast to so-called " diseases of poverty ", which largely result from and contribute to human impoverishment. These diseases of affluence have vastly increased in prevalence since the end of World War II.
Examples of diseases of affluence include mostly chronic non-communicable diseases (NCDs) and other physical health conditions for which personal lifestyles and societal conditions associated with economic development are believed to be an important risk factor — such as type 2 diabetes , asthma , coronary heart disease , cerebrovascular disease , peripheral vascular disease , obesity , hypertension , cancer , alcoholism , gout , and some types of allergy . [1] [2] They may also be considered to include depression and other mental health conditions associated with increased social isolation and lower levels of psychological well being observed in many developed countries. [3] Many of these conditions are interrelated, for example obesity is thought to be a partial cause of many other illnesses. [4]
In contrast, the diseases of poverty have tended to be largely infectious diseases ,  or the result of poor living conditions.  These include tuberculosis , malaria , and intestinal diseases . [5] Increasingly, research is finding that diseases thought to be diseases of affluence also appear in large part in the poor.  These diseases include obesity and cardiovascular disease and, coupled with infectious diseases, these further increase global health inequalities . [1]
Diseases of affluence started to become more prevalent in developing countries as diseases of poverty decline, longevity increases, and lifestyles change. [1] [2] In 2008, nearly 80% of deaths due to NCDs — including heart disease, strokes, chronic lung diseases, cancers and diabetes — occurred in low- and middle-income countries. [6]
According to the World Health Organization (WHO) , the top 10 causes of deaths in high income/affluent countries in 2016 were from:
Except for the lower respiratory infections, all of them are non-communicable diseases . In 2016 WHO reported 56.9 million deaths worldwide, and more than half (54%), were due to the top causes of death previously mentioned. [7]
Factors associated with the increase of these conditions and illnesses appear to be things that are a direct result of technological advances. They include:
Diabetes is a chronic metabolic disease characterized by increase blood glucose level. Type 2 diabetes is the most common form of diabetes. It is caused by resistance to insulin or the lack of production of insulin. It is seen most commonly in adults. Type 1 diabetes or juvenile diabetes affects mostly children. This condition is due to little or lack of insulin production from the pancreas. [9]
According to WHO the prevalence of diabetes has quadrupled from 1980 to 422 million adults. [10] [11] The global prevalence of diabetes has increased from 4.7% in 1980 to 8.5% in 2014. [9] Diabetes has been a major cause for blindness, kidney failure, heart attack, stroke and lower limb amputation. [9]
The Centers of Disease Control and Prevention (CDC) released a report in 2015 indicating that more than 100 million Americans have diabetes or pre-diabetes. Diabetes was the seventh leading cause of death in United States in 2015. [12] In developed countries like the United States, the risk for diabetes is seen in people with low socioeconomic status (SES). [13] Socioeconomic status is defined by the education and the income level of a person. [14] The prevalence of diabetes varies by education level. Of those diagnosed with diabetes:12.6% of adults had less than a high school education, 9.5% had a high school education and 7.2% had more than high school education. [15]
Differences in diabetes prevalence are seen in the population and ethnic groups in USA. Diabetes is more common in non-Hispanic whites who are less educated and have a lower income. It is also more common in less educated Hispanics. [16] The highest prevalence of diabetes is seen in the southeast, southern and Appalachian portion of the United States. [16] In the United States the prevalence of diabetes is increasing in children and adolescents. In 2015, 25 million people were diagnosed with diabetes, of which 193,000 were children. [16] The total direct and indirect cost of diagnosed diabetes in US in 2012 was $245 billion. [15]
In 2009, the Canadian Diabetes Association (CDA) estimated that diagnosed diabetes will increase from 1.3 million in 2000 to 2.5 million in 2010 and 3.7 million in 2020. [17] Diabetes was the 7th leading cause of death in Canada in 2015. Like United States, diabetes in more prevalent in the low socioeconomic group of people in Canada. [17]
According to the International Diabetes Federation , more than 58 million people are diagnosed with diabetes in the European Union Region (EUR), and this will go up to 66.7 million by 2045. Similar to other affluent countries like America and Canada, diabetes is more prevalent in the poorer parts of Europe like Central and Eastern Europe. [18]
In Australia according to self-reported data, 1 in 7 adults or approximately 1.2 million people had diabetes in 2014–2015. People who were living in remote or socioeconomically disadvantaged areas were 4 times more likely to develop type 2 diabetes as compared to non-indigenous Australians. [19] Australia incurred $20.8 million in direct costs towards hospitalization, medication, and out-patient treatment towards diabetes. In 2015, $1.2 billion were lost in Australia's Gross Domestic Product (GDP) due to diabetes. [20]
In these countries of affluence, diabetes is prevalent in low socioeconomic groups of people as there is abundance of unhealthy food choices, high energy rich food, and decreased physical activity. [21] More affluent people are typically more educated and have tools to counter unhealthy foods, such as access to healthy food, physical trainers, and parks and fitness centers. [22]
Obesity and being overweight is one of the main risk factors of type 2 diabetes. Other risk factors include lack of physical activity, genetic predisposition, being over 45 years old, tobacco use, high blood pressure and high cholesterol. [15] In United States, the prevalence of obesity was 39.8% in adults and 18.5% in children and adolescents in 2015–2016. [23] In Australia in 2014–2015, 2 out 3 adults or 63% were overweight or obese. Also, 2 out of 3 adults did little or no exercise. [24] According to the World Health Organization,  Europe had the 2nd highest proportion of overweight or obese people in 2014 behind the Americas. [25]
According to WHO the prevalence of diabetes is rising more in the middle and low income countries. [26] Over the next 25 years, the number of people with diabetes in developing countries will increase by over 150%. Diabetes is typically seen in people above the retirement age in developed countries, but in developing countries people in the age of 35-64 are mostly affected. Although, diabetes is considered a disease of affluence affecting the developed countries, there is more loss of life and premature death among people with diabetes in the developing countries. [27] Asia accounts for 60% of the world's diabetic population. In 1980 less than 1% of Chinese adults were affected by diabetes, but by 2008 the prevalence was 10%. [28] It is predicted that by 2030 diabetes may affect 79.4 million people in India, 42.3 million people in China and 30.3 million in United States. [29]
These changes are the result of developing nations having rapid economic development. This rapid economic development has caused a change in the lifestyle and food habits leading to over-nutrition, increased intake of fast food causing increase in weight, and insulin resistance. [28] Compared to the west, obesity in Asia is low. India has very low prevalence of obesity, but a very high prevalence of diabetes suggesting that diabetes may occur at a lower BMI in Indians as compared to the Europeans. Smoking increases the risk for diabetes by 45%. In developing countries around 50–60% adult males are regular smokers, increasing their risk for diabetes. [28] In developing countries, diabetes is more commonly seen in the more urbanized areas. The prevalence of diabetes in rural population is 1/4th that of urban population for countries like India, Bangladesh, Nepal, Bhutan and Sri Lanka. [29]
Cardiovascular disease refers to a disease of the heart and blood vessels. Conditions and diseases associated with heart disease include: stroke, coronary heart disease, congenital heart disease, heart failure, peripheral vascular disease, and cardiomyopathy. [30] Cardiovascular disease is known as the world's biggest killer. 17.5 million people die from it each year, which equals 31% of all deaths. Heart disease and stroke cause 80% of these deaths. [31]
High blood pressure is the leading risk factor for cardiovascular disease and has contributed to 12% of the cardiovascular related deaths worldwide. [31] Other significant risk factors for heart disease include high cholesterol and smoking. 47% of all Americans have one of these three risk factors. [32] Lifestyle choices, such as poor diet and physical inactivity, and excessive alcohol use can also contribute to cardiovascular disease. Medical conditions, like diabetes and obesity can also be risk factors. [32]
In the United States, 610,000 people die every year from heart disease which is equal to 1 in 4 deaths. The leading cause of death for both men and women in the United States is heart disease. [32] In Canada, heart disease is the second leading cause of death. In 2014, it was the cause of death for 51,000 people. [33] In Australia, heart disease is also the leading cause of death. 29% of deaths in 2015, had an underlying cause of heart disease. [34] Heart disease causes one in four premature deaths in the United Kingdom and in 2015 heart disease caused 26% of all deaths in that country. [35]
People of lower socio-economic status are more likely to have cardiovascular disease than those who have a higher socio-economic status. [36] This inequality gap has occurred in developed countries because people who have a lower socio-economic status often face many of the risk factors of tobacco and alcohol use, obesity as well as having a sedentary lifestyle. [37] Further social and environmental factors such as poverty, pollution, family history, housing and employment contribute to this inequality gap and to risk of having a health condition caused by cardiovascular disease. The increasing inequality gap between the higher and lower income populations continues in countries such as Canada, despite the availability of health care for everyone. [38]
Dementia is a chronic syndrome which is characterized by deterioration in the thought process beyond what is expected from normal aging. It affects the persons memory, thinking, orientation, comprehension, behavior and ability to perform everyday activity. There are many different forms of dementia . Alzheimer is the most common form which contributes to 60–70% of the dementia cases. Different forms of dementia can co-exist. Young onset dementia which occurs in individuals before the age of 65 contributes to 9% of the total cases. It is the major cause of disability and dependency among old people. [39]
Worldwide, there are 50 million people who are suffering from dementia and every year 10 million new cases are being reported. The total number of people with dementia is projected to reach 82 million by 2030 and 152 million in 2050 . [39]
According to CDC, Alzheimer is the 6th leading cause of death in U.S adults and 5th leading cause of death in adults over the age of 65. In 2014, 5 million Americans above the age of 65 were diagnosed with Alzheimer. This number is predicted to triple by the year 2060 and reach up to 14 million. Dementia and Alzheimer has been shown to go unreported on death certificates, leading to under representation of the actual mortality caused by these diseases. [40] Between 2000 and 2015, mortality due to cardiovascular diseases has decreased by 11%, where as death from Alzheimer has increased by 123%. 1 in 3 people over the age of 65 die from Alzheimer or other forms of dementia. Furthermore, 200,000 individuals have been affected by young onset dementia. In United States, Alzheimer affects more women than men. It is twice more common in African-Americans and Hispanics than in whites. As the number of older Americans increases rapidly, the number of new cases of Alzheimer will rise too . [41]
East Asia has the most people living with dementia (9.8 million) followed by Western Europe (7.5 million), South Asia (5.1 million) and North America (4.8 million). [42] In 2016, the prevalence of Alzheimer was 5.05% in Europe. Like in United States, it is more prevalent in women than in men. [43] In the European Union, Finland has the highest mortality among both men and women due to dementia. [44] In Canada, over half a million people are living with dementia. It is projected that by 2031 the number will go up by 66% to 937,000. Every year 25,000 new cases of dementia are diagnosed . [45]
Dementia is the second leading cause of death in Australia. In 2016, it was the leading cause of deaths in females. In Australia 436,366 people are living with dementia in 2018. 3 in 10 people over the age of 85 and 1 in 10 people over the age of 65 have dementia. It is the single greatest cause of disability in older Australians . [46] Rates of dementia are higher for indigenous people. In people from the northern territory and western Australia the prevalence of dementia is 26 times higher in the 45–69 year old group and about 20 times greater in 60–69 year old group. [47]
The risk factors for developing dementia or Alzheimer's include age, family history, genetic factors, environmental factors, brain injury, viral infections, neurotoxic chemicals, and various immunological and hormonal disorders. [48] [49]
A new research study has found an association between the affluence of a country, hygiene conditions and the prevalence of Alzheimer in their population. According to the Hygiene Hypothesis, affluent countries with more urbanized and industrialized areas have better hygiene, better sanitation, clean water and improved access to antibiotics. [50] This reduces the exposure to the friendly bacteria, virus and other microorganisms that help stimulate our immune system. Decreased microbial exposure leads to immune system that is poorly developed, which exposes the brain to inflammation as is seen in Alzheimer's disease. [51]
Countries like the UK and France that have access to clean drinking water, improved sanitation facilities and have a high GDP show a 9% increase in Alzheimer's disease as opposed to countries like Kenya and Cambodia. [51] Also countries like UK and Australia, where three quarters of their population lives in urban areas, have a 10% higher Alzheimer's rate than in countries like Bangladesh and Nepal where less than one tenth of their population live in urban areas. [51]
Alzheimer's risk changes with the environment. Individuals from the same ethnic background living in an area of low sanitation will have a lower risk as compared to the same individuals living in an area of high sanitation who will be exposed to a higher risk of developing Alzheimer's. An African-American in U.S. has a higher risk of developing Alzheimer's as compared to one living in Nigeria. [50] Immigrant populations exhibit Alzheimer disease rates intermediate between their home country and adopted country. Moving from a country of high sanitation to a country of low sanitation reduces the risk associated with the disease. [50]
People who face poverty have more risks related to having a mental illness and also do not have as much access to treatment. The stressful events that they face, unsafe living condition and poor physical health lead to cycle of poverty and mental illness that is seen all over the world. [52] According to the World Health Organization 76–85% of people living in lower and middle income countries are not treated for their mental illness. For those in higher-income counties, 35–50% of people with mental illness do not receive treatment. [53] It is estimated that 90% of deaths by suicide are caused by substance use disorders and mental illness in higher income countries. In lower to middle income countries, this number is lower. [54]
One in four people have experienced mental illness at one time in their lives, and approximately 450 million people in the world currently have a mental illness. [52] Those who are impoverished live in conditions associated with a higher risk for mental illness and, to compound the issue, do not have as much access to treatment. Stress, unsafe living conditions, and poor physical health associated with lack of sufficient income lead to a cycle of poverty and mental illness that is observed worldwide. [52] Of all countries, India, China, and the United States have the highest levels of anxiety, depression and schizophrenia, according to the WHO. The proportion of people with depression is between 2% to 6%; Greenland, Australia, and the United States have the highest rates of this disorder. Of these three, the U.S. is reported to have the greatest rate of depression. [55] In the U.S., approximately one in five adults has a mental illness, or 44.7 million people. [56] In 2016, it was estimated that 268 million people in the world had depression. [54]
Anxiety disorders, such as generalized anxiety, Obsessive Compulsive Disorder, and Post Traumatic Stress Disorder affected 275 million people worldwide in 2016. The global proportion of people affected by anxiety disorders is between 2.5–6.5%. Australia, Brazil, Argentina, Iran, the United States, and a number of countries in Western Europe appear to have a higher prevalence of anxiety disorders. [54]
Cancer is a generic term for a large group of disease which is characterized by rapid creation of abnormal cells that grow beyond their usual boundaries. These cells can invade adjoining parts of the body and spread to other organs causing metastases, which is a major cause of death. According to WHO, Cancer is the second leading cause of death globally. One in six deaths worldwide are caused due to cancer, accounting to a total of 9.6 million deaths in 2018.Tracheal, bronchus, and lung cancer is the leading form of cancer deaths across most high and middle-income countries. [57]
In United States, 1,735,350 new cases of cancer will be diagnosed in 2018. Most common forms of cancer are cancer of the breast, lung, bronchus, prostate, colorectal cancer, melanoma of skin, Non-Hodgkin's lymphoma, renal cancer, thyroid cancer and liver cancer. Cancer mortality is higher among men than in women. African-Americans have the highest risk of mortality due to cancer. [58] Cancer is also the leading cause of death in Australia. The most common cancers in Australia are prostate, breast, colorectal, melanoma and lung cancer. These account for 60% of the cancer cases diagnosed in Australia. [59]
Europe contains only 1/8 of the world population, but has around one quarter of the global cancer cases, with 3.7 million new cases each year. Lung, breast, stomach, liver, colon are the most common cancers in Europe. [60] The overall incidences among different cancers vary across countries. [61]
About one in two Canadians will develop cancer in their lifetime, and one in four will die of the disease. In 2017, 206,200 new cases of cancer were diagnosed. Lung, colorectal, breast, and prostate cancer accounted for about half of all cancer diagnoses and deaths. [62]
High prevalence of cancer in high-income countries is attributed to lifestyle factors like obesity, smoking, physical inactivity, diet and alcohol intake. [57] Around 40% of the cancers can be prevented by modifying these factors. [63]
The rate of allergies around the world has risen in industrialized nations over the past 50 years. [64] A number of public health measures, such as sterilized milk, use of antibiotics and improved food production have contributed to a decrease in infections in developed countries. There is a proposed causal relationship, known as the "hygiene hypothesis" that indicates that there are more autoimmune disorders and allergies in developed countries with fewer infections. [65] In developing countries, it is assumed that the rates of allergies are lower than developed countries. That assumption may not be accurate due to limited data on prevalence. [66] Research has found an increase in asthma by 10% in countries such as Peru, Costa Rica, and Brazil. [65]
Diseases of poverty (also known as poverty related diseases ) are diseases that are more prevalent in low-income populations. [1] They include infectious diseases, as well as diseases related to malnutrition and poor health behaviour. Poverty is one of the major social determinants of health. The World Health Report, 2002 states that diseases of poverty account for 45% of the disease burden in the countries with high poverty rate which are preventable or treatable with existing interventions. [2] Diseases of poverty are often co-morbid and ubiquitous with malnutrition. [3]
Poverty increases the chances of having these diseases as the deprivation of shelter, safe drinking water, nutritious food, sanitation, and access to health services contributes towards poor health behaviour. At the same time, these diseases act as a barrier for economic growth to affected people and families caring for them which in turn results into increased poverty in the community. [4]
These diseases produced in part by poverty are in contrast to diseases of affluence , which are diseases thought to be a result of increasing wealth in a society. [5]
Human Immunodeficiency Virus (HIV) , Malaria and Tuberculosis (TB) also known as “the big three” have been acknowledged as infectious diseases that disproportionately affect developing countries. [6] Poverty and infectious diseases are causally related. Even before the time of vaccines and antibiotics, before 1796, it can be speculated that, leaders were adequately protected in their castles with decent food and standard accommodation, conversely the vast majority of people were living in modest, unsanitary homes; cohabiting with their animals. [7] [8] [9] During this time people were unknowingly dying of infectious diseases in an event that; they touched their sick animals, had cuts in their skins, drank something that was not boiled or ate food that was contaminated by microbes. To exacerbate the situation, epidemics known as plagues then would emerge and wipe out the whole community. [10] During this time, people had no knowledge on the cause of these unfavourable series of events. After speculations that their illnesses were being caused by an invisible army of tiny living beings, microorganisms, Antoni van Leeuwenhoek invented the first microscope that confirmed the existence of microorganisms that cannot be visualised with the naked eye (around the 17th century). [11] [12]
HIV is a viral illness that can be transmitted sexually, by transfusion, shared needles and during child birth from mother to child. Due to its long latent period, there is a danger of its spread without action. [13] It affects the human body by targeting T-cells, that are responsible for protection from uncommon infections and cancers. It is managed by life prolonging drugs known as Antiretroviral drugs (ARVs). TB was discovered by Robert Koch in 1882. [14] [15] It is characterised by fever, weight loss, poor appetite and night sweats. Throughout the years, there has been an improvement in mortality and morbidity caused by TB. This improvement has been attributed to the introduction of the TB vaccine in 1906. Despite this, each year the majority infected by TB are the poor. Finally, Malaria used to be prevalent throughout the world. It is now limited to developing and warm regions; Africa, Asia and South America.
For many environmental and social factors, including poor housing conditions and working conditions, inadequate sanitation , and disproportionate occupation as sex workers , the poor are more likely to be exposed to infectious diseases .  Malnutrition, mental stress, overwork, inadequate knowledge, and minimal health care can hinder recovery and exacerbate the disease. [16] Malnutrition is associated with 54% of childhood deaths from diseases of poverty, and lack of skilled attendants during childbirth is primarily responsible for the high maternal and infant death rates among the poor. [17]
Lack of exercise is an issue strongly related to poverty, due to lack of access to suitable recreational areas. The lack of physical activity increases the risk of developing chronic health diseases, cancer as well as decreasing one's quality of life. [18] Poverty is a risk factor for many different health issues, which can be impacted by their lack of access. Obesity and risks of chronic health diseases can be prevented through increasing physical activity and being able to have access to places to exercise. Physical inactivity isn't just a personal choice, but one linked to socioeconomic status as well.
For individuals in poverty, it can be difficult to find a place to exercise. Within low income neighbourhood or towns, there are fewer opportunities to increase physical activity due to the lack of; parks, opportunities within the schools to participate in sports or recreational activities , and recreational facilities within the community. [19] In low income communities only about one in five homes have parks within a half-mile distance, and about the same number have a fitness or recreation center within that distance. [20] Since there are a lack of places to increase physical activity, the rates of obesity and chronic health diseases are on a rise among those in poverty.
One of the major concerns for impoverished neighborhoods is safety, which is a determinant of how often people exercise within the community. The ability to find transportation can also cause issues within the lack of access to exercise because of transportation and even the expense at which parents might pay if transportation is available. Children and adults who do not exercise frequently lower their quality of life, which will impact them as they age. [18] One in three children are physically active on a daily basis, and children spend seven or more hours a day is spent in front of a screen whether it be a computer, a TV, or video games. [20] By just participating in exercise for 30 minutes, 3 times a week can show many benefits on one's life. [21] Some examples of benefits from exercise include; managing weight better, decreasing risk for heart disease and heart attacks, lowering blood pressure, shorter recovery times from injury, improves mood and sleep patterns, increases social contact, and makes one feel better overall. [21]
Mental health is “a state of successful performance of mental function, resulting in productive activities, fulfilling relationships with other people, and the ability to adapt to change and to cope with adversity”. [22] Poverty has a profound effect on a person's mental health . According to Alyssa Brown of the Washington D.C. Gallup, 31% of people living in poverty have reported at some point been diagnosed with depression compared with 15.8% of those not in poverty. Many people attribute their depression to unemployment, life stressors, and witnessing more violence. These are very relevant in the impoverished world.
It is uncertain whether poverty induces depression or depression causes poverty. What is certain is that the two are closely linked. [23] A reason for this link could be due to the lack of support groups such as church community centers. Isolation plays an integral role in depression. For example, results from a cohort study of approximately 2,000 older adults aged 65 years and older from the New Haven Established Populations for the Epidemiological Study of the Elderly found that social engagement was associated with lower depression scores after adjustment for various demographic characteristics, physical activity and functional status. [22] This proves that an increase in community based centers, should decrease mental illness in high poverty areas of the United States.
Each year many children and adults die as a result of a lack of access to clean drinking water and poor sanitation. Many poverty related diseases such as diarrhea acquire and spread as a result of inadequate access to clean drinking water.  According to UNICEF, 3,000 children die every day, worldwide due to contaminated drinking water and poor sanitation. [24]
Although the Millennium Development Goal (MDG) of halving the number of people who did not have access to clean water by 2015, was reached five years ahead of schedule in 2010, there are still 783 million people who rely on unimproved water sources. [24] In 2010 the United Nations declared access to clean water a fundamental human right, integral to the achievement of other rights.  This made it enforceable and justifiable to permit governments to ensure their populations access to clean water. [25] Though access to water has improved for some, it continues to be especially difficult for women and children.  Women and girls bear most of the burden for accessing water and supplying it to their households.
In India , Sub-Saharan Africa , and parts of Latin America, women are required to travel long distances in order to access a clean water source and then bring some water home. This has a significant impact on girls’ educational attainment. [25] [26]
There have been further efforts to improve water quality using new technology which allows water to be disinfected immediately upon collection and during the storage process.  Clean water is necessary for cooking, cleaning, and laundry because many people come into contact with disease causing pathogens through their food, or while bathing or washing. [27]
An ongoing issue of contaminated water in the United States has been taking place in Flint, Michigan . On September 4, 2018, evidence of E Coli and other organisms that can cause disease was found in the water. The issue of contaminated water in Flint, Michigan started when the source for drinking water in Flint was  changed from the Lake Huron and the Detroit River to the very cheap Flint River. [ citation needed ]
Education is affected by poverty which is known as the income achievement gap. This gap shows that children living in poverty or have lower-income are less likely to have the cognitive and early literacy levels of those who don't. [28] The amount of income affects the amount of extra money a family has to spend on additional educational programs; including summer camps and out of school assistance. In addition to finances, environmental toxins, including lead and stress and lack of nutritious food can diminish cognitive development. [28]
In later education, students considered low-income or in poverty are more likely to dropout of school or only receive a high school diploma. [29] The failure to achieve higher levels of education attributes to the cycle of poverty which can continue for generations in the same family and even in the community. [29] Higher educational achievement correlates with achieving a more secure job and economic future. [30]
One in three people worldwide do not have access to adequate sanitation . Inadequate sanitation can lead to diarrheal diseases that often result in serious illness and not uncommonly, death—especially in children. These diarrheal diseases contribute not only to the decreased health of an individual, but also to an increase in poverty. Diseases of this nature cause an inability to attend school and work, thus directly decreasing income as well as educational development. [ citation needed ] The problem of inadequate sanitation is cyclical in nature—just as it is caused by poverty, it also worsens poverty.
Malnutrition disproportionately affects those in sub-Saharan Africa. Over 35 percent of children under the age of 5 in sub-Saharan Africa show physical signs of malnutrition. [31] Malnutrition, the immune system, and infectious diseases operate in a cyclical manner: infectious diseases have deleterious effects on nutritional status, and nutritional deficiencies can lower the strength of the immune system which affects the body's ability to resist infections. [31] Similarly, malnutrition of both macronutrients (such as protein and energy ) and micronutrients (such as iron , zinc , and vitamins ) increase susceptibility to HIV infections by interfering with the immune system and through other biological mechanisms. Depletion of macro-nutrients and micro-nutrients promotes viral replication that contributes to greater risks of HIV transmission from mother-to-child as well as those through sexual transmission. [32] Increased mother-to-child transmission is related to specific deficiencies in micro-nutrients such as vitamin A. [33] [34] Further, anemia , a decrease in the number of red blood cells , increases viral shedding in the birth canal, which also increases risk of mother-to-child transmission. [35] Without these vital nutrients, the body lacks the defense mechanisms to resist infections. [31] At the same time, HIV lowers the body's ability to intake essential nutrients. HIV infection can affect the production of hormones that interfere with the metabolism of carbohydrates, proteins, and fats. [31]
In the United States, 11.1 percent of households struggle with food insecurity. [36] Food insecurity refers to the lack of access to quality food for a healthy lifestyle. [36] The rate of hunger and malnutrition in female headed households was three times the national average at 30.2 percent. According to the Food and Agriculture Organization of the United Nations, 10 percent of the population in Latin America and the Caribbean are affected by hunger and malnutrition. [37]
Families living in poverty often struggle not only with housing problems, but neighborhood safety and affordability problems as well. [38] Avoiding neighborhood safety problems often means staying home which reduces opportunity for exercise outside the home which exacerbates health issues due to lack of exercise. Staying in the home can mean exposure to lead , mold and rodents within that home that can lead to an increased risk of illness due to these inadequate housing issues. [38]
According to WHO, medical strategies report, approximately 30% of the global population does not have regular access to medicines. In the poorest parts of Africa and Asia, this percent goes up to 50%. [39] The population below the poverty line lacks access due to higher retail price and unavailability of the medicines. The higher cost can be due to the higher manufacturing price or due to local or regional tax and Value Added Tax. There is a significant disparity in the research conducted in the health sector. It is claimed that only 10% of the health research conducted globally focuses on 90% disease burden. However, diseases such as cancer, cardiovascular diseases etc. that traditionally were associated with the wealthier community are now becoming more prevalent in the poor communities as well. Hence, the research conducted now is relevant to poor population. [40] Political priority is also one of the contributing factors of inaccessibility. The government of poor countries may allocate less funding to public health due to the scarcity of resources.
The cycle of poverty is the process through which families already in poverty are likely to remain in those circumstances unless there is an intervention of some kind. This cycle of poverty has an impact on the types of diseases that are experienced by these individuals, and will often be passed down through generations. Mental illnesses are particularly important when discussing the cycle of poverty, because these mental illnesses prevent individuals from obtaining gainful employment. [41] The stressful experience of living in poverty can also exacerbate mental illnesses. [41]
This cycle of poverty also impacts the familial diseases that are passed down each generation. [42] By experiencing the same stressful situations for decades, individuals become more susceptible to diseases like cardiovascular disease , obesity , diabetes , and mental illnesses including schizophrenia and bipolar disorder .
Together, diseases of poverty kill approximately 14 million people annually. [43] Gastroenteritis with its associated diarrhea results in about 1.8 million deaths in children yearly with most of these in the world's poorest nations. [44]
At the global level, the three primary PRDs are tuberculosis, AIDS/HIV and malaria. [45] Developing countries account for 95% of the global AIDS prevalence [46] and 98% of active tuberculosis infections. [43] Furthermore, 90% of malaria deaths occur in sub-Saharan Africa. [47] Together, these three diseases account for 10% of global mortality . [45]
Treatable childhood diseases are another set which have disproportionately higher rates in poor countries despite the availability of cures for decades. These include measles , pertussis and polio . [40] The largest three poverty-related diseases (PRDs)  — AIDS , malaria , and tuberculosis — account for 18% of diseases in poor countries. [40] The disease burden of treatable childhood diseases in high-mortality, poor countries is 5.2% in terms of disability-adjusted life years but just 0.2% in the case of advanced countries. [40]
In addition, infant mortality and maternal mortality are far more prevalent among the poor. For example, 98% of the 11,600 daily maternal and neonatal deaths occur in developing countries . [17]
Three other diseases, measles , pneumonia , and diarrheal diseases , are also closely associated with poverty, and are often included with AIDS, malaria, and tuberculosis in broader definitions and discussions of diseases of poverty. [48]
Based upon the spread of research in cures for diseases, certain diseases are identified and referred to as " neglected diseases ". These include the following diseases: [40]
Tropical diseases such as these tend to be neglected in research and development efforts. Of 1393 new drugs brought into use over a period of 25 years (1975–1999), only a total of thirteen, less than 1%, related to these diseases. Of 20 MNC drug companies surveyed for research on PRDs, only two had projects targeted towards these neglected PRDs. However, the combined total number of deaths due to these diseases is dwarfed by the enormous number of patients affected by PRDs such as respiratory infections, HIV/AIDS, diarrhea and tuberculosis, besides many others. [40] Similar to the spread of tropical neglected diseases in developing nations, these neglected infections disproportionately affect poor and minority populations in the United States. [49] These diseases have been identified by the Centers for Disease Control and Prevention, as priorities for public health action based on the number of people infected, the severity of the illnesses, and the ability to prevent and treat them. [50]
Trichomoniasis is the most common sexually transmitted infection affecting more than 200 million people worldwide. It is especially prevalent among young, poor and African American women.  This infection is also common in poor communities in Sub-Saharan Africa and impoverished parts of Asia. This neglected infection is one of special concern because it is associated with a heightened risk for contracting HIV and pre-term deliveries. [51]
In addition, availability of cures and recent advances in medicine have led to only three diseases being considered neglected diseases, namely, African trypanosomiasis, Chagas disease and Leishmaniasis. [40]
Africa accounts for a majority of malaria infections and deaths worldwide. Over 80 percent of the 300 to 500 million malaria infections occurring annually worldwide are in Africa. [52] Each year, about one million children under the age of five die from malaria. [53] Children who are poor, have mothers with little to no education, and live in rural areas are more susceptible to malaria and more likely to die from it. [54] Malaria is directly related to the spread of HIV in sub-Saharan Africa . [55] It increases viral load seven to ten times, which increases the chances of transmission of HIV through sexual intercourse from a patient with malaria to an uninfected partner. [56] After the first pregnancy , HIV can also decrease the immunity to malaria. This contributes to the increase of the vulnerability to HIV and higher mortality from HIV, especially for women and infants. [57] HIV and malaria interact in a cyclical manner—being infected with malaria increases susceptibility to HIV infection, and HIV infections increase malarial episodes. The co-existence of HIV and malaria infections helps spread both diseases, particularly in Sub-Saharan Africa. [58] Malaria vaccines are an area of intensive research.
Intestinal parasites are extremely prevalent in tropical areas. [59] These include hookworms , roundworms , and other amoebas . They can aggravate malnutrition by depleting essential nutrients through intestinal blood loss and chronic diarrhea . Chronic worm infections can further burden the immune system. [60] [61] At the same time, chronic worm infections can cause immune activation that increases susceptibility of HIV infection and vulnerability to HIV replication once infected.
Schistosomiasis (bilharzia) is a parasitic disease caused by the parasitic flatworm trematodes . Moreover, more than 80 percent of the 200 million people worldwide who have schistosomiasis live in sub-Saharan Africa. [62] Infections often occur in contaminated water where freshwater snails release larval forms of the parasite. After penetrating the skin and eventually traveling to the intestines or the urinary tract, the parasite lays eggs and infects those organs. [59] [62] It damages the intestines , bladder , and other organs and can lead to anemia and protein-energy deficiency. [63] [64] Along with malaria, schistosomiasis is one of the most important parasitic co-factors aiding in HIV transmission. Epidemiological data shows schistosome-endemic areas coincide with areas of high HIV prevalence, suggesting that parasitic infections such as schistosomiasis increase risk of HIV transmission. [65]
Tuberculosis is the leading cause of death around the world for an infectious disease. [66] This disease is especially prevalent in sub-Saharan Africa, and the Latin American and Caribbean region.  While the tuberculosis rate is decreasing in the rest of the world, it is increasing by rate of 6 percent per year in Sub-Saharan Africa.  It is the leading cause of death for people with HIV in Africa. Tuberculosis (TB) is closely related to lifestyles of poverty, overcrowded conditions, alcoholism, stress, drug addiction and malnutrition.   This disease spreads quickly among people who are undernourished. [3] According to the Center for Disease Control and Prevention, in the United States, tuberculosis is more prevalent among foreign born persons, and ethnic minorities.  The rates are especially high among Hispanics, Blacks and Asians. [67] [68] HIV infection and TB are also closely tied.  Being infected with HIV increases the rate of activation of latent TB infections, and having TB, increases the rate of HIV replication, therefore accelerating the progression of AIDS. [3]
AIDS is a disease of the human immune system caused by the human immunodeficiency virus (HIV). [69] Primary modes of HIV transmission in sub-Saharan Africa are sexual intercourse , mother-to-child transmission (vertical transmission), and through HIV-infected blood. [59] [70] [71] Since rate of HIV transmission via heterosexual intercourse is so low, it is insufficient to cause AIDS disparities between countries. [59] Critics of AIDS policies promoting safe sexual behaviors believe that these policies miss the biological mechanisms and social risk factors that contribute to the high HIV rates in poorer countries. [59] In these developing countries, especially those in sub-Saharan Africa, certain health factors predispose the population to HIV infections. [33] [63] [72] [73] [74]
Many of the countries in Sub-Saharan Africa are ravaged with poverty and many people live on less than one United States dollar a day. [75] The poverty in these countries gives rise to many other factors that explain the high prevalence of AIDS. The poorest people in most African countries suffer from malnutrition, lack of access to clean water, and have improper sanitation.  Because of a lack of clean water many people are plagued by intestinal parasites that significantly increase their chances of contracting HIV due to compromised immune system.  Malaria, a disease still rampant in Africa also increases the risk of contracting HIV.  These parasitic diseases, affect the body's immune response to HIV, making people more susceptible to contracting the disease once exposed.  Genital schistosomiasis, also prevalent in the topical areas of Sub-Saharan Africa and many countries worldwide, produces genital lesions and attract CD4 cells to the genital region which promotes HIV infection.  All these factors contribute to the high rate of HIV in Sub-Saharan Africa. Many of the factors seen in Africa are also present in Latin America and the Caribbean and contribute to the high rates of infections seen in those regions.  In the United States, poverty is a contributing factor to HIV infections. There is also a large racial disparity , with African Americans having a significantly higher rate of infection than their white counterparts. [75]
More than 300 million people worldwide have asthma . The rate of asthma increases as countries become more urbanized and in many parts of the world those who develop asthma do not have access to medication and medical care. [76] Within the United States, African Americans and Latinos are four times more likely to suffer from severe asthma than whites. The disease is closely tied to poverty and poor living conditions. [77] Asthma is also prevalent in children in low income countries. Homes with roaches and mice, as well as mold and mildew put children at risk for developing asthma as well as exposure to cigarette smoke. [78]
Unlike many other Western countries, the mortality rate for asthma has steadily risen in the United States over the last two decades. [79] Mortality rates for African American children due to asthma are also far higher than that of other racial groups. [80] For African Americans, the rate of visits to the emergency room is 330 percent higher than their white counterparts. The hospitalization rate is 220 percent higher and the death rate is 190 percent higher. [78] Among Hispanics, Puerto Ricans are disporpotionatly affected by asthma with a disease rate that is 113 percent higher than non-Hispanic Whites and 50 percent higher than non-Hispanic Blacks. [78] Studies have shown that asthma morbidity and mortality are concentrated in inner city neighborhoods characterized by poverty and large minority populations and this affects both genders at all ages. [81] [82] Asthma continues to have an adverse effects on the health of the  poor and school attendance rates among poor children. 10.5 million days of school are missed each year due to asthma. [78]
Though heart disease is not exclusive to the poor, there are aspects of a life of poverty that contribute to its development.  This category includes coronary heart disease , stroke and heart attack .  Heart disease is the leading cause of death worldwide and there are disparities of morbidity between the rich and poor.  Studies from around the world link heart disease to poverty. Low neighborhood income and education were associated with higher risk factors.   Poor diet, lack of exercise and limited (or no) access to a specialist were all factors related to poverty, though to contribute to heart disease. [83] Both low income and low education were predictors of coronary heart disease, a subset of cardiovascular disease.  Of those admitted to hospital in the United States for heart failure, women and African Americans were more likely to reside in lower income neighborhoods.  In the developing world, there is a 10 fold increase in cardiac events in the black and urban populations. [84]
Obstetric fistula or vaginal fistula is a medical condition in which a fistula (hole) develops between either the rectum and vagina (see rectovaginal fistula ) or between the bladder and vagina (see vesicovaginal fistula ) after severe or failed childbirth , when adequate medical care is not available. [85] It is considered a disease of poverty because of its  tendency to occur women in poor countries who do not have health resources comparable to developed nations. [86]
Dental decay or dental caries is the gradual destruction of tooth enamel.  Poverty is a significant determinant for oral health. [87] Dental caries is one of the most common chronic diseases worldwide.  In the United States it is the most common chronic disease of childhood.  Risk factors for dental caries includes living in poverty, poor education, low socioeconomic status, being part of an ethnic minority group, having a developmental disability, recent immigrants and people infected with HIV/AIDS. [88] In Peru, poverty was found to be positively correlated with dental caries among children. [89] According to a report by U.S health surveillance, tooth decay peaks earlier in life and is more severe in children with families living below the poverty line. [89] Tooth decay is also strongly linked to dietary behaviors, and  in poor rural areas where nutrient dense foods, fruits and vegetables are unavailable, the consumption  of sugary and fatty food increases the risk of dental decay. [90] Because the mouth is a gateway to the respiratory and digestive tracts, oral health has a significant impact on other health outcomes.  Gum disease has been linked to diseases such as cardiovascular disease. [91]
Diseases of poverty reflect the dynamic relationship between poverty and poor health; while such diseases result directly from poverty, they also perpetuate and deepen impoverishment by sapping personal and national health and financial resources. For example, malaria decreases GDP growth by up to 1.3% in some developing nations, and by killing tens of millions in sub-Saharan Africa, AIDS alone threatens “the economies, social structures, and political stability of entire societies”. [92] [93]
Women and children are often put at a high risk of being infected by schistosomiasis, which in turn puts them at a higher risk of acquiring HIV. [59] Since the mode of schistosomiasis transmission is usually through contaminated water in streams and lakes, women and children who do their household chores by the water are more likely to acquire the disease. Activities that women and children often do around waterfront include washing clothes, collecting water, bathing, and swimming. [59] [62] Women who have schistosomiasis lesions are three times more likely to be infected with HIV. [94]
Women also have a higher risk of HIV transmission through the use of medical equipment such as needles. [59] Because more women than men use health services, especially during pregnancy, they are more likely to come across unsterilized needles for injections. [70] [94] Although statistics estimate that unsterilized needles only account for 5 to 10 percent of primary HIV infections, studies show this mode of HIV transmission may be higher than reported. [59] [95] This increased risk of contracting HIV through non-sexual means has social consequences for women as well. Over half of the husbands of HIV-positive women in Africa tested HIV-negative. [96] When HIV-positive women reveal their HIV status to their HIV-negative husbands, they are often accused of infidelity and face violence and abandonment from their family and community. [59] [96]
Malnutrition associated with HIV impacts people's ability to provide for themselves and their dependents, thus limiting the human capabilities of both themselves and their dependents. [31] HIV can negatively affect work output, which impacts the ability to generate income. [97] This is crucial in parts of Africa where farming is the primary occupation and obtaining food is dependent on the agricultural outcome. Without adequate food production , malnutrition becomes more prevalent. Children are often collateral damage in the AIDS crisis. As dependents, they can be burdened by the illness and eventual death of one or both parents due to HIV/AIDS. Studies have shown that orphaned children are more likely to display physical symptoms of malnutrition than children whose parents are both alive. [31]
There are a number of proposals for reducing the diseases of poverty and eliminating health disparities within and between countries. The World Health Organization proposes closing the gaps by acting on social determinants. [98] Their first recommendation is to improve daily living conditions.  This area involves improving the lives of women and girls so that their children are born in healthy environments and placing an emphasis on early childhood health.  
Their second recommendation is to tackle the inequitable distribution of money, power and resources.  This would involve building stronger public sectors and changing the way in which society is organized.  
Their third recommendation is to measure and understand the problem and assess the impact of action. This would involve training policy makers and healthcare practitioners to recognize problems and form policy solutions. [98]
The 8th Global Conference on Health Promotion held in Helsinki in June 2013 [99] has proposed an approach termed Health in All Policies. Health inequalities are shaped by many powerful forces and social, political, and economic determinants. Governments have a responsibility to ensure that their people are able to live healthy lives and have equitable access to achieving a reasonable state of good health. Policies that governments craft and implement in all sectors have a significant and ongoing impact on public health, health equity, and the lives of their citizens. Increases in technology, medical innovation, and living conditions have led to the disappearance of diseases and other factors contributing to poor health. However, there are many diseases of poverty that still persist in developed and developing countries. Tackling these health inequalities and diseases of poverty requires a willingness to engage the whole government in health. The Helskinki Statement lays out a framework of action for countries and calls on governments to make a commitment to building health equity within their country.
Health in All Policies (HiAP) is an approach to public policies across all sectors of government that takes into account the health implications of all government and policy decisions to improve health equity across all populations residing within the borders of a country. This concept is built upon principles in line with the Universal Declaration of Human Rights, The United Nations Millennium Development Declaration, and principles of good governance: [99] legitimacy given by national and international law, accountability of government, transparency of policy making, participation of citizens, sustainability ensuring policies meet the needs of both present and future generations, and collaboration across sectors and levels of government.
Finally the Framework lists and expands upon six steps for implementation [99] that may be undertaken by a country in taking action towards Health in All Policies. These are components of action and not a rigid checklist of steps to adhere to. The most important aspect of this policy is that governments should adapt the policy to suit the needs of their citizens, their socioeconomic situation, and their governance system.
This is a list of disorders included in newborn screening programs around the world, along with information on testing methodologies, disease incidence and rationale for being included in screening programs.
The following conditions and disorders were recommended as a "core panel" by the 2005 report of the American College of Medical Genetics (ACMG). [1] The incidences reported below are from the full report, though the rates may vary in different populations. [2]
Blood cell disorders
Inborn errors of amino acid metabolism
Inborn errors of organic acid metabolism
Inborn errors of fatty acid metabolism
Miscellaneous multisystem diseases
Newborn screening by other methods than blood testing

The following disorders are additional conditions that may be detected by screening. Many are listed as "secondary targets" by the 2005 ACMG report. [1] Some states are now screening for more than 50 congenital conditions. Many of these are rare and unfamiliar to pediatricians and other primary health care professionals. [1]
Blood cell disorders
Inborn errors of amino acid metabolism
Inborn errors of organic acid metabolism
Inborn errors of fatty acid metabolism
Miscellaneous multisystem diseases
In addition to identifying a core list of disorders that infants in the United States should be screened for, the ACMG also established a framework for nominating future conditions, and the structure under which those conditions should be considered.

Domestic violence occurs across the world, in various cultures, [1] and affects people across society, at all levels of economic status; [2] however, indicators of lower socioeconomic status (such as unemployment and low income) have been shown to be risk factors for higher levels of domestic violence in several studies. [3] In the United States, according to the Bureau of Justice Statistics in 1995, women reported a six times greater rate of intimate partner violence than men. [4] [5] However, studies have found that men are much less likely to report victimization in these situations. [6]
While some sources state that gay and lesbian couples experience domestic violence at the same frequency as heterosexual couples, [7] other sources report that domestic violence rates among gay, lesbian and bisexual people might be higher but more under-reported. [8]
According to various national surveys, the percentage of women who were ever physically assaulted by an intimate partner varies substantially by country: Barbados (30%), Canada (29%), Egypt (34%), New Zealand (35%), Switzerland (21%), United States (33%). [9] [10] Some surveys in specific places report figures as high as 50–70% of women who were ever physically assaulted by an intimate partner. [9] Others, including surveys in the Philippines and Paraguay , report figures as low as 10%. [9]
In India , around 70% of women are victims of domestic violence. [11] [12]
Statistics published in 2004, show that the rate of domestic violence victimisation for Indigenous women in Australia may be 40 times the rate for non-Indigenous women. [13]
South Africa is said to have the highest statistics of gender-based violence in the world, including rape and domestic violence (Foster 1999; The Integrated Regional Network [IRIN], Johannesburg, South Africa, May 25, 2002). [14] 80% of women surveyed in rural Egypt said that beatings were common and often justified, particularly if the woman refused to have sex with her husband. [15] Up to two-thirds of women in certain communities in Nigeria 's Lagos State say they are victims to domestic violence. [16]
In Turkey 42% of women over 15 have suffered physical or sexual violence [17]
Between 1993 and 2001, U.S. women reported intimate partner violence almost seven times more frequently than men (a ratio of 20:3). [18] Statistics for the year 1994 showed that more than five times as many females reported being victimized by an intimate than did males. [19]
Domestic violence during pregnancy can be missed by medical professionals because it often presents in non-specific ways. A number of countries have been statistically analyzed to calculate the prevalence of this phenomenon:
There are a number of presentations that can be related to domestic violence during pregnancy: delay in seeking care for injuries; late booking, non-attenders at appointments, self-discharge; frequent attendance, vague problems; aggressive or over-solicitous partner; burns , pain, tenderness, injuries; vaginal tears, bleeding, STDs ; and miscarriage . [ citation needed ]
Domestic violence against a pregnant woman can also affect the fetus and can have lingering effects on the child after birth.  Physical abuse is associated with neonatal death (1.5% versus 0.2%), and verbal abuse is associated with low birth weight (7.6% versus 5.1%). [27]
Due to social stigmas regarding male victimization, men who are victims of domestic violence face an increased likelihood of being overlooked by healthcare providers. [28] [29] [30] Women's violence towards men is a serious social problem. [31] While much attention has been focused on domestic violence against women, researchers argue that domestic violence against men is a substantial social problem worthy of attention. [5] The issue of victimization of men by women has been contentious, due in part to studies which report drastically different statistics regarding domestic violence.
Men and women commit equivalent or similar rates of minor intimate partner violence via physical altercation, but more severe perpetration of physical violence tends to be committed by men, [32] [33] and victimization reports generally show women being more likely to experience domestic violence than men. [34] A 2013 review of the literature that combined perpetration and victimization reports indicate that, worldwide, most studies only look at female victimization. The review examined studies from five continents and the correlation between a country's level of gender inequality and rates of domestic violence. The authors found that when partner abuse is defined broadly to include emotional abuse, any kind of hitting, and who hits first, partner abuse is relatively even. They also stated if one examines who is physically harmed and how seriously, expresses more fear, and experiences subsequent psychological problems, domestic violence is significantly gendered toward women as victims. [35]
Sherry Hamby argues that victimization reports are more reliable than perpetration reports and therefore studies showing women being more likely to suffer domestic violence than men are the accurate ones. [36] A 2016 meta-analysis indicated that the only risk factors for the perpetration of intimate partner violence that differ by gender are witnessing intimate partner violence as a child, alcohol use, male demand, and female withdrawal communication patterns. [37]
Some sources state that gay and lesbian couples experience domestic violence at the same frequency as heterosexual couples, [7] while other sources state domestic violence among gay and lesbian couples might be higher than among heterosexual couples, that gay, lesbian, and bisexual individuals are less likely to report domestic violence that has occurred in their intimate relationships than heterosexual couples are, or that lesbian couples experience domestic violence less than heterosexual couples do. [8] By contrast, some researchers commonly assume that lesbian couples experience domestic violence at the same rate as heterosexual couples, and have been more cautious when reporting domestic violence among gay male couples. [38] In a survey by the Canadian Government , some 19% of lesbian women reported being victimized by their partners. [39] Other research reports that lesbian relationships exhibit substantially higher rates of physical aggression. [40]
The U. S. Department of Health and Human Services reports that for each year between 2000 and 2005, "female parents acting alone" were most common perpetrators of child abuse. [41]
When it comes to domestic violence towards children involving physical abuse, research in the UK by the NSPCC indicated that "most violence occurred at home" (78 per cent). 40—60% of men and women who abuse other adults also abuse their children. [42] Girls whose fathers batter their mothers are 6.5 times more likely to be sexually abused by their fathers than are girls from non-violent homes. [43] In China in 1989, 39,000 baby girls died during their first year of life because they didn't receive the same medical care that would be given to a male child. [12]
In Asia alone, about one million children working in the sex trade are held in slavery -like conditions. [12]
Teen dating violence is a pattern of controlling behavior by one teenager over another teenager who are in a dating relationship. While there are many similarities to "traditional" domestic violence there are also some differences. Teens are much more likely than adults to become isolated from their peers as the result of controlling behavior by their boyfriend/girlfriend. Also, for many teens the abusive relationship may be their first dating experience and have never had a "normal" dating experience with which to compare it. While teenagers are trying to establish their sexual identities, they are also confronting violence in their relationships and exposure to technology. Studies document that teenagers are experiencing significant amounts of dating or domestic violence. Depending on the population studied and the way dating violence is defined, between 9 and 35% of teens have experienced domestic violence in a dating relationship. When a broader definition of abuse that encompasses physical, sexual, and emotional abuse is used, one in three teen girls is subjected to dating abuse." [44]
Additionally, a significant number of teens are victims of stalking by intimate partners. Although involvement with romantic relationships is a critical aspect of adolescence, these relationships also present serious risks for teenagers. Unfortunately, adolescents in dating relationships are at greater risk of intimate partner violence than any other age group. Approximately one third of adolescent girls are victims of physical, emotional, or verbal abuse from a dating partner. Estimates of sexual victimization range from 14% to 43% of girls and 0.3% to 36% for boys. According to the Center for Disease Control, in 2009, nearly 10% of students nationwide had been intentionally hit, slapped, or physically hurt by their boyfriend or girlfriend. Twenty-six percent of girls in a relationship reported being threatened with violence or experiencing verbal abuse; 13% reported being physically hurt or hit. [44]
Measures of the incidence of violence in intimate relationships can differ markedly in their findings depending on the measures used. Care is needed when using domestic violence statistics to ensure that both gender bias and under-reporting issues do not affect the inferences that are drawn from the statistics.
Some researchers, such as Michael P. Johnson, suggest that where and how domestic violence is measured also affects findings, and caution is needed to ensure statistics drawn from one class of situations are not applied to another class of situations in a way that might have fatal consequences. [45] Other researchers, such as David Murray Fergusson, counter that domestic violence prevention services, and statistics that they produce, target the extreme end of domestic violence and preventing child abuse rather than domestic violence between couples. [46]
A 1992 Council of Europe study on domestic violence against women found that 1 in 4 women experience domestic violence over their lifetimes and between 6 and 10% of women suffer domestic violence in a given year. [ citation needed ]
In the European Union , DV is a serious problem in the Baltic States . These three countries –  Estonia, Latvia, and Lithuania – have also lagged behind most post-communist countries in their response to DV. [47] The problem in these countries is very severe, and in 2013 a DV victim won a European Court of Human Rights case against Lithuania. [48] [49]
The British Crime Survey for 2006–2007 reported that 0.5% of people (0.6% of women and 0.3% of men) reported being victims of domestic violence during that year and 44.3% of domestic violence was reported to the police. According to the survey, 312,000 women and 93,000 men were victims of domestic violence. [50]
The Northern Ireland Crime Survey for 2005 reported that 13% of people (16% of women and 10% of men) reported being victims of domestic violence at some point in their lives. [51]
The National Study of Domestic Abuse for 2005 reported that 213,000 women and 88,000 men reported being victims of domestic violence at some point in their lives. According to the study, one in seven women and one in sixteen men were victims of severe physical abuse, severe emotional abuse, or sexual abuse. [5]
In the United Kingdom, the police estimate that around 35% of domestic violence against women is actually reported. [ citation needed ] A 2002 Women's Aid study found that 74% of separated women suffered from post-separation violence. [ citation needed ]
In Canada, the Assembly of First Nations evaluation of the Canada Prenatal Nutrition Program conducted by CIET offers an inclusive and relatively unbiased national estimate. It documented domestic violence in a random sample of 85 First Nations across Canada: 22% (523/2359) of mothers reported suffering abuse in the year prior to being interviewed; of these, 59% reported physical abuse. [52]
Results of studies which estimate the prevalence of domestic violence vary significantly, depending on specific wording of survey questions, how the survey is conducted, the definition of abuse or domestic violence used, the willingness or unwillingness of victims to admit that they have been abused and other factors. For instance, Straus (2005) conducted a study which estimated that the rate of minor assaults by women in the United States was 78 per 1,000 couples, compared with a rate for men of 72 per 1,000 and the severe assault rate was 46 per 1,000 couples for assaults by women and 50 per 1,000 for assaults by men. Neither difference is statistically significant. He claimed that since these rates were based exclusively on information provided by women respondents, the near-equality in assault rates could not be attributed to a gender bias in reporting. [53]
One analysis found that "women are as physically aggressive or more aggressive than men in their relationships with their spouses or male partners". [5] However, studies have shown that women are more likely to be injured. Archer's meta-analysis [54] found that women in the United States suffer 65% of domestic violence injuries. A Canadian study showed that 7% of women and 6% of men were abused by their current or former partners, but female victims of spousal violence were more than twice as likely to be injured as male victims, three times more likely to fear for their life, twice as likely to be stalked, and twice as likely to experience more than ten incidents of violence. [55] However, Straus notes that Canadian studies on domestic violence have simply excluded questions that ask men about being victimized by their wives. [53]
According to a 2004 survey in Canada, the percentages of males being physically or sexually victimized by their partners was 6% versus 7% for women. However, females reported higher levels of repeated violence and were more likely than men to experience serious injuries; 23% of females versus 15% of males were faced with the most serious forms of violence including being beaten, choked, or threatened with or having a gun or knife used against them. Also, 21% of women versus 11% of men were likely to report experiencing more than 10 violent incidents. Women who often experience higher levels of physical or sexual violence from their current partner, were 44%, compared with 18% of men to suffer from an injury. Cases in which women are faced with extremely abusive partners, results in the females having to fear for their lives due to the violence they had faced. In addition, statistics show that 34% of women feared for their lives, and 10% of men feared for theirs. [56]
Some studies show that lesbian relationships have similar levels of violence as heterosexual relationships. [57]
Approximately 1.3 million women and 835,000 men report being physically assaulted by an intimate partner annually in the United States. [58] In the United States, domestic violence is the leading cause of injury to women between the ages of 15 and 44. [59]
Victims of DV are offered legal remedies, which include the criminal law, as well as obtaining a protection order . The remedies offered can be both of a civil nature (civil orders of protection and other protective services) and of a criminal nature (charging the perpetrator with a criminal offense). People perpetrating DV are subject to criminal prosecution, most often under assault and battery laws. [60]
In Russia, according to a representative of the Russian Ministry of Internal Affairs one in four families experiences domestic violence. [61] Domestic violence is not a specific criminal offense, but it can be charged under various crimes of the criminal code (e.g. assault), but in practice cases of domestic violence turn into criminal cases only when they involve severe injuries, or the victim has died. [62] For more details see Domestic violence in Russia .
In Turkey 42% of women over 15 have suffered physical or sexual violence. [63]
Fighting the prevalence of domestic violence in Kashmir has brought Hindu and Muslim activists together. [64] According to some Islamic clerics and women's advocates, women from Muslim-majority cultures often face extra pressure to submit to domestic violence, as their husbands may manipulate Islamic law to exert their control. [65]
One study found that half of Palestinian women have been the victims of domestic violence. [66]
A study on Bedouin women in Israel found that most have experienced DV, most accepted it as a decree from God, and most believed they were to blame themselves for the violence. The study also showed that the majority of women were not aware of existing laws and policies which protect them: 60% said they did not know what a restraining order was. [67]
In Iraq husbands have a legal right to "punish" their wives. The criminal code states at Paragraph 41 that there is no crime if an act is committed while exercising a legal right; examples of legal rights include: "The punishment of a wife by her husband, the disciplining by parents and teachers of children under their
authority within certain limits prescribed by law or by custom". [68]
In Jordan , part of article 340 of the Penal Code states that "he who discovers his wife or one of his female relatives committing adultery and kills, wounds, or injures one of them, is exempted from any penalty." [69] This has twice been put forward for cancellation by the government, but was retained by the Lower House of the Parliament, in 2003: a year in which at least seven honor killings took place. [70] Article 98 of the Penal Code is often cited alongside Article 340 in cases of honor killings. "Article 98 stipulates that a reduced sentence is applied to a person who kills another person in a 'fit of fury'". [71]
The Human Rights Watch found that up to 90% of women in Pakistan were subject to some form of maltreatment within their own homes. [72] Honor killings in Pakistan are a very serious problem, especially in northern Pakistan. [73] [74] In Pakistan, honour killings are known locally as karo-kari . Karo-kari is a compound word literally meaning "black male" (Karo) and "black female (Kari). [ citation needed ]
Domestic violence in India is widespread, and is often related to the custom of dowry . [75] Honor killings are more common in some regions of India, particularly in northern regions of the country. Honor killings have been reported in the states of Punjab , Rajasthan, Haryana , Uttar Pradesh, and Bihar , as a result of people marrying without their family's acceptance, and sometimes for marrying outside their caste or religion. [76] [77]
A UN report compiled from a number of different studies conducted in at least 71 countries found domestic violence against women to be most prevalent in Ethiopia . [78]
Up to two-thirds of women in certain communities in Nigeria 's Lagos State say they are victims to domestic violence. [79]
80% of women surveyed in rural Egypt said that beatings were common and often justified, particularly if the woman refused to have sex with her husband. [15]
Statistics published in 2004, show that the rate of domestic violence victimisation for Indigenous women in Australia may be 40 times the rate for non-Indigenous women. [80]
Findings from the 2006 Australian Bureau of Statistics Personal Safety Survey show that among the female victims of physical assault, 31 percent were assaulted by a current or previous partner. Among male victims, 4.4 percent were assaulted by a current or previous partner. Thirty per cent of people who had experienced violence by a current partner since the age of 15 were male, and seventy per cent were female. [81]
The Dutch hypothesis provides one of several biologically plausible explanations for the pathogenesis of chronic obstructive pulmonary disease (COPD) , a progressive disease known to be aetiologically linked to environmental insults such as tobacco smoke . [1]
The Dutch hypothesis was originally proposed by Dick Orie and his team in 1961 at the University of Groningen . [2] [3] According to Orie, "Bronchitis and Asthma  may be found in one patient at the same age but as a rule there is a fluent development from bronchitis in youth to a more asthmatic picture in adults, which in turn develops into bronchitis of elderly patients." [4] This supposition was later named the Dutch hypothesis by a colleague, Professor C. Fletcher. [2] Specifically, clinical characteristics such as allergy and bronchial hyperresponsiveness that are commonly observed in individuals afflicted with asthma were viewed as likely determinants of the life-threatening disease, COPD (in the Netherlands, the term chronic non-specific lung disease was adopted as an umbrella term for asthma and COPD). [3] </ref>
More recent molecular biology research suggests that the pathogenesis of asthma and COPD may share overlapping pathways involving innate biological susceptibility, coupled with environmental factors which can trigger the different diseases. Genetic association studies that have uncovered the same polymorphisms in people with asthma and COPD provide support for the notion that the two conditions share some biological characteristics; implicated genes include ADAM33 , CCL5 and IL17F . [5]
Although clinically debated, [6] [7] the Dutch hypothesis remains one of four main plausible explanations which could help explain the complex pathogenesis of COPD, others being the protease -antiprotease hypothesis (involving alpha 1-antitrypsin overexpression and consequent alpha-1 proteinase deficiency), the British hypothesis (regarding a putative aetiological role of acute bronchial infections), and the autoimmunity hypothesis. [1]
E-epidemiology (also known as Digital Epidemiology) is the science underlying the acquisition, maintenance and application of epidemiological knowledge and information using digital media such as the internet , mobile phones , digital paper , digital TV .  E-epidemiology also refers to the large-scale epidemiological studies that are increasingly conducted through distributed global collaborations enabled by the Internet. [1]
The traditional approach in performing epidemiological trials by using paper questionnaires is both costly and time-consuming. The questionnaires have to be transformed to analyzable data and a large number of personnel are needed throughout the procedure.
Modern communication tools, such as the web, cell phones and other current and future communication devices, allow rapidly and cost-efficient assembly of data on determinants for lifestyle and health for broad segments of the population. Modern IT technology provides means for storage, organization and retrieval of large amounts of biological and lifestyle data, which will ensure more data and more reliable statistical results. Efficient number crunching computing, using modern analytical tools and simulation based inference procedures allow knowledge to be extracted from the resulting large and complex data-structures. 
Web portals directly connected to the studies enables instant feedback and information to the participants. It also allows animations and other web based tools linked to the questionnaires, which can increase the interactivity and facilitates flow of information between the study participant and the study centre. The web portal will also generate a possibility for the Universities to carry out the third assignment, which is to spread the knowledge generated at the University to the public.
Important aspects of e-epidemiology include the development of security and confidentiality preserving solutions to protect individual integrity and research data ownership. [2] [3] But entering an epidemiological trial via the Internet is probably safer then traditional manners. Accurate security programmes and firewalls are a critical condition for handling personal records over the Internet.
The Early Warning and Response System ( EWRS ) for communicable diseases in the European Union was created by the European Commission to "ensure a rapid and effective response by the EU to events (including emergencies) related to communicable diseases." [1]
"EWRS is a web-based system linking the Commission, the public health authorities in Member States responsible for measures to control communicable diseases and the European Centre for Disease Prevention and Control (ECDC). EEA countries ( Iceland , Liechtenstein and Norway ) are also linked to the system." [1]
In 2020, the Department of Health and Social Care requested that the United Kingdom should keep its access to the EWRS after Brexit in order to combat the global COVID-19 outbreak. This was supported by numerous medical experts and organizations, with Niall Dickson , chief executive of the NHS Confederation , stating that EWRS access was essential to maintain the best possible response. [2] The request was denied by Boris Johnson 's government for political reasons, to preserve the government's bargaining position in post-Brexit negotiations. [3] [4]
Ecological studies are used to understand the relationship between outcome and exposure at a population level, where 'population' represents a group of individuals with a shared characteristic such as geography, ethnicity, socio-economic status of employment. [1] What differentiates ecological studies from other studies is that the unit analysis being studied is the group, therefore inferences cannot be made about individual study participants. [2] On the other hand, details of outcome and exposure can be generalized to the population being studied. Examples of such studies include investigating associations between units of grouped data, such as electoral wards, regions, or even whole countries. [3]
Generally, three different designs can be used to conduct ecological studies depending on the situation. Such studies may compare populations or groups using a multiple-group design, periods of time using a time-trend design, or groups and time using a mixed design. [1] [4]
The study by John Snow regarding a cholera outbreak in London is considered the first ecological study to solve a health issue. He used a map of deaths from cholera to determine that the source of the cholera was a pump on Broad Street. He had the pump handle removed in 1854 and people stopped dying there. [5] It was only when Robert Koch discovered bacteria years later that the mechanism of cholera transmission was understood. [6]
Dietary risk factors for cancer have also been studied using both geographical and temporal ecological studies. Multi-country ecological studies of cancer incidence and mortality rates with respect to national diets have shown that some dietary factors such as animal products (meat, milk, fish and eggs), added sweeteners/sugar, and some fats appear to be risk factors for many types of cancer, while cereals/grains and vegetable products as a whole appear to be risk reduction factors for many types of cancer. [7] [8] Temporal changes in Japan in the types of cancer common in Western developed countries have been linked to the nutrition transition to the Western diet. [9]
An important advancement in the understanding of risk-modifying factors for cancer was made by examining maps of cancer mortality rates. The map of colon cancer mortality rates in the United States was used by the brothers Cedric and Frank C. Garland to propose the hypothesis that solar ultraviolet B (UVB) radiation , through vitamin D production, reduced the risk of cancer (the UVB-vitamin D-cancer hypothesis). [10] Since then many ecological studies have been performed relating the reduction of incidence or mortality rates of over 20 types of cancer to higher solar UVB doses. [11]
Links between diet and Alzheimer’s disease have been studied using both geographical and temporal ecological studies. The first paper linking diet to risk of Alzheimer’s disease was a multi-country ecological study published in 1997. [12] It used prevalence of Alzheimer’s disease in 11 countries along with dietary supply factors, finding that total fat and total energy (caloric) supply were strongly correlated with prevalence, while fish and cereals/grains were inversely correlated (i.e., protective). Diet is now considered an important risk-modifying factor for Alzheimer’s disease. [13] Recently it was reported that the rapid rise of Alzheimer’s disease in Japan between 1985 and 2007 was likely due to the nutrition transition from the traditional Japanese diet to the Western diet. [14]
Another example of the use of temporal ecological studies relates to influenza . John Cannell and associates hypothesized that the seasonality of influenza was largely driven by seasonal variations in solar UVB doses and calcidiol levels. [15] A randomized controlled trial involving Japanese school children found that taking 1000 IU per day vitamin D3 reduced the risk of type A influenza by two-thirds. [16]
Ecological studies are particularly useful for generating hypotheses since they can use existing data sets and rapidly test the hypothesis. The advantages of the ecological studies include the large number of people that can be included in the study and the large number of risk-modifying factors that can be examined.
The term “ ecological fallacy ” means that risk-associations apparent between different groups of people may not accurately reflect the true association between individuals within those groups. Ecological studies should include as many known risk-modifying factors for any outcome as possible, adding others if warranted. Then the results should be evaluated by other methods, using, for example, Hill’s criteria for causality in a biological system.
Economic epidemiology is a field at the intersection of epidemiology and economics . Its premise is to incorporate incentives for healthy behavior and their attendant behavioral responses into an epidemiological context to better understand how diseases are transmitted. This framework should help improve policy responses to epidemic diseases by giving policymakers and health-care providers clear tools for thinking about how certain actions can influence the spread of disease transmission .
The main context through which this field emerged was the idea of prevalence-dependence, or disinhibition , which suggests that individuals change their behavior as the prevalence of a disease changes. However, economic epidemiology also encompasses other ideas, including the role of externalities, global disease commons and how individuals’ incentives can influence the outcome and cost of health interventions.
Strategic epidemiology is a branch of economic epidemiology that adopts an explicitly game theoretic approach to analyzing the interplay between individual behavior and population wide disease dynamics.
The spread of an infectious disease is a population-level phenomenon, but decisions to prevent or treat a disease are typically made by individuals who may change their behavior over the course of an epidemic, especially if their perception of risk changes depending on the available information on the epidemics [1] – their decisions will then have population-level consequences. For example, an individual may choose to have unsafe sex or a doctor may prescribe antibiotics to someone without a confirmed bacterial infection . In both cases, the choice may be rational from the individual’s point of view but undesirable from a societal perspective.
Limiting the spread of a disease at the population level requires changing individual behavior, which in turn depends on what information individuals have about the level of risk. When risk is low, people will tend to ignore it. However, if the risk of infection is higher, individuals are more likely to take preventive action. Moreover, the more transmissible the pathogen , the greater the incentive is to make personal investments for control. [2]
The converse is also true: if there is a lowered risk of disease, either through vaccination or because of lowered prevalence, individuals may increase their risk-taking behavior. This effect is analogous to the introduction of safety regulations, such as seatbelts in cars, which because they reduce the cost of an accident in terms of expected injury and death, could lead people to drive with less caution and the resulting injuries to nonoccupants and increased nonfatal crashes may offset some of the gains from the use of seatbelts. [2]
Prevalence-dependent behavior introduces a crucial difference with respect to the way individuals respond when the prevalence of a disease increases. If behavior is exogenous or if behavioral responses are assumed to be inelastic with respect to disease prevalence , the per capita risk of infection in the susceptible population increases as prevalence increases. In contrast, when behavior is endogenous and elastic, hosts can act to reduce their risks. If their responses are strong enough, they can reduce the average per capita risk and offset the increases in the risk of transmission associated with higher prevalence. [3] [4] [5] [6]
Alternatively, the waning of perceived risk, either through the diminution of prevalence or the introduction of a vaccine, may lead to increases in risky behavior. For example, models suggested that the introduction of highly active antiretroviral therapy (HAART) , which significantly reduced the morbidity and mortality associated with HIV /AIDS, may lead to increases in the incidence of HIV as the perceived risk of HIV/AIDS decreased. [7]
Recent analysis suggests that an individual’s likelihood of engaging in unprotected sex is related to their personal analysis of risk, with those who believed that receiving HAART or having an undetectable viral load protects against transmitting HIV or who had reduced concerns about engaging in unsafe sex given the availability of HAART were more likely to engage in unprotected sex regardless of HIV status. [8]
This behavioral response can have important implications for the timing of public interventions, because prevalence and public subsidies may compete to induce protective behavior. [9] In other words, if prevalence induces the same sort of protective behavior as public subsidies, the subsidies become irrelevant because people will choose to protect themselves when prevalence is high, regardless of the subsidy, and subsidies may not be helpful at the times when they are typically applied.
Although STDs are logical targets for examining the role of human behavior in a modeling framework, personal actions are important for other infectious diseases as well. The rapidity with which individuals reduce their contact rate with others during an outbreak of a highly transmissible disease can significantly affect the spread of the disease. [10] Even small reductions in the contact rate can be important, especially for diseases like influenza or severe acute respiratory syndrome (SARS) . However, this may also affect policy planning for a biological attack with a disease such as smallpox .
Individual behavioral responses to interventions for non-sexually transmitted diseases are also important. For example, mass spraying to reduce malaria transmission can reduce the irritating effects of biting by nuisance mosquitoes and so lead to reduced personal use of bednets. [6] Economic epidemiology strives to incorporate these types of behavior responses into epidemiological models to enhance a model’s utility in evaluating control measures.
Immunization represents a classic case of a social dilemma: a conflict of interest between the private gains of individuals and the collective gains of a society, and prevalence-dependent behavior may have significant effects on vaccine policy formation. For instance, it was found in an analysis of the hypothetical introduction of a vaccine that would reduce (though not eliminate) the risk of contracting HIV, that individual levels of risk behavior were a significant barrier to eliminating HIV, as small changes in behavior could actually increase the incidence/prevalence of HIV, even if the vaccine were highly efficacious. [3] These results, as well as others, [11] [12] [13] [14] [15] [16] [17] may have contributed to a decision not to release existing semi-efficacious vaccines. [18]
An individual's self-interest and choice often leads to a vaccination uptake rate less than the social optimum as individuals do not take into account the benefit to others. In addition, prevalence dependent behavior suggests how the introduction of a vaccine may affect the spread of a disease. As the prevalence of a disease increases, people will demand to be vaccinated. As prevalence decreases, however, the incentive, and thus demand, will slacken and allow the susceptible population to increase until the disease can reinvade. As long as a vaccine is not free, either monetarily or through true or even perceived side effects , [19] [20] demand will be insufficient to pay for the vaccine at some point, leaving some people unvaccinated. If the disease is contagious, it could then begin spreading again among non-vaccinated individuals. Thus, it is impossible to eradicate a vaccine-preventable disease through voluntary vaccination if people act in their own self-interest. [21] [22] [23]
The "étude des déterminants pré et postnatals du développement de la santé de l'enfant" (study of pre and postnatal determinants of the child's health and development ), more commonly known as the EDEN cohort study , is a French epidemiological mother-child cohort study conducted by Inserm . [1] The main study aim is to improve the understanding of the role of early life factors that influence the development of children . This study falls in the scientific paradigm of the developmental origins of health and development (DOHaD), also known as the theory of Barker .
The main health outcomes investigated in the cohort include child growth and adiposity , allergies , infections , and cognitive development . Early life factors of interest include maternal diet , nutritional and metabolic status during pregnancy , mental health , other lifestyle factors, environmental factors, social determinants, and genetic and epigenetic markers.
From 2003 to 2006, the study recruited 2002 pregnant women in the maternity unit of Nancy and Poitiers hospitals. After delivery, 1907 infants were enrolled in the cohort and followed up with regular clinical and psychological examinations and questionnaires sent to the parents. The study is still active.
More than 15 research teams contribute to the EDEN study and more than 120 peer-reviewed articles have been published so far.
An emergent virus (or emerging virus ) is a virus that is either newly appeared , notably increasing in incidence / geographic range or has the potential to increase in the near future. [1] Emergent viruses are a leading cause of emerging infectious diseases and raise public health challenges globally, given their potential to cause outbreaks of disease which can lead to epidemics and pandemics . [2] As well as causing disease , emergent viruses can also have severe economic implications. [3] Recent examples include the SARS-related coronaviruses , which have caused the 2002-2004 outbreak of SARS ( SARS-CoV-1 ) and the 2019–20 pandemic of COVID-19 ( SARS-CoV-2 ). [4] [5] Other examples include the human immunodeficiency virus which causes HIV/AIDS ; the viruses responsible for Ebola ; [6] the H5N1 influenza virus responsible for avian flu ; [7] and H1N1/09 , which caused the 2009 swine flu pandemic [8] (an earlier emergent strain of H1N1 caused the 1918 Spanish flu pandemic). [9] Viral emergence in humans is often a consequence of zoonosis , which involves a cross-species jump of a viral disease into humans from other animals. As zoonotic viruses exist in animal reservoirs , they are much more difficult to eradicate and can therefore establish persistent infections in human populations. [10]
Emergent viruses should not be confused with re-emerging viruses or newly detected viruses. A re-emerging virus is generally considered to be a previously appeared virus that is experiencing a resurgence, [1] [11] for example measles . [12] A newly detected virus is a previously unrecognized virus that had been circulating in the species as endemic or epidemic infections. [13] Newly detected viruses may have escaped classification because they left no distinctive clues , and/or could not be isolated or propagated in cell culture . [14] Examples include human rhinovirus (a leading cause of common colds which was first identified in 1956), [15] hepatitis C (eventually identified in 1989), [16] and human metapneumovirus (first described in 2001, but thought to have been circulating since the 19th century). [17] As the detection of such viruses is technology driven, the number reported is likely to expand.
Given the rarity of spontaneous development of new virus species, the most frequent cause of emergent viruses in humans is zoonosis . This phenomenon is estimated to account for 73% of all emerging or re-emerging pathogens , with viruses playing a disproportionately large role. [18] RNA viruses are particularly frequent, accounting for 37% of emerging and re-emerging pathogens. [18] A broad range of animals - including wild birds, rodents and bats - are associated with zoonotic viruses. [19] It is not possible to predict specific zoonotic events that may be associated with a particular animal reservoir at any given time. [20]
Zoonotic spillover can either result in self-limited 'dead-end' infections, in which no further human-human transmission occurs (as with the rabies virus ), [21] or in infectious cases, in which the zoonotic pathogen is able to sustain human-human transmission (as with the Ebola virus ). [6] If the zoonotic virus is able to maintain successful human-human transmission, an outbreak may occur. [22] Some spillover events can also result in the virus adapting exclusively for human infection (as occurred with the HIV virus ), [23] in which case humans become a new reservoir for the pathogen.
A successful zoonotic 'jump' depends on human contact with an animal harbouring a virus variant that is able to infect humans. In order to overcome host-range restrictions and sustain efficient human-human transmission, viruses originating from an animal reservoir will normally undergo mutation , genetic recombination and reassortment . [20] Due to their rapid replication and high mutation rates, RNA viruses are more likely to successfully adapt for invasion of a new host population. [3]
While bats are essential members of many ecosystems, [24] they are also frequently implicated as frequent sources of emerging virus infections. [25] Their immune systems have evolved in such a way as to suppress any inflammatory response to viral infections, thereby allowing them to become tolerant hosts for evolving viruses, and consequently provide major reservoirs of zoonotic viruses. [26] They are associated with more zoonotic viruses per host species than any other mammal, and molecular studies have demonstrated that they are the natural hosts for several high-profile zoonotic viruses, including severe acute respiratory syndrome-related coronaviruses and Ebola / Marburg hemorrhagic fever filoviruses. [27] In terms of their potential for spillover events, bats have taken over the leading role previously assigned to rodents. [26] Viruses can be transmitted from bats via several mechanisms, including bat bite, [28] aerosolization of saliva (e.g. during echolocation ) and faeces/urine. [29]
Due to their distinct ecology /behaviour, bats are naturally more susceptible to viral infection and transmission. Several bat species (e.g. brown bats) aggregate in crowded roosts, which promotes intra- and interspecies viral transmission. Moreover, as bats are widespread in urban areas, humans occasionally encroach on their habitats which are contaminated with guano and urine. Their ability to fly and migration patterns also means that bats are able to spread disease over a large geographic area, while also acquiring new viruses. [30] Additionally, bats experience persistent viral infections which, together with their extreme longevity (some bat species have lifespans of 35 years), helps to maintain viruses and transmit them to other species. Other bat characteristics which contribute to their potency as viral hosts include: their food choices, torpor / hibernation habits and susceptibility to reinfection. [30]
Viral emergence is often a consequence of both nature and human activity . In particular, ecological changes can greatly facilitate the emergence and re-emergence of zoonotic viruses. [31] Factors such as deforestation , reforestation , habitat fragmentation and irrigation can all impact the ways in which humans come into contact with wild animal species, and consequently promote virus emergence. [3] [32] Additionally, climate change can affect ecosystems and vector distribution, which in turn can affect the emergence of vector-borne viruses. Other ecological changes - for example, species introduction and predator loss - can also affect virus emergence and prevalence. Some agricultural practices, for example livestock intensification and inappropriate management/disposal of farm animal faeces, are also associated with an increased risk of zoonosis. [3] [33]
Viruses may also emerge due to the establishment of human populations that are vulnerable to infection. For example, a virus may emerge following loss of cross-protective immunity , which may occur due to loss of a wild virus or termination of vaccination programmes. Well-developed countries also have higher proportions of aging citizens and obesity-related disease , thus meaning that their populations may be more immunosuppressed and therefore at risk of infection. [3] Contrastingly, poorer nations may have immunocompromised populations due to malnutrition or chronic infection; these countries are also unlikely to have stable vaccination programmes. [3] Additionally, changes in human demographics [3] – for example, the birth and/or migration of immunologically naïve individuals – can lead to the development of a susceptible population that enables large-scale virus infection.
Other factors which can promote viral emergence include globalisation ; in particular, international trade and human travel/ migration can result in the introduction of viruses into new areas. [3] Moreover, as densely populated cities promote rapid pathogen transmission, uncontrolled urbanization (i.e. the increased movement and settling of individuals in urban areas ) can promote viral emergence. [34] Animal migration can also lead to the emergence of viruses, as was the case for the West Nile virus which was spread by migrating bird populations. [35] Additionally, human practices regarding food production and consumption can also contribute to the risk of viral emergence. In particular, wet markets (i.e. live animal markets) are an ideal environment for virus transfer, due to the high density of people and wild/farmed animals present. [29] Consumption of bush meat is also associated with pathogen emergence. [29]
Control and prevention of zoonotic diseases depends on appropriate global surveillance at various levels, including identification of novel pathogens, public health surveillance (including serological surveys ), and analysis of the risks of transmission. [36] The complexity of zoonotic events around the world predicates a multidisciplinary approach to prevention. [36] The One Health Model has been proposed as a global strategy to help prevent the emergence of zoonotic diseases in humans, including novel viral diseases. [36] The One Health concept aims to promote the health of animals, humans, and the environment, both locally and globally, by fostering understanding and collaboration between practitioners of different interrelated disciplines, including wildlife biology , veterinary science , medicine , agriculture , ecology , microbiology , epidemiology , and biomedical engineering . [36]
As hosts are immunologically naïve to pathogens they have not encountered before, emergent viruses are often extremely virulent in terms of their capacity to cause disease. Their high virulence is also due to a lack of adaptation to the new host; viruses normally exert strong selection pressure on the immune systems of their natural hosts, which in turn exerts a strong selection pressure on viruses. [37] This coevolution means that the natural host is able to manage infection. However, when the virus jumps to a new host (e.g. humans), the new host is unable to deal with infection due to a lack of coevolution, which results in mismatch between host immunoeffectors and virus immunomodulators .
Additionally, in order to maximise transmission, viruses often naturally undergo attenuation (i.e. virulence is reduced) so that infected animals can survive long enough to infect other animals more efficiently. [38] However, as attenuation takes time to achieve, new host populations will not initially benefit from this phenomenon. Moreover, as zoonotic viruses also naturally exist in animal reservoirs , [10] their survival is not dependent on transmission between new hosts; this means that emergent viruses are even more unlikely to attenuate for the purpose of maximal transmission, and they remain virulent.
Although emergent viruses are frequently highly virulent, they are limited by several host factors including: innate immunity , natural antibodies and receptor specificity . If the host has previously been infected by a pathogen that is similar to the emergent virus, the host may also benefit from cross-protective immunity .
Influenza is a highly contagious respiratory infection, which affects approximately 9% of the global population and causes 300,000 to 500,000 deaths annually. [39] [40] Based on their core proteins, influenza viruses are classified into types A, B, C and D. [41] [42] While both influenza A and B can cause epidemics in humans, influenza A also has pandemic potential and a higher mutation rate, therefore is most significant to public health. [42] [43]
Influenza A viruses are further classified into subtypes, based on the combinations of the surface glycoproteins hemagglutinin (HA) and neuraminidase (NA). The primary natural reservoir for most influenza A subtypes are wild aquatic birds; [42] however, through a series of mutations, a small subset of these viruses have adapted for infection of humans (and other animals). [44] A key determinant of whether a particular influenza A subtype can infect humans is its binding specificity. Avian influenza A preferentially binds to cell surface receptors with a terminal α2,3‐linked sialic acid , while human influenza A preferentially binds to cell surface receptors with a terminal α2,6‐linked sialic acid. Via mutation, some avian influenza A viruses have successfully altered their binding specificity from α2,3‐ to α2,6‐linked sialic acid. [45] However, in order to emerge in humans, avian influenza A viruses must also adapt their RNA polymerases for function in mammalian cells, [46] as well as mutating for stability in the acidic respiratory tract of humans. [47]
Following adaptation and host switch , influenza A viruses have the potential to cause epidemics and pandemics in humans. Minor changes in HA and NA structure ( antigenic drift ) occur frequently, which enables the virus to cause repetitive outbreaks (i.e. seasonal influenza ) by evading immune recognition. [41] Major changes in HA and NA structure ( antigenic shift ), which are caused by genetic reassortment between different influenza A subtypes (e.g. between human and animal subtypes), can instead cause large regional/global pandemics . [41] Due to the emergence of antigenically different influenza A strains in humans, four pandemics occurred in the 20th century alone. [48]
Additionally, although animal influenza A viruses (e.g. swine influenza ) are distinct from human influenza viruses, they can still cause zoonotic infection in humans. These infections are largely acquired following direct contact with infected animals or contaminated environments, but do not result in efficient human-human transmission; examples of this include H5N1 influenza and H7N9 influenza . [42]
In 2002, a highly pathogenic SARS-CoV (Severe Acute Respiratory Syndrome Coronavirus) strain emerged from a zoonotic reservoir; approximately 8000 people were infected worldwide, and mortality rates approached 50% or more in the elderly. [49] As SARS-CoV is most contagious post-symptoms, the introduction of strict public health measures effectively halted the pandemic. [49] The natural reservoir host for SARS-CoV is thought to be horseshoe bats , although the virus has also been identified in several small carnivores (e.g. palm civets and racoon dogs ). The emergence of SARS-CoV is believed to have been facilitated by Chinese wet markets, in which civets positive for the virus acted as intermediate hosts and passed SARS-CoV onto humans (and other species). [49] [50] However, more recent analysis suggests that SARS-CoV may have directly jumped from bats to humans, with subsequent cross-transmission between humans and civets. [49]
In order to infect cells, SARS-CoV uses the spike surface glycoprotein to recognise and bind to host ACE-2 , which it uses as a cellular entry receptor; [49] the development of this characteristic was crucial in enabling SARS-CoV to ‘jump’ from bats to other species.
First reported in 2012, MERS-CoV (Middle East Respiratory Syndrome Coronavirus) marks the second known introduction of a highly pathogenic coronavirus from a zoonotic reservoir into humans. The case mortality rate  of this emergent virus is approximately 35%, with 80% of all cases reported by Saudi Arabia. [51] Although MERS-CoV is likely to have originated in bats, [52] dromedary camels have been implicated as probable intermediate hosts. MERS-CoV is believed to have been circulating in these mammals for over 20 years, [52] and it is thought that novel camel farming practices drove the spillover of MERS-CoV into humans. [53] Studies have shown that humans can be infected with MERS-CoV via direct or indirect contact within infected dromedary camels, while human-human transmission is limited. [51]
MERS-CoV gains cellular entry by using a spike surface protein to bind to the host DPP4 surface receptor; the core subdomain of this spike surface protein shares similarities with that of SARS-CoV, but its receptor binding subdomain (RBSD) significantly differs. [52]
Bluetongue disease is a non-contagious vector-borne disease caused by bluetongue virus, which affects species of ruminants (particularly sheep ). [54] Climate change has been implicated in the emergence and global spread of this disease, due to its impact on vector distribution. The natural vector of the bluetongue virus is the African midge C. imicola , which is normally limited to Africa and subtropical Asia. However, global warming has extended the geographic range of C. imicola , so that it now overlaps with a different vector ( C. pulcaris or C. obsoletus ) with a much more northward geographic range. This change enabled the bluetongue virus to jump vector, thus causing the northward spread of bluetongue disease into Europe. [55]
In epidemiology , an infection is said to be endemic (from Greek ἐν en "in, within" and δῆμος demos "people") in a population when that infection is constantly maintained at a baseline level in a geographic area without external inputs. [1] For example, chickenpox is endemic (steady state) in the United Kingdom, but malaria is not. Every year, there are a few cases of malaria reported in the UK, but these do not lead to sustained transmission in the population due to the lack of a suitable vector (mosquitoes of the genus Anopheles ). While it might be common to say that AIDS is "endemic" in many African countries, meaning found in an area, this is a use of the word in its etymological, rather than epidemiological or ecological, form. AIDS cases in Africa are increasing, so the disease is not in an endemic steady state. The spread of AIDS in Africa could be correctly called an epidemic or even pandemic , however. [2] [3]
For an infection that relies on person-to-person transmission, to be endemic, each person who becomes infected with the disease must pass it on to one other person on average. Assuming a completely susceptible population, that means that the basic reproduction number (R 0 ) of the infection must equal one. In a population with some immune individuals, the basic reproduction number multiplied by the proportion of susceptible individuals in the population ( S ) must be one. This takes account of the probability of each individual to whom the disease may be transmitted being susceptible to it, effectively discounting the immune sector of the population. So, for a disease to be in an endemic steady state it is:
In this way, the infection neither dies out nor does the number of infected people increase exponentially but the infection is said to be in an endemic steady state. An infection that starts as an epidemic will eventually either die out (with the possibility of it resurging in a theoretically predictable cyclical manner) or reach the endemic steady state, depending on a number of factors, including the virulence of the disease and its mode of transmission .
If a disease is in an endemic steady state in a population, the relation above allows us to estimate the R 0 (an important parameter ) of a particular infection. This in turn can be fed into a mathematical model for the epidemic.
An environment-wide association study , also known as an environmental-wide association study (abbreviated EWAS ), is a type of epidemiological study analogous to the genome-wide association study , or GWAS. The EWAS systematically examines the association between a complex disease and multiple individual environmental factors, controlling for multiple hypothesis testing . [1] [2] [3]
This medical article is a stub . You can help Wikipedia by expanding it .
Environmental epidemiology is a branch of epidemiology concerned with determining how environmental exposures impact human health. [2] This field seeks to understand how various external risk factors may predispose to or protect against disease, illness, injury, developmental abnormalities, or death. These factors may be naturally occurring or may be introduced into environments where people live, work, and play.
The World Health Organization European Centre for Environment and Health (WHO-ECEH) claims that 1.4 million deaths per year in Europe alone are due to avoidable environmental exposures. [3] Environmental exposures can be broadly categorized into those that are proximate (e.g., directly leading to a health condition), including chemicals, physical agents, and microbiological pathogens , and those that are distal (e.g., indirectly leading to a health condition), such as socioeconomic conditions, climate change , and other broad-scale environmental changes . Proximate exposures occur through air, food, water, and skin contact. Distal exposures cause adverse health conditions directly by altering proximate exposures, and indirectly through changes in ecosystems and other support systems for human health. [4]
Environmental epidemiology research can inform government policy change, risk management activities, and development of environmental standards. Vulnerability is the summation of all risk and protective factors that ultimately determine whether an individual or subpopulation experiences adverse health outcomes when an exposure to an environmental agent occurs. Sensitivity is an individual’s or subpopulation’s increased responsiveness, primarily for biological reasons, to that exposure. [5] Biological sensitivity may be related to developmental stage, [6] pre-existing medical conditions , acquired factors, and genetic factors . Socioeconomic factors also play a critical role in altering vulnerability and sensitivity to environmentally mediated factors by increasing the likelihood of exposure to harmful agents, interacting with biological factors that mediate risk, and/or leading to differences in the ability to prepare for or cope with exposures or early phases of illness. Populations living in certain regions may be at increased risk due to location and the environmental characteristics of a region.
Acknowledgement that the environment impacts human health can be found as far back as 460 B.C. in Hippocrates ' essay On Airs, Waters, and Places . [7] In it, he urges physicians to contemplate how factors such as drinking water can impact the health of their patients. Another famous example of environment-health interaction is the lead poisoning experienced by the ancient Romans , who used lead in their water pipes and kitchen pottery. [8] Vitruvius , a Roman architect , wrote to discourage the use of lead pipes, citing health concerns:
"Water conducted through earthen pipes is more wholesome than that through lead; indeed that conveyed in lead must be injurious, because from it white lead is obtained, and this is said to be injurious to the human system. Hence, if what is generated from it is pernicious, there can be no doubt that itself cannot be a wholesome body. This may be verified by observing the workers in lead, who are of a pallid colour; for in casting lead, the fumes from it fixing on the different members, and daily burning them, destroy the vigour of the blood; water should therefore on no account be conducted in leaden pipes if we are desirous that it should be wholesome. That the flavour of that conveyed in earthen pipes is better, is shewn at our daily meals, for all those whose tables are furnished with silver vessels, nevertheless use those made of earth, from the purity of the flavour being preserved in them" [9]
Generally considered to be one of the founders of modern epidemiology , John Snow conducted perhaps the first environmental epidemiology study in 1854. He showed that London residents who drank sewage-contaminated water were more likely to develop cholera than those who drank clean water. [10]
Throughout the 20th century, the United States Government passed legislation and regulations to address environmental health concerns. A partial list is below.
1977
The precautionary principle is a concept in the environmental sciences that if an activity is suspected to cause harm, we should not wait until sufficient evidence of that harm is collected to take action. [11] It has its roots in German environmental policy, and was adopted in 1990 by the participants of the North-Sea Conferences in The Hague by declaration. [12] In 2000, the European Union began to formally adopt the precautionary principle into its laws as a Communication from the European Commission . [13] The United States has resisted adoption of this principle, citing concerns that unfounded science could lead to obligations for expensive control measures, especially as related to greenhouse gas emissions. [12]
Environmental epidemiology studies are most frequently observational in nature, [14] meaning researchers look at people's exposures to environmental factors without intervening and then observe the patterns that emerge. This is due to the fact that it is often unethical or unfeasible to conduct an experimental study of environmental factors in humans. [15] For example, a researcher cannot ask some of their study subjects to smoke cigarettes to see if they have poorer health outcomes than subjects who are asked not to smoke. The study types most often employed in environmental epidemiology are: [14]
Epidemiologic studies that assess how an environmental exposure and a health outcome may be connected use a variety of biostatistical approaches to attempt to quantify the relationship. Risk assessment tries to answer questions such as "How does an individual's risk for disease A change when they are exposed to substance B?," and "How many excess cases of disease A can we prevent if exposure to substance B is lowered by X amount?." [16]
Some statistics and approaches used to estimate risk are: [14]
To differentiate between correlation and causation , epidemiologists often apply a set of criteria to determine the likelihood that an observed relationship between an environmental exposure and health consequence is truly causal. [16] In 1965, Austin Bradford Hill devised a set of postulates to help him determine if there was sufficient evidence to conclude that cigarette smoking causes lung cancer. [17]
The Bradford Hill criteria are:
These criteria are generally considered to be a guide to scientists, and it is not necessary that all of the criteria be met for a consensus to be reached. [16]
Epicore is "a crowdsourced disease surveillance project" [1] which "draws on the knowledge of a global community of human, animal, and environmental health professionals to verify information on disease outbreaks in their geographic regions." [2]

This organization-related article is a stub . You can help Wikipedia by expanding it .
EpiData is a group of applications used in combination for creating documented data structures and analysis of quantitative data. The EpiData Association, which created the software, was created in 1999 and is based in Denmark . EpiData was developed in Pascal and uses open standards such as HTML where possible.
EpiData is widely used by organizations and individuals to create and analyze large amounts of data. The World Health Organization (WHO) uses EpiData in its STEPS method of collecting epidemiological , medical, and public health data, for biostatistics , and for other quantitative-based projects.
Epicentre, the research wing of Médecins Sans Frontières ,  uses EpiData to manage data from its international research studies and field epidemiology studies. E.g.: Piola P, Fogg C et al.: Supervised versus unsupervised intake of six-dose artemether-lumefantrine for treatment of acute, uncomplicated Plasmodium falciparum malaria in Mbarara, Uganda: a randomised trial. Lancet. 2005 Apr 23-29;365(9469):1467-73 ' PMID 15850630 '. Other examples: ' PMID 16765397 ', ' PMID 15569777 ' or ' PMID 17160135 '.
EpiData has two parts:
The software is free; development is funded by governmental and non-governmental organizations like WHO.
The Epidemic Intelligence Service ( EIS ) is a program of the U.S. Centers for Disease Control and Prevention (CDC). [3] The modern EIS is a two-year, hands-on post-doctoral training program in epidemiology , with a focus on field work .
Creation of the Epidemic Intelligence Service was proposed by Dr. Alexander Langmuir , chief of epidemiologic services, communicable disease center at the U.S. Public Health Service on March 30, 1951. [4] Dr. Langmuir said that it was of utmost importance to planning of appropriate defense measures against biological warfare germs, development of new detection devices, and train laboratory workers for rapid recognition of biological warfare germs. [4] It arose from biological warfare concerns relating to the Korean War . [5]
The Epidemic Intelligence Service was organized on September 26, 1951, with the purpose of investigating disease outbreaks that are beyond the control of state and local health departments, enforcement of interstate quarantine regulations, and providing epidemic aid at the request of state health agencies. The Epidemic Intelligence Service's first staff members consisted of 21 medical officers from the U.S. Public Health Service. [1]
The EIS is operated by the CDC's Center for Surveillance, Epidemiology, and Laboratory Services (CSELS), in the Office of Public Health Scientific Services (OPHSS). [6]
Persons participating in the program, known colloquially as " disease detectives ", are formally called "EIS officers" (or EIS fellows) by the CDC and have been dispatched to investigate hundreds of possible epidemics created by natural and artificial causes. Since 1951, more than 3,000 EIS officers have been involved in domestic and international response efforts, including the anthrax , hantavirus , West Nile virus in the United States, and the 2014–16 Ebola epidemic in West Africa. [7] [8]
EIS officers begin their fellowship with a one-month training program at CDC headquarters in Atlanta, Georgia; however, 95% of their two-year term consists of experiential rather than classroom training. [9] For the remainder of their service, EIS officers are assigned to operational branches within the CDC or at state and local health departments around the country. Placement is determined via a highly competitive matching process. [9] The CDC pairs EIS officers with a Public Health Advisor or "PHA", forming a scientist (EIS officer) and operations (PHA) team. [10] EIS is a common recruiting pathway into the Public Health Service Commissioned Corps . [11]
The EIS is the prototype for Field Epidemiology Training Programs (FETP), which operate in numerous countries with technical assistance provided by the CDC. [12]
Since the inception of the EIS, officers have been involved with treatment, eradication, and disease-control efforts for a variety of medically related crises. [13] Below is an abridged timeline of their work.
EIS officers attend an annual conference in Atlanta, Georgia, to present components of their work from the preceding year. [15]
During the conference, the Alexander D. Langmuir Prize is awarded "to a current officer or first-year alumnus of the EIS for the best scientific publication. The award consists of a $100 cash prize, an engraved paperweight, a case of ale or beer redolent of the John Snow Pub in London, and an inscription on the permanent plaque at CDC." [16]
A complete list of Langmuir Prize winners is included below: [17]
In the 2011 film Contagion , the character Doctor Erin Mears (portrayed by Kate Winslet ) is a physician and investigator with the Epidemic Intelligence Service who was tasked by the CDC to discover the origin of a highly contagious and deadly virus known as MEV-1 which was rapidly spreading throughout the world following initial outbreaks in Kowloon , Hong Kong and Minneapolis , Minnesota .
Classic epidemic models of disease transmission are described in Compartmental models in epidemiology .  Here we discuss the behavior when such models are simulated on a lattice.
The mathematical modelling of epidemics was originally implemented in terms of differential equations, which effectively assumed that the various states of individuals were uniformly distributed throughout space.  To take into account correlations and clustering, lattice-based models have been introduced.  Grassberger [1] considered synchronous (cellular automaton) versions of models, and showed how the epidemic growth goes through a critical behavior such that transmission remains local when infection rates are below critical values, and spread throughout the system when they are above a critical value. Cardy and Grassberger [2] argued that this growth is similar to the growth of percolation clusters, which are governed by the "dynamical percolation" universality class (finished clusters are in the same class as static percolation, while growing clusters have additional dynamic exponents).  In asynchronous models, the individuals are considered one at a time, as in kinetic Monte Carlo or as a "Stochastic Lattice Gas."
In the "SIR" model, there are three states:
It is to be distinguished from the "SIS" model, where sites recover without immunization, and are thus not "removed".
The asynchronous simulation of the model on a lattice is carried out as follows:
Making a list of I sites makes this run quickly.
The net rate of infecting one neighbor over the rate of removal is  λ = (1-c)/c.
For the synchronous model, all sites are updated simultaneously (using two copies of the lattice) as in a cellular automaton.
I → S with unit rate;
S → I with rate λn I /z where n I is the number of nearest neighbor I sites, and z is the total number of nearest neighbors  (equivalently, each I attempts to infect one neighboring site with rate λ)
(Note: S → I with rate λn in some definitions, implying that lambda has one-fourth the values given here).
The simulation of the asynchronous model on a lattice is carried out as follows, with c = 1 / (1 + λ):
Note that the synchronous version is the same as the directed percolation model.
The science of epidemiology has matured significantly from the times of Hippocrates , Semmelweis and John Snow . The techniques for gathering and analyzing epidemiological data vary depending on the type of disease being monitored but each study will have overarching similarities. [1]
Epidemiologists are famous for their use of rates. Each measure serves to characterize the disease giving valuable information about contagiousness, incubation period, duration, and mortality of the disease.
Epidemiological (and other observational) studies typically highlight associations between exposures and outcomes, rather than causation. While some consider this a limitation of observational research, epidemiological models of causation (e.g. Bradford Hill criteria) [7] contend that an entire body of evidence is needed before determining if an association is truly causal. [8] Moreover, many research questions are impossible to study in experimental settings, due to concerns around ethics and study validity. For example, the link between cigarette smoke and lung cancer was uncovered largely through observational research; however research ethics would certainly prohibit conducting a randomized trial of cigarette smoking once it had already been identified as a potential health threat.
In demography and medical geography , epidemiological transition is a theory which "describes changing population patterns in terms of fertility , life expectancy , mortality, and leading causes of death." [1] For example, a phase of development marked by a sudden increase in population growth rates brought by improved food security and innovations in public health and medicine, can be followed by a re-leveling of population growth due to subsequent declines in fertility rates . Such a transition can account for the replacement of infectious diseases by chronic diseases over time due to increased life span as a result of improved health care and disease prevention . [2] [3] This theory was originally posited by Abdel Omran in 1971. [4] [5]
Omran divided the epidemiological transition of mortality into three phases, in the last of which chronic diseases replace infection as the primary cause of death. [6] These phases are:
In 1998 Barrett et al. [7] proposed two additional phases in which cardiovascular diseases diminish as a cause of mortality due to changes in culture, lifestyle and diet, and diseases associated with aging increase in prevalence. In the final phase, disease is largely controlled for those with access to education and health care, but inequalities persist.
The epidemiological transition occurs when a country undergoes the process of transitioning from developing nation to developed nation status. The developments of modern healthcare and medicine, such as antibiotics , drastically reduce infant mortality rates and extend average life expectancy which, coupled with subsequent declines in fertility rates, reflects a transition to chronic and degenerative diseases as more important causes of death.
The theory of epidemiological transition uses patterns of health and disease as well as their forms of demographic, economical and sociological determinants and outcomes. [4]
In general human history, Omran's first phase occurs when human population sustains cyclic, low-growth, and mostly linear, up-and-down patterns associated with wars, famine, epidemic outbreaks, as well as small golden ages , and localized periods of "prosperity". In early pre-agricultural history, infant mortality rates were high and average life expectancy low. Today, life expectancy in developing countries remains relatively low, as in many Sub-Saharan African nations where it typically doesn't exceed 60 years of age. [8]
The second phase involves improved nutrition as a result of stable food production along with advances in medicine and the development of health care systems . Mortality in Western Europe and North America was halved during the 19th century due to closed sewage systems and clean water provided by public utilities, with a particular benefit for children of both sexes and to females in the adolescent and reproductive age periods, probably because the susceptibility of these groups to infectious and deficiency diseases is relatively high. [9] [10] An overall reduction in malnutrition enabled populations to better resist infectious disease. Treatment breakthroughs of importance included the initiation of vaccination during the early nineteenth century, and the discovery of penicillin in the mid 20th century, which led respectively to a widespread and dramatic decline in death rates from previously serious diseases such as smallpox and sepsis . Population growth rates surged in the 1950s, 1960's and 1970's to 1.8% per year and higher, with the world gaining 2 billion people between 1950 and the 1980s. [ citation needed ] A decline in mortality without a corresponding decline in fertility leads to a population pyramid assuming the shape of a bullet or a barrel, as young and middle-age groups comprise equivalent percentages of the population.
Omran's third phase occurs when human birth rates drastically decline from highly positive replacement rates to stable replacement numbers. In several European nations replacement rates have even become negative. [11] This transition generally represents the net effect of individual choices on family size and the ability to implement those choices. Omran gives three possible factors tending to encourage reduced fertility rates: [4]
Improvements in female and childhood survival that occur with the shift in health and disease patterns discussed above have distinct and seemingly contradictory effects on fertility. While better health and greater longevity enjoyed by females of reproductive age tend to enhance fertility, the reduced risks to infants and young children that occurs in the later stages of the transition tends to have the opposite effect: prolonged breastfeeding associated with reduced mortality among infants and toddlers, together with parental recognition of improved childhood survival, tend to lengthen birth intervals and depress overall reproductive rates. [4]
The transition may also be associated with demographic movements to urban areas , and a shift from agriculture and labor-based production output to technological and service-sector -based economies. This shift in demographic and disease profiles is currently under way in most developing nations, however every country is unique and transition speed is based on numerous geographical and sociopolitical factors. Whether the transition is due to socioeconomic improvements (as in developed countries) or by modern public health programs (as has been the case in many developing countries), the lowering of mortality and of infectious disease tends to increase economic productivity through better functioning of adult members of the labor force and through an increase in the proportion of children who survive and mature into productive members of society. [4]
Omran developed three models to explain the epidemiological transition. [4]
McMichael, Preston, and Murray offer a more nuanced view of the epidemiological transition, highlighting macro trends and emphasizing that there is a change from infectious to non-communicable disease , but arguing that it happens differently in different contexts.
One of the first to refine the idea of the epidemiological transition was Preston, who in 1976 proposed the first comprehensive statistical model relating mortality and cause-specific mortality. Preston used life tables from 43 national populations, including both developed countries such as United States and England and developing countries such as Chile, Colombia, Costa Rica, Guatemala, México, Panama, Taiwan, Trinidad and Tobago, and Venezuela. He used multiple linear regression to analyze the cause-specific-age-standardized death rates by sex. The estimated slopes represented the proportional contribution of each cause to a unit change in the total mortality rate. With the exception of neoplasms in both sexes and cardiovascular disease in males, all of the estimated slopes were positive and statistically significant . This demonstrated that the mortality rates from each specific cause were expected to decline as total mortality declined. The major causes accounting for the decline were all infectious and parasitic diseases . [12]
McMichael et al. argue (2004) that the epidemiological transition has not taken place homogeneously in all countries. Countries have varied in the speed with which they go through the transition as well as what stage of the transition they are in. The global burden of disease website provides visual comparisons of the disease burdens of countries and the changes over time. [ citation needed ] The epidemiological transition correlates with changes in life expectancy. Worldwide, mortality rates have decreased as both technological and medical advancements have led to a tremendous decrease in infectious diseases. With fewer people dying from infectious diseases, there is a rising prevalence of chronic and/or degenerative diseases in the older surviving population.
McMichael et al. describe life expectancy trends as grouped into three categories, as suggested by Casselli et al.:
Murray and Lopez (1996) offered one of the most important cause-of-death models as part of the 1990 Global Burden of Disease Study . Their "cause of death" patterns sought to describe the fraction of deaths attributed to a set of mutually exclusive and collectively exhaustive causes. They divided diseases into three cause groups and made several important observations:
The regression approach underlying the Global Burden of Disease received some critique in light of real-world violations of the model's "mutually exclusive and collectively exhaustive" cause attribution. [14]
Building on the existing body of evidence, Salomon and Murray (2002), further add nuances to the traditional theory of epidemiological transition by disintegrating it based on disease categories and different age-sex groups, positing that the epidemiological transition entails a real transition in the cause composition of age-specific mortality, as opposed to just a transition in the age structure. Using Global Burden of Disease data from 1990, they disintegrate the transition across three cause groups: communicable diseases, non-communicable diseases and injuries, seeking to explain the variation in all-cause mortality as a function of cause-specific mortality in 58 countries from 1950 to 1998. This analysis validates the underlying premise of the classic epidemiological transition theory: as total mortality declines and income rises, communicable diseases cause less and less mortality compared to non-communicable diseases and injuries. Decomposing this overall impact by age-sex groups, they find that for males, when overall mortality decreases, the importance of non-communicable diseases (NCDs) increases relative to the other causes with an age-specific impact on the role of injuries, whereas for women, both NCDs and injuries gain a more significant share with mortality decreases. For children over one year, they find that there is a gradual transition from communicable to non-communicable diseases, with injuries remaining significant in males. For young adults, the epidemiological transition is particularly different: for males, there is a shift from injuries to NCDs in lower income settings, and the opposite in higher-income settings; for females, rising income also signifies a shift from NCDs to injuries, but the role of injuries becomes more significant over time compared to males. Finally, for both males and females over 50, there is no epidemiological transition impact on the cause composition of mortality. [14]
The majority of the literature on the epidemiological transition that was published since these seminal papers confirms the context-specific nature of the epidemiological transition: while there is an overall all-cause mortality decline, the nature of cause-specific mortality declines differs across contexts. Increasing obesity rates in high-income countries are further confirming the epidemiological transition theory as the epidemic leads to an increase in NCDs. The picture is more nuanced in low- and middle-income countries, where there are signs of a protracted transition with the double burden of communicable and noncommunicable disease. A recent review of cause-specific mortality rates from 12 low- and middle-income countries in Asia and sub-Saharan Africa by Santosa and Byass (2016) shows that broadly, low- and middle-income countries are rapidly transitioning to lower total mortality and lower infectious disease mortality. [14] A more macro-level analysis from the Global Burden of Disease data conducted by Murray and others (2015) finds that while there is a global trend towards decreasing mortality and increasing NCD prevalence, this global trend is being driven by country-specific effects as opposed to a broader transition; further, there are varying patterns within and between countries, which makes it difficult to have a single unified theory of epidemiological transition. [15]
A theory of epidemiological transition aimed at explaining not just describing changes in population disease and mortality profiles would need to encompass the role in different morbid conditions of infectious diseases contracted over the life course. The concept of linear transition from infectious diseases to other conditions referred to as degenerative or non-communicable, was based on a false dichotomy as common microorganisms have now been confirmed as causal agents in several conditions recorded as the underlying cause of many deaths. A revised transition model might focus more on disease aetiology and the determinants of cause-specific mortality change, while encompassing the possibility that infectious causation may be established for other morbid conditions through the vast amount of ongoing research into associations with infectious diseases. [16] [17]

Epidemiological studies of the health effects of low levels of ionizing radiation , in particular the incidence and mortality from various forms of cancer , have been carried out in different population groups exposed to such radiation. These have included survivors of the atomic bombings of Hiroshima and Nagasaki in 1945, workers at nuclear reactors , and medical patients treated with X-rays .
Survivors of the atomic bomb explosions at Hiroshima and Nagasaki, Japan have been the subjects of a Life Span Study (LSS), which has provided valuable epidemiological data.
The LSS population went through several changes:
However, some 44,000 individuals were censured or excluded from the LSS project, so there remained about 86,000 people who were followed through the study.  There is a gap in knowledge of the earliest cancer that developed in the first few years after the war, which impacts the assessment of leukemia to an important extent and for solid cancers to a minor extent.  Table 1 shows summary statistics of the number of persons and deaths for different dose groups. These comparisons show that the doses that were received by the LSS population overlap strongly with the doses that are of concern to NASA Exploration mission (i.e., 50 to 2,000 milliSieverts (mSv)).
Figure 1 shows the dose response for the excess relative risk (ERR) for all solid cancers from Preston et al. [1] Tables 2 and 3 show several summary parameters for tissue-specific cancer mortality risks for females and males, respectively, including estimates of ERR, excess absolute risk (EAR), and percentage attributable risks .  Cancer incidence risks from low- LET radiation are about 60% higher than cancer mortality risks. [3]
The BEIR VII Report [2] contains an extensive review of data sets from human populations, including nuclear reactor workers and patients who were treated with radiation.  The recent report from Cardis et al. [4] describes a meta-analysis for reactor workers from several countries.  A meta-analysis at specific cancer sites, including breast , lung , and leukemia, has also been performed. [2] These studies require adjustments for photon energy , dose -rate, and country of origin as well as adjustments made in single population studies.  Table 4 shows the results that are derived from Preston et al. [5] for a meta-analysis of breast cancer risks in eight populations, including the atomic-bomb survivors.  The median ERR varies by slightly more than a factor of two, but confidence levels significantly overlap.  Adjustments for photon energy or dose-rate and fractionation have not been made.  These types of analysis lend confidence to risk assessments as well as showing the limitations of such data sets.
Of special interest to NASA is the dependence on age at exposure of low-LET cancer risk projections. The BEIR VII report prefers models that show less than a 25% reduction in risk over the range from 35 to 55 years, while NCRP Report No. 132 [6] shows about a two-fold reduction over this range.
This article incorporates public domain material from the National Aeronautics and Space Administration document: "Human Health and Performance Risks of Space Exploration Missions" (PDF) . (NASA SP-2009-3405, pp. 132-134)
Epidemiology in Country Practice is a book by William Pickles (1885–1969), a rural general practitioner (GP) physician in Wensleydale , North Yorkshire , England, first published in 1939. The book reports on how careful observations can lead to correlations between transmission of infective disease between families, farms and villages.
It contains the detailed observational studies of a 1928 epidemic of catarrhal jaundice and a 1929 epidemic of Bornholm disease which were published in the British Medical Journal (BMJ) in 1930 and 1933 respectively.
William Pickles first realised the possibilities for epidemiological studies for a GP after he read James Mackenzie 's The Principles of Diagnosis and Treatment in Heart Affections in the 1920s. [1] [2]
With the assistance of his wife Gertie, who kept the charts, Pickles recorded his observations on a 1928 epidemic of catarrhal jaundice and a 1929 epidemic of Bornholm Disease in his district. His findings were published in the British Medical Journal in 1930 and 1933 respectively [2] and in 1935 he presented them at the Royal Society of Medicine . The work was praised by Major Greenwood who wrote that Pickles's work would mark a "new era in epidemiology ". [3] His observations led to new understandings of the transmission of infective disease within families, farms and villages. [4]
Epidemiology in Country Practice contains Pickles's observational studies and a number of articles previously published in medical journals, including the detailed observational studies of the 1928 epidemic of jaundice and the 1929 epidemic of Bornholm disease. [2] The book has been described as more of an essay on epidemiology than a book filled with masses of data. [5]
Most of the research was done between 1929 and 1939. [6] From 1937, in order to work on the book, Pickles kept evening surgeries "to a minimum and often there were no patients at all". [7] He methodically reported patterns of prevalent diseases in his area; [4] however, his data collection and publications lacked the consent processes now considered necessary to avoid identification of individuals afflicted by epidemics, particularly in small communities where recognition of persons is deemed easier. [8] Pickles was well acquainted with the eight villages he looked after, [9] and once, as he looked down upon Wensleydale from the top of a hill, realised that he knew everyone in the village and most on first-name basis. [7]
The book begins with a personal appeal by Pickles for GPs to consider the importance of observations, followed by eight chapters that cover cases such as " influenza , measles , scarlet fever , whooping-cough , and mumps ", as a well as jaundice and myalgia. [10] One story is that of a "gypsy" who accompanied her sick husband into the village in a caravan. Her husband was suffering from typhoid and Pickles was able to trace the source of the disease to a faulty water pump that the wife used to wash her laundry. In the book, he compares the case to the work of one of his heroes, William Budd , who carried out similar observations. [4]
The book also describes an epidemic of catarrhal jaundice that resulted in 250 cases out of a population of almost 6,000, many of which were children. The exact incubation period was not known and ranged from 3 to 40 days. After two years of keeping records and with assistance from the Ministry of Health, Pickles was able to show that the incubation period was 26–35 days. He cross-referenced the evidence with smaller studies in neighbouring villages and in one case was able to trace 13 infections to a single country maid who was determined to attend a fete despite Pickles attending to her in her sickbed the same morning. [3] [7] [11] One of the cases was another young woman and her male friend who, according to his (the man's) sister, often went "in the back door in the evenings, and helps her wash up", causing Pickles to observe that "studies in epidemiology sometimes reveal romances." [11]
The book was first published by John Wright & Sons of Bristol in 1939. It had a preface by Major Greenwood, professor of epidemiology and vital statistics at the University of London. In April 1941, during the Second World War, the entire stock of the book, unbound sheets and the type were completely destroyed by enemy action but such was the demand for the book that in 1949 it was reissued in virtually identical form. [2] [5]
In 1970, a limited edition was published with profits going to the Royal College of General Practitioners (RCGP) appeal. [12] The book was subsequently reprinted by Devonshire Press of Torquay in 1972 and in later editions by the RCGP (1984: ISBN 0-85084-097-X ). [5]
The book was described by John Horder in 1969 as a "classic", that "makes it all sound too easy and one wonders why no one had thought of it all before". [6] Pickles's obituary in the British Medical Journal in 1969 declared that it had "received excellent notices" [2] and in 2004, J.A. Reid portrayed it as "a seminal  book  that  has  been  read and assessed during past decades by  many  public  health  students and  practitioners". [13] Later, RCGP president Denis Pereira Gray described it as "a masterpiece recognised throughout the world" [12] and that practice-based research should be modelled on Pickles's thorough and accurate recording. [7]
The book facilitated the link between research and primary care, resulting in the modern expansion of surveillance practices for improvements in public health . [4] In addition, it revealed that general practitioners could carry out "world class research" in the community. [14]
Epidemiology in Relation to Air Travel is a book by Arthur Massey , the medical officer of health of Coventry , published by H. K. Lewis and Company in 1933. By comparing the travel times of journeys by ship to those of travelling by air, he demonstrated how the quarantinable diseases plague , cholera , yellow fever and smallpox , could arrive in the UK in the early 1930s.
Massey noted that travelling by aeroplane, from countries where major infectious diseases were common, to countries where those diseases were rare or non-existent, risked spreading those diseases during the incubation period.
It was noted by Air Commodore H. E. Whittingham and in the The Indian Medical Gazette to be one of the earliest works of its kind.
Epidemiology in Relation to Air Travel was written by the Coventry -based medical officer of health Arthur Massey , and published by H. K. Lewis and Co. Limited in 1933, when the topic was relatively new, [1] and in the year after the International Sanitary Convention for Aerial Navigation was drawn up. [2] It has 59 pages and five maps. [1]
The book deals briefly with the danger of spreading infectious disease via aircraft as flight times in the 1930s brought West Africa and India within a few days' travel of England and Europe, and the United States more speedily reached from Central and South America. [3] [2] Massey noted that travelling by aeroplane, from countries where major infectious diseases were common, to countries where those diseases were rare or non-existent, risked spreading those diseases during the incubation period . [2] It was aimed at informing health authorities and offered solutions for prevention. [3] By comparing the travel times of travelling by ship to those of travelling by air, he demonstrated how particularly four quarantinable diseases ( plague , cholera , yellow fever and smallpox ), could arrive in the UK in the early 1930s. [2] He made particular note of mosquitoes and the risk of transferring yellow fever. [1] In the preface, he wrote:
Speedier transport is equivalent to a reduction of distance. This was shown when steamships superseded sailing vessels. It is demonstrated more forcibly today by the events of civil aviation. Among the momentous advantages, fraternal and commercial, born of this development, there is the disadvantage that countries affected by certain major infectious diseases are brought nearer to countries which ordinarily enjoy freedom therefrom. [2]
The book addresses disinfection and sanitation in aircraft, and the prevention of aircraft transmitting plague, cholera, malaria , relapsing fever and smallpox. [1] How to dispose of excrement and implement procedures to avoid carrying disease bearing insects are included in the book. [4]
It was noted by Air Commodore H. E. Whittingham, to be one of the earliest works of its kind, along with that of Air Commodore David Munro , who wrote on the subject in 1925. [5] The Indian Medical Gazette also noted it to be a new field, but disagreed with some of the maps showing plague and cholera distribution in Asia and noted some minor errors in the text. [3]

Measles is extremely contagious, but surviving the infection results in lifelong immunity, [1] so its continued circulation in a community depends on the generation of susceptible hosts by birth of children. In communities which generate insufficient new hosts the disease will die out. This concept was first recognized by Bartlett in 1957, who referred to the minimum number supporting measles as the critical community size (CCS). [2] Analysis of outbreaks in island communities suggested that the CCS for measles is c. 250,000. [3]
In 2011, the WHO estimated that there were about 158,000 deaths caused by measles. This is down from 630,000 deaths in 1990. [5] In developed countries, death occurs in 1 to 2 cases out of every 1,000 (0.1% - 0.2%). [6] Death from measles was reported in approximately 0.2% of the cases in the United States from 1985 through 1992. [7] In populations with high levels of malnutrition and a lack of adequate healthcare, mortality can be as high as 10%. [8] Increased immunization has led to an estimated 78% drop in measles deaths among UN member states . [9] [10]
Even in countries where vaccination has been introduced, vaccination rates may remain low due to parents choosing not to have their children vaccinated. In Ireland , vaccination was introduced in 1985. There were 99,903 cases that year. Within two years, the number of cases had fallen to 201, but this fall was not sustained. Measles is a leading cause of vaccine-preventable childhood mortality. Worldwide, the fatality rate has been significantly reduced by a vaccination campaign led by partners in the Measles Initiative : the American Red Cross , the United States Centers for Disease Control and Prevention (CDC), the United Nations Foundation, UNICEF and the WHO. Globally, measles fell 60% from an estimated 873,000 deaths in 1999 to 345,000 in 2005. [11] Estimates for 2008 indicate deaths fell further to 164,000 globally, with 77% of the remaining measles deaths in 2008 occurring within the Southeast Asian region. [12]
In 2006–07 there were 12,132 cases in 32 European countries: 85% occurred in five countries: Germany, Italy, Romania, Switzerland and the UK. 80% occurred in children and there were 7 deaths. [13]
Five out of six WHO regions have set goals to eliminate measles, and at the 63rd World Health Assembly in May 2010, delegates agreed a global target of a 95% reduction in measles mortality by 2015 from the level seen in 2000, as well as to move towards eventual eradication . However, no specific global target date for eradication has yet been agreed to as of May 2010. [14] [15]
On January 22, 2014, the World Health Organization and the Pan American Health Organization declared and certified Colombia free of the measles while becoming the first Latin American country to abolish the infection within its borders. [16] [17]
Cases reported in the first three months of 2019 were 300% higher than in the first three months of 2018, with outbreaks in every region of the world, even in countries with high overall vaccination coverage where it spread among clusters of unvaccinated people. [18]
In April 2020, the WHO indicated that many countries had started suspending their measles vaccination programs due to the impact of the COVID-19 pandemic . It is projected that 117 million children will be at risk of infection. [19]
Commonly outbreaks in one country spread to others and this can be traced by close examination of the virus DNA. As of 2020 measles is widespread and there have been over the last decade many outbreaks in areas that were formerly declared measles free. See below for individual countries by continent.
Some examples of measles spreading between countries are:
Some experts stated that the persistence of the disease in Europe could be a stumbling block to global eradication.  It has proven difficult to vaccinate a sufficient number of children in Europe to eradicate the disease, because of opposition on philosophical or religious grounds, or fears of side-effects, or because some minority groups are hard to reach, or simply because parents forget to have their children vaccinated.  Vaccination is not mandatory in some countries in Europe, in contrast to the United States and many Latin American countries, where children must be vaccinated before they enter school. [27]
In January 2020, the World Health Organization announced that the death toll from the measles outbreak in the Democratic Republic of the Congo had reached 6,000, triple that of Ebola . 310,000 cases have been reported since 2019. US $27.6 million has been spent, but $40 million more is needed. [28]
In 2019, 1,200 had died of measles in an outbreak in Madagascar . [29]
Beginning in September 2009, Johannesburg , South Africa reported about 48 cases of measles. Soon after the outbreak, the government ordered all children to be vaccinated. Vaccination programs were then initiated in all schools, and parents of young children were advised to have them vaccinated. [30] Many people were not willing to have the vaccination done, as it was believed to be unsafe and ineffective. The Health Department assured the public that their program was indeed safe. Speculation arose as to whether or not new needles were being used. [31] By mid-October, there were at least 940 recorded cases, and four deaths. [32]
Indigenous measles was declared to have been eliminated in North, Central, and South America; the last endemic case in the region was reported on November 12, 2002, with only northern Argentina and rural Canada , particularly in Ontario, Quebec, and Alberta, having minor endemic status. [33] Outbreaks are still occurring, however, following importations of measles viruses from other world regions . In June 2006, an outbreak in Boston resulted after a resident became infected in India . [34]
The Canadian government defines endemic measles as a situation where a chain of transmission continues uninterrupted for 12 months. [35] By this standard, Canada has been free of endemic measles since 1998, but sporadic imported outbreaks have continued. [35]
Southern regions of the province of Quebec witnessed a measles outbreak affecting 94 persons in the Spring and summer 2007. The outbreak lasted 25 weeks, included more than one strain of the measles virus and had 12-17 generations of spread.
In 2008, Canada had more than 30 confirmed cases in Ontario in 2008, with more than half reported in Toronto. [36]
In 2011, Quebec experienced the largest outbreak of measles in the Americas since 2002. [37] The outbreak began on 8 January with unvaccinated individuals acquiring the disease whilst traveling to France, a country with high measles incidence, and returning home to Quebec. [38] Public health officials responded to the outbreak by launching a mass vaccination campaign, [39] and on 22 December, the outbreak ended with a total of 776 cases having occurred. 615 cases (79%) had not been vaccinated, including 29 infants too young to receive the vaccine. 11% of cases required hospitalization, and complications occurred in 64 cases (8%), with pneumonia being the most common complication observed (3% of cases). No deaths were reported. [37]
A measles outbreak was declared on 8 March 2014 in regions east of Vancouver in the Fraser Valley area where vaccination rates were known to be low in school and religious groups.  A total of 228 cases were confirmed.  On March 24, Fraser Health Authority said the outbreak have been contained and confined to the original community. [40]
Twenty-five cases of measles were reported in Mexico City on March 18, 2020. The outbreak began in the Reclusario Norte (Northern penitentiary) the previous week. 8,000 vaccines were applied at the penitentiary and 10,000 doses were applied at the other penitentiaries in the city. Eleven children and five adults in the nearby Gustavo A. Madero borough were among the 25 infected. [41] [42]
Indigenous measles has been declared eliminated in North, Central, and South America; the last endemic case in the region was reported on November 12, 2002. [43] Though measles is considered “eliminated,” outbreaks are still occurring following importations of measles viruses from other world regions . In June 2006, an outbreak in Boston resulted after a resident became infected in India , [44] and in October 2007, a Michigan girl who had been vaccinated contracted the disease overseas. [45]
In 1991 in the Philadelphia region, thousands of children were sick with measles. The center of this outbreak was traced to the Faith Tabernacle Congregation, a Faith Healing church that actively discouraged parishioners from vaccinating their children. A judge issued a Court Order to forcibly treat children whose parents refused to seek medical care, and nine children were forcibly vaccinated. Nine children eventually died as a result of this outbreak. [46]
Between January 1 and April 25, 2008, a total of 64 confirmed measles cases were preliminarily reported in the United States to the Centers for Disease Control and Prevention , [48] [49] the most reported by this date since 2001, and the highest total number in six years. [50] Of the 64 cases, 54 were associated with importation of measles from other countries into the United States, and 63 of the 64 patients were unvaccinated or had unknown or undocumented vaccination status. [51] By July 9, 2008, a total of 127 cases were reported in 15 states, making it the largest US outbreak since 1997 (when 138 cases were reported). [52] Most of the cases were acquired outside of the United States and afflicted individuals who had not been vaccinated.
In early 2008 there was an outbreak of measles in San Diego, California . The outbreak is traced to an unvaccinated 7-year-old child who went on a family trip to Europe. [53] [54] The CDC refers to this as an "import-associated outbreak". [53] The final diagnosis included 11 additional cases of measles in unvaccinated children in San Diego. [53] All of the confirmed patients were not vaccinated because they were younger than 1, the minimum age for measles inoculation, or because their parents declined to have them vaccinated. [55] The typical vaccine would be the MMR vaccine . The incident drew attention to the controversy over MMR vaccination .  This was San Diego County 's first measles outbreak since 1991. [55]
In February 2008 there was an outbreak of measles in Pima County, Arizona .  There were 13 laboratory confirmed and 4 probable measles cases, though 22 cases were previously reported. [56] The outbreak started with a visitor from Switzerland and resulted in a public health emergency declaration by Pima County. [56] The last confirmed Pima County case occurred in 1994, and the last outbreak occurred in 1991. [57]
By July 9, 2008, a total of 127 cases were reported in 15 states (including 22 in Arizona ), [58] making it the largest U.S. outbreak since 1997 (when 138 cases were reported). [52] Most of the cases were acquired outside of the United States and afflicted individuals who had not been vaccinated. By July 30, 2008, the number of cases had grown to 131. Of these, about half involved children whose parents rejected vaccination. The 131 cases occurred in seven different outbreaks. There were no deaths, and 15 hospitalizations. Eleven of the cases had received at least one dose of measles vaccine .  Children who were unvaccinated or whose vaccination status was unknown accounted for 122 cases. Some of these were under the age when vaccination is recommended, but in 63 cases, the vaccinations had been refused for religious or philosophical reasons.
On May 24, 2011, the Centers for Disease Control and Prevention reported that the United States has had 118 measles cases so far this year. The 118 cases were reported by 23 states and New York City between Jan 1 and May 20. Of the 118 cases, 105 (89%) were associated with cases abroad and 105 (89%) of the 118 patients had not been vaccinated. [59]
In 2013, at least 20 members of the Eagle Mountain International Church in North Texas were diagnosed with measles after a few members of the congregation traveled abroad on a mission trip and contracted the disease. The church is part of Kenneth Copeland Ministries, which used to advocate abstaining from vaccinations and immunizations for fear they cause autism. [60] The church has sponsored several vaccination drives. [61] Senior Pastor Terri Pearsons, who had previously expressed concerns about potential links between the measles vaccine and autism, was encouraging parishioners to get vaccinated. However, she said she still has some concerns about vaccines, particularly for young children with a family history of autism, and where several immunizations are given at the same time. William Schaffner, professor at the Vanderbilt University School of Medicine , described the pastor as "misinformed" and said that young children are among the most vulnerable to measles. [62]
In February and March 2014, 20 confirmed cases appeared in New York City. [63]
In December 2014, a measles outbreak began at Disneyland in Southern California.  At least 173 people have become infected with measles in 21 states, as of May 2015.  Health officials say 39 cases have been traced to direct exposure at the park, with 117 infections linked by primary or secondary exposure. Among the 110 California patients, 49 (45%) were unvaccinated; five (5%) had 1 dose of measles-containing vaccine, seven (6%) had 2 doses, one (1%) had 3 doses, 47 (43%) had unknown or undocumented vaccination status, and one (1%) had immunoglobulin G seropositivity documented, which indicates prior vaccination or measles infection at an undetermined time. 12 of the unvaccinated patients were infants too young to be vaccinated. [64] [65] Medical professionals, such as David Gorski , have criticized physicians and pediatricians in the area who do not adhere to the CDC 's recommended vaccination schedule or discourage vaccination, among them Bob Sears and Jay Gordon for reducing vaccination rates and thus weakening herd immunity , and creating a situation in which an outbreak was more likely. [66] California passed a mandatory vaccination law in June 2015. [67]
In January 2015, it was reported that over 70 people who had visited Disneyland or Disney California Adventure between Dec. 15 and Dec. 20 fell ill with measles, with 62 of them residing in California . The total number of cases included five people who had been fully vaccinated against the disease. [68] Between the dates of January 1 and 28, 2015, most of the 84 people who were diagnosed with measles were either infected during their visit to Disneyland or by someone who visited the theme park. [69]
In Spring 2015, the death of an immune-suppressed woman in Washington State caused by measles was diagnosed after autopsy. This was the first U.S. measles death since 2012. [70]
In spring 2017, a measles outbreak occurred in Minnesota. As of June 16, 78 cases of measles had been confirmed in the state, 71  were unvaccinated and 65 were Somali-Americans. [71] [72] [73] [74] [75] [76] The outbreak has been attributed to low vaccination rates among Somali-American children, which can be traced back to 2008, when Somali parents expressed concerns about disproportionately high numbers of Somali preschoolers in special education classes who were receiving services for autism spectrum disorder. Around this time, Andrew Wakefield visited Minneapolis, teaming up with vaccine-skeptical groups to raise concerns about the MMR vaccine. [77] Multiple studies have shown no connection between the MMR vaccine and autism . [78]
In January 2019, Washington's Clark County Public Health officials declared a public health emergency due to a measles outbreak. As of February 28, 2019, 65 cases were identified. [79]
In March 2019, a disaster was declared by county authorities and the county health department in Rockland County, New York, over a growing measles outbreak there. [80] Additionally there has been 259 confirmed cases of measles in Brooklyn and Queens, most of which are affecting the Orthodox Jewish community. [81]
The first half of 2018 saw 1613 cases in addition to the 413 cases reported in 2017, according to the Pan American Health Organization epidemiological alerts and updates. [82]
Approximately 100 cases of the disease were reported in Israel between August 2007 and May 2008 (in sharp contrast to just some dozen cases the year before). [ citation needed ] Many children in ultra-Orthodox Jewish communities were affected due to low vaccination coverage. [83] [84]
In 2007, a large measles outbreak in Japan caused a number of universities and other institutions to close in an attempt to contain the disease. [85] [86]
In early 2010, there was a serious outbreak of measles in the Philippines with 742 cases, leaving four unvaccinated children dead in the capital city of Manila . [87]
In late 2013, it was reported in the Philippines that 6,497 measles cases occurred which resulted in 23 deaths. [88]
In 2014 the Philippines experienced a large measles outbreak. According to the World Health Organization there were 57,564 suspected cases of measles, including 21,403 confirmed cases, and 110 measles deaths reported in the Philippines from January 1 through December 20, 2014. Most of the cases were among unvaccinated people. [47] A major outbreak was declared on February 6, 2019, with 70 recorded deaths of children, this outbreak was attributed to the " Dengvaxia scare ". [89]
Despite the highest ever vaccination rate of 90% achieved in 2017 in the European region, number of measles cases tripled the next year reaching 82,596 with 72 of them resulting in death. [91] Almost two thirds of them were registered in Ukraine, [92] where vaccination rates dropped to 31% in 2016. [93]
Ukraine has had a multiple large outbreaks of measles. In 2001–2002, there were around 25,000 cases and 14 deaths reported. [94] In 2006 with 44,534 reported cases with at least 2 deaths. [95] 13,517 cases were reported in 2012. [96] In 2019 Ukraine reported over 57,000 cases, over half the total reports in the WHO European Region. [97]
Ukraine is suffering one of the world's worst measles epidemics with more than 100,000 cases from 2017 to June 2019, with 16 deaths in 2018. In 2016 only 31% of the population had been immunised with the MMR vaccine . Various reasons are given for the low rate of vaccination including: a distrust of the state in the 1990s, a failure to keep vaccine supplies reliably refrigerated leading to ineffectiveness, a poorly informed medical profession and a high level of vaccine distrust in the wider population. Children are required to be vaccinated before entering the school system, but UNICEF estimates that as many as 30% of vaccination certificates are falsified. [98] [93]
Germany has faced repeated outbreaks in the 21st century. 6,037 cases were reported in 2001 with at least two deaths. [99] More than 1,500 cases were reported in 2006. [100] 1,600 cases were reported in an outbreak in 2013. [100] An outbreak in 2015 had more than 1,700 cases had been reported by May 11 with one death. [101]
In September 2008 an outbreak occurred among anthroposophists' children in the cities of The Hague and Utrecht . Some 90 infections of unvaccinated children were recorded by the Dutch National Institute for Public Health and the Environment (RIVM) by September 29. It was expected the outbreak would spread to the region of the Veluwe , locally known as a bijbelgordel ("Bible Belt") with a large population of unvaccinated children on religious grounds. [102]
In June 2013, another outbreak occurred in the Bible Belt in The Netherlands. According to newspaper Algemeen Dagblad , there were 161 infections, of which 5 infected victims were hospitalized critically ill. 2 of the victims had meningitis , two others had pneumonia and from one of them, the complications are still unknown. [103]
After the MMR vaccine controversy began, the MMR vaccination compliance dropped sharply in the United Kingdom, from 92% in 1996 to 84% in 2002. In some parts of London, it was as low as 61% in 2003, far below the rate needed to avoid an epidemic of measles. [104] By 2006 coverage for MMR in the UK at 24 months was 85%, lower than the about 94% coverage for other vaccines. [105]
After vaccination rates dropped, the incidence of two of the three diseases increased greatly in the UK. In 1998 there were 56 confirmed cases of measles in the UK; in 2006 there were 449 in the first five months of the year, with the first death since 1992. [106] Cases occurred in inadequately vaccinated children. [106] The age group affected was too old to have received the routine MMR immunizations around the time the paper by Wakefield et al. was published, and too young to have contracted the natural disease as a child, and thus to achieve a herd immunity effect. With the decline in infection that followed the introduction of the MMR vaccine, these individuals had not been exposed to the disease, but still had no immunity, either natural or vaccine induced. Therefore, as immunization rates declined following the controversy and the disease re-emerged, they were susceptible to infection. [107] [108] Measles cases continued in 2006, at incidence rates 13 times greater than 1998 levels. [109] Two children were severely and permanently injured by measles encephalitis despite undergoing kidney transplantation in London. [110] Disease outbreaks also caused casualties in nearby countries including Ireland.
In 2008, for the first time in 14 years, measles was declared endemic in the UK, meaning that the disease was sustained within the population.  This was caused by the preceding decade's low MMR vaccination rates, which created a population of susceptible children who could spread the disease. In May 2008, a British 17-year-old with an underlying immunodeficiency died of measles. [111]
An outbreak centered on the Swansea area of Wales started in November 2012; as of 22 April there have been 886 cases. [112]
In March 2013, an epidemic was declared in Swansea , Wales, UK with 1,219 cases and 88 hospitalizations to date. [113] A 25-year-old male had measles at the time of death and died from giant cell pneumonia caused by the disease. [114] There have been growing concerns that the epidemic could spread to London and infect many more people due to poor MMR uptake, [115] prompting the Department of Health to set up a mass vaccination campaign targeted at one million school children throughout England . [116]
In April 2019 a senior epidemiologist at Public Health England said that confidence in the immunisation programme was high and that timing, availability and location of appointments were the main barriers to vaccination. [117]
1,500 cases and three deaths were reported in the Irish outbreak of 2000, which occurred as a direct result of decreased vaccination rates following the MMR scare. [110]
In 2017, there were 4,991 cases and four deaths, representing almost six-times the number of cases in 2016. [118] The number of cases for 2013 through 2016 were 2211, 1674, 251 and 844. [118]
Beginning in April 2009 there was a large outbreak of measles in Bulgaria, with 23,791 cases, including 24 deaths, reported up to 28 July 2010. [119] From Bulgaria, the strain was carried to Germany, Turkey, Greece, Macedonia, and other European countries. [120]
Between January 2008 and May 2012, 22,000 measles cases were reported in France.  5,000 patients were hospitalised including 1,023 with severe pneumonia, 27 with encephalitis and/or myelitis, and 10 died. [121] An awareness campaign about MMR vaccination was launched. [122]
Most recent reported cases of measles in Portugal are originally imported from other European countries (United Kingdom, France and Romania), Africa (Angola, South African and Ethiopia) and Asia (China). [123] Since 2004, Portugal reported 22 confirmed cases of measles. Virus isolates from 2005 and 2008 outbreaks belong the genotype D4. [124] Vaccination coverage in Portugal is ≥95% (since 2006) but pockets of reduced immunization coverage (85-94%) still persist in the population. [125]
Fourteen cases have been reported in multiple Australian and New Zealand cities including Melbourne and Auckland in the period between December 7, 2013 and January 3, 2014. The outbreak is believed to have begun at the 2013 World Supremacy Battlegrounds dance festival held in Sydney, Australia . [126]
Ten cases were reported in Christchurch in July 2009. [127]
An outbreak between 25 May 2011 and 24 July 2012 in the Auckland region had 489 confirmed or probable cases of measles, 82 of which required hospitalisation. [128] The outbreak was started with an unimmunised child becoming infected on a family trip to England , then developing measles back in Auckland. [128]
In June and July 2014, 124 confirmed cases of measles were reported in Hamilton . Eighty percent of persons infected were aged between 10 and 20, and all but four were not immunised. Most of those infected were linked with Fraser High School in the city's west, resulting in the school cancelling all school trips and cultural and sporting events. [129] [130]
In 2019, New Zealand saw its worst outbreak in two decades. As of 5 September 2019, there had been 1,051 reported cases, mainly in Auckland which has seen 877 cases. [131] [132] There were also reported cases in the South Island 's Canterbury region , Otago and Southland regions. [133] In response to the epidemic, the Government established a National Health Coordination Centre in Auckland. [134] On 5 September, the United States Department of State and the Center for Disease Control issued a health travel advisory for US citizens traveling to New Zealand. [135] [136] By 17 September, the number of measles cases had reached 1,327, with 1,108 reported in Auckland. Over 54,000 doses of vaccination had been distributed that month with 52,000 more doses arriving from Belgium on that date. [137]
An outbreak in November 2019 resulted in the deaths of 24 children and one adult from over 2,200 cases as of November 25, 2019. The Health Ministry estimates two-thirds of its 200,000 residents are vaccinated, while UNICEF puts the vaccination rate even lower at 28-40 per cent. [138] [139] [140] The death toll had increased to 39 by November 28, 2019. [141] The death toll reached 53 on December 2. The government had declared a state of emergency on November 15, when schools were closed and children were banned from public gatherings. [142]
This article provides a global overview of the current trends and distribution of metabolic syndrome .  Metabolic syndrome (also known as the cardiometabolic syndrome) refers to a cluster of related risk factors for cardiovascular disease that includes abdominal obesity , diabetes , hypertension , and elevated cholesterol . [1] [2]
Data from the World Health Organization suggests 65% of the world's population live in countries where being overweight or obese kills more people than being underweight. [3] The WHO defines “overweight” as a BMI greater than or equal to 25, and “obesity” as a BMI greater than or equal to 30. [3] Both overweight and obesity are major risk factors for cardiovascular diseases , specifically heart disease and stroke, and diabetes.
The International Diabetes Federation reports that as of 2011, 366 million people suffer from diabetes; this number is projected to increase to over half a billion (estimated 552 million) by 2030. [4] 80 percent of people with diabetes live in developing countries and in 2011, diabetes caused 4.6 million deaths and approximately 78,000 children were diagnosed with type 1 diabetes . [4]
Different definitions of the cardiometabolic syndrome have been proposed by different public health organizations, but recently the International Diabetes Federation (IDF), the National Heart, Lung, and Blood Institute (NHLBI), the American Heart Association (AHA), and others proposed a definition for diagnosing the cardiometabolic syndrome that includes the presence of 3 out of the following 5 risk factors: [1] [2]
Approximately 40 – 46 percent of the world’s adult population has the cluster of risk factors that is metabolic syndrome. [5] In 2000, approximately 32% of U.S. adults had the metabolic syndrome. [1] In more recent years that figure has climbed to 34%. [2] [6]
People with the cardiometabolic syndrome have twice the likelihood of developing and dying from cardiovascular disease, and more than seven times the risk of developing diabetes, compared to those with no cardiometabolic risk factors. [7] [8] [9] [10]
Diabetes now affects over 14 million people in the central and southern regions of Africa; this number is expected to increase to 28 million people by 2030, according to the IDF Africa. [11] The NGO Project Hope cites lifestyle changes as the primary cause of the increase of diabetes, specifically type 2 diabetes, which seems to correspond with a growing waist line.  Lack of physical activity, increased consumption of processed food and unmanaged portion sizes all contribute to the rise of diabetes – a major component of cardiometabolic risk. [12] In countries where there are food crises, “much of the foods donated from the international community are calorie-dense foods”, [12] according to Project Hope’s Senior Advisor, Paul Madden.  Nutrition education is essential to prevent type 2 diabetes from consuming the continent. The NGO also suggests that in some villages, 70 to 80 percent of the people may not even be aware that they are living with the disease. [12]
Studies published in the Indian Journal of Endocrinology and Metabolism focused on the prevalence of metabolic syndrome and its components in different African populations using various criteria. Reports from Lagos, Nigeria , for instance, showed the prevalence rate of metabolic syndrome as high as over 80% among diabetic patients. [13] The current trend of rising metabolic syndrome in African populations is largely and generally attributed to “adoption of western lifestyle which is characterized by reduced physical activity, substitution of the traditional African diet rich in fruits, and vegetables for the more energy-laden foods”. [13]
Currently, more than 55 million people in Europe have been diagnosed with diabetes, according to the IDF; by 2030 this total will rise to 64 million people. [14] Roughly 8.4% of adults are suffering from the effects of this disease, which caused 622,114 deaths in the region this year. 33 IDF studies have also concluded that Europe has the highest number of children with type 1 diabetes . [14]
The European Global Cardiometabolic Risk Profile in Patients with Hypertension Disease (GOOD) survey investigated the cardiometabolic risk profile in adult patients with hypertension across 289 locations in four European regions.  Across the Northwest, Mediterranean, Atlantic European Mainland and Central Europe zones, demographic, lifestyle, clinical and laboratory data were collected from eligible patients during one clinic visit. [15] In Central Europe 44% of the participants had type 2 diabetes compared with 33% in the Atlantic European Mainland, and 26% in the Northwest and the Mediterranean regions. [15] The study revealed a prevalence of metabolic syndrome affected 68% of Central Europe, 60% of the Atlantic European Mainland, 52% of the Mediterranean regions and 50% of Northwest Europe. [15] Fasting blood glucose, total cholesterol and triglyceride levels were all highest in Central Europe compared with the other three regions. [15] Roughly 80% of the Atlantic European Mainland patients had uncontrolled blood pressure, whereas the other three regions tallied approximately 70-71%. [15] Compared to the Northwest, Mediterranean, and Central Europe regions, declared alcohol consumption was also recorded the highest in the Atlantic European Mainland; exercise was lowest in Central Europe. [15]
The GOOD survey recorded cases of congestive heart failure , left ventricular hypertrophy , coronary artery disease and stable/unstable angina were highest in Central Europe compared with the other regions. [15] Family history of premature stroke or myocardial infarction , stroke, coronary revascularization and transient ischaemic attacks had the highest prevalence in the Atlantic European Mainland. [15] Statistical conclusions indicate that hypertensive patients across Europe exhibit multiple cardiometabolic risk factors, with greater predominance in Central Europe and the Atlantic European Mainland compared with Northwest and Mediterranean regions. [15]
The International Diabetes Federation reports more than 34.2 million people in the Middle East and North Africa have diabetes; this number will rise to 59.7 million by the year 2030 unless counteractive measures are introduced. [16] In 2012, diabetes caused 356,586 deaths in this region, a zone with the highest prevalence of diabetes in adults (11%) in the world. [16]
Turkey reported a prevalence of 33.9% for metabolic syndrome (MS), with a higher prevalence in women (39.6%) than in men (28%). [17] The survey included random samples from both urban and rural populations in seven geographical regions of Turkey.  More than one-third (35.08%) of the participants were obese. [17] Of those tested, 13.66% had hypertension, while those with diabetes mellitus (DM) and MS were 4.16% and 17.91%, respectively. [17] The frequency of hypertension, MS and obesity were higher in females than males; however, DM was higher in males than females. [17] According to the IDF, metabolic syndrome was prevalent in 16.1% of the Saudi Arabian population. [17] In Tunisia, metabolic syndrome incidence was 45.5% based on the IDF criteria. [17] 37.4% of Iranians aging from 25-64, living in both urban and rural areas of all 30 provinces in Iran, had MS (based on the IDF definition); results based on the Adult Treatment Panel III (ATPIII)/American Heart Association (AHA)/National Heart, Lung, and Blood Institute (NHLBI) standards suggest 41.6% of the same group of Iranians suffer from metabolic syndrome. [17] The affliction of the MS is estimated to affect more than 11 million Iranians. [17]
Current IDF data proposes more than 38.4 million people in North American and the Caribbean have diabetes and projects this number will increase to 51.2 million by 2030. [18] In 2012, 11% of (or approximately 4.2 million) adults in the NAC Region endured the disease; this year, diabetes was responsible for 287,020 deaths in North America. [18]
The National Center for Biotechnological Information notes the incidence of the metabolic syndrome among Caribbean-born persons in the U. S. Virgin Islands is comparable to that among the population on the mainland of the United States. The groups involved in the study were Hispanic white, Hispanic black, non-Hispanic black born in the U.S. Virgin Islands, and non-Hispanic black born elsewhere in the Caribbean. [19] Hispanic ethnicity was independently associated with an increased risk of having the metabolic syndrome, high triglycerides, and low high density lipoprotein cholesterol levels. [19] Among Caribbean-born persons living in the U.S. Virgin Islands, those who are Hispanic blacks may have a greater risk of cardiovascular disease than do other groups. [19]
Approximately 27 million Americans, or nearly 11% of the population, have diabetes, according to the American Diabetes Association and the Centers for Disease Control and Prevention . [20] By 2050, the prevalence of diabetes could increase to as much as 33% of the population, largely due to the aging of the population and to people with diabetes living longer. [21] Approximately 1.9 million new cases of diabetes are diagnosed each year. The disease was the seventh leading cause of death in 2007, directly claiming more than 71,000 lives and contributing to approximately 160,000 additional deaths. [20] Patients with diabetes are two to four times more likely than those without it to die from cardiovascular disease, and diabetes is an important cause of blindness, kidney disease, and lower-extremity amputations. [20]
An additional 79 million Americans have prediabetes . [20] Individuals with prediabetes have blood glucose levels that are higher than normal but not high enough to be classified as diabetes. [22] Without intervention, most people with prediabetes will develop diabetes within ten years. [23] In addition, studies have shown that these individuals are at increased risk for cardiovascular disease, including a heart attack or stroke. [24] [25] [26] Individuals with prediabetes are also likely to have additional cardiovascular risk factors such as elevated cholesterol and high blood pressure. [24]
Diabetes and prediabetes are strongly linked to obesity and overweight. [27] Nearly 50% of people with diabetes are obese, and 90% are overweight. [27] [28] 19 A chief risk factor for prediabetes is excess abdominal fat. [22] [23] Obesity increases one’s risk for a variety of other medical problems, including hypertension, stroke, other forms of cardiovascular disease, arthritis , and several forms of cancer . [29] [30] Obese individuals are at twice the risk of dying from any cause than normal-weight individuals. [31] The prevalence of obesity and overweight have risen to epidemic proportions in the United States, where 67% of adults are overweight and, of these, approximately half are obese. [30] [31]
The prevalence of hypertension, another cardiometabolic syndrome component, has been increasing for the last decade. In 1994, 24% of U.S. adults had hypertension. Today, that figure has risen to 29%, according to data from the National Health and Nutrition Examination Survey. [31] [32] In addition, nearly 30% of U.S. adults have prehypertension. [31] Hypertension increases one’s risk of suffering a stroke, developing end-stage renal disease, and dying from cardiovascular disease. In recent years, hypertension has directly claimed more than 61,000 U.S. lives and has contributed to approximately 347,000 deaths each year. [31]
Of the components of the cardiometabolic syndrome, only the prevalence of dyslipidemia has declined in the United States. Between 1999 and 2010, the percentage of U.S. adults with high total cholesterol declined from approximately 18% to 13%. [33] However, according to the American Heart Association , nearly half of U.S. adults today (44%) are still at increased risk for atherosclerotic disease because their levels of total cholesterol are elevated (200 mg/dL or higher). [31] Of these 98.8 million individuals with elevated cholesterol, 33.6 million have high cholesterol (240 mg/dL or above), and 71.3 million have low-density lipoprotein (LDL) cholesterol levels of 130 mg/dL or higher. [31] In addition, approximately 19% of U.S. adults have low levels of high-density lipoprotein (HDL) cholesterol, [10] and one-third have elevated triglycerides. [34] Finally, dyslipidemia affects the vast majority (up to 97%) of individuals with diabetes and contributes to their elevated risk for cardiovascular disease. [35]
According to estimates from the American Heart Association, more than 9% of U.S. children and adolescents aged 12–19, or nearly three million individuals, have the metabolic syndrome. [31] Among overweight and obese adolescents, this prevalence rate rises to 44%. Two-thirds of adolescents have at least one metabolic abnormality. [31]
Preliminary prospective studies report that children and adolescents with the metabolic syndrome are at high risk of developing cardiovascular disease and diabetes as adults. One 25-year prospective study found that, compared with children without the syndrome, those with the metabolic syndrome are 14 times more likely to suffer from cardiovascular disease and 11 times more likely to develop diabetes when they reach adulthood. [36] Cardiometabolic risk among children and adolescents is fueled by the rising prevalence of obesity in this age group. From 1980 to 2008, rates of obesity have increased from 5% to 10% among preschool children aged 2–5. During the same time period, obesity increased from 6.5% to nearly 20% among 6-11year-olds and from 5% to 18% among adolescents aged 12–19. [37] Hypertension among children and adolescents has increased by 1% since 1999 and is estimated to affect 3.6% of those aged 3–18. [31] This increase is attributed to the rising number of overweight and obese children. [31] The prevalence of lipid abnormalities among children and adolescents is also tied to obesity and overweight. Approximately 14% of normal-weight youths aged 12–19 have lipid abnormalities. [31] That figure rises to 22% of overweight youths and nearly 43% of obese youths. [31]
Obesity is also tied to the rise of type 2 diabetes among U.S. children. Until recently, diabetes in children was typically assumed to be type 1, formerly known as juvenile-onset diabetes. [38] However, according to the Centers for Disease Control and Prevention, recent clinical evidence indicates that the prevalence of type 2 diabetes, formerly known as adult-onset diabetes, is increasing among American children and adolescents. [38] This increase is most notable among Blacks, Asian/Pacific Islanders, Hispanics, and American Indians. Children who develop type 2 diabetes are typically overweight or obese. “Type 2 diabetes in children and adolescents already appears to be a sizable and growing problem,” the CDC says. “Better physician awareness and monitoring of the disease’s magnitude will be necessary.” [38]
The National Cholesterol Education Program compiled and presented data from the Indian Health Service that indicates increasing mortality rates due to cardiovascular disease vary among American Indian communities.  The significant independent predictors of CVD in Native American women were diabetes, age, obesity, LDL, albuminuria, triglycerides, and hypertension. [39] In men the significant predictors of CVD were diabetes, age, LDL, albuminuria, and hypertension. [39] Unlike other ethnic groups, Native Americans appear to have an increasing frequency of coronary heart disease, possibly related to the high and increasing prevalence of diabetes in these communities. [39] Although total and LDL-cholesterol levels are lower than the U.S. average, importance of LDL cholesterol as a contributor to CHD in this group should not be underestimated. Moreover, because of the high frequency of type 2 diabetes, many Native Americans will have an even lower LDL goal. [39] The evidence for differences in baseline risk between Native American and white populations is not strong enough to justify separate guidelines for Native American populations.
The IDF reports 9.2% of adults in the South and Central America have diabetes and 12.3% of deaths in adults in the SACA Region can be attributed to the disease. [40] More than 26.4 million people in the SACA Region have diabetes; by 2030 this will rise to 39.9 million. [40] Approximately 236,328 disease related fatalities occurred in the SACA Region in 2012. [40]
The Latin American populations exhibit a high prevalence of abdominal obesity and metabolic syndrome, similar or even higher than developed countries. It is attributed to changes in their lifestyle, migration from rural to urban areas and a higher susceptibility to accumulate abdominal fat and develop more insulin resistance compared to other ethnically different populations. [41] Some genetic factors and metabolic adaptations during fetal life can be claimed as etiological factors of this condition. [41]
Although cardiovascular disease (CVD) is the leading cause of death and disability in the majority of the countries in Latin America, few data about regional differences on this topic has emerged. [42] Developing countries have scarce epidemiological data on cardiovascular (CV) risk factor prevalence, which only allows for limited control and treatment options. [43] The load of the CV risk factors, especially hypertension, remains uncertain.
A 2012 IDF South-East Asia report states one fifth of all adults living with diabetes live in South East Asia and 8.7% of adults in the region endure the disease, according to the International Diabetes Federation. [43] As of this 2012, 70.3 million people in the SEA Region have diabetes; by 2030 this will rise to 120.9 million diagnoses. [43]
There has been special interest in South Asians because they have been reported to have very high frequency rates of coronary heart disease at younger ages in the absence of traditional risk factors.  The higher CHD risk in this population may be related in part to a higher prevalence of insulin resistance, the metabolic syndrome, and diabetes. [39] Lipoprotein levels have also been reported to be elevated, elevating the importance of initiating remedies to reduce cholesterol and other CHD risk factors in this group with South Asian Indian ancestry. [39] A growing body of evidence indicates that South Asians are at high baseline risk for CHD, compared to American whites; they are particularly at risk for the metabolic syndrome and type 2 diabetes. [39] Also, increased emphasis should be given to life habit changes to mitigate the metabolic syndrome in this population. [39] All other data reflects cholesterol management guidelines should remain the same for the SEA population as well as for other population groups.
The IDF Western pacific reports more people with diabetes live in the Western Pacific than any other region in the world. [44] Approximately 132.2 million people in the WP Region have diabetes; if proper precautions are not utilized, this number is projected to escalate to 187.9 million people by 2030. 44 8% of all Western Pacific adults have diabetes and in 2012, the illness caused 1.7 million deaths in the Western Pacific. [44]
There is limited information on the risks and benefits of lipid management for reduction of coronary heart disease (CHD) and cardiovascular disease (CVD) in this population. [39] In the Honolulu Heart Program report, CHD and CVD mortality rates are lower than in the general U.S. population. [39] However, the evidence for differences between Pacific Islander and general U.S. Populations is not strong enough to justify the creating of separate guidelines. [39]
Globally about 600 cases of plague are reported a year. [1] In 2017 and November 2019 the countries with the most cases include the Democratic Republic of the Congo , Madagascar , and Peru . [1]
Local outbreaks of the plague are grouped into three plague pandemics , whereby the respective start and end dates and the assignment of some outbreaks to either pandemic are still subject to discussion. [2] The pandemics were:
However, the late medieval Black Death (roughly 1331 to 1353) is sometimes seen not as the start of the second, but as the end of the first pandemic – in that case, the first pandemic ended in around 1353, and the second pandemic's start would be about 1361. Also various end dates of the second pandemic are given in the literature, ranging from about 1840 to 1890. [2]
The word plague is believed to come from the Latin word plāga ("blow, wound") and plangere (“to strike, or to strike down”), via the German Plage (“infestation”).
Plasmids of Y. pestis have been detected in archaeological samples of the teeth of seven Bronze Age individuals from 5000 years ago (3000 BC), in the Afanasievo culture in Siberia, the Corded Ware culture in Estonia, the Sintashta culture in Russia, the Unetice culture in Poland and the Andronovo culture in Siberia. Y. pestis existed over Eurasia during the Bronze Age. Estimates of the age of the Most recent common ancestor of all Y. pestis is estimated at 5,783 years Before Present .
The Yersinia murine toxin ( ymt ) allows the bacteria to infect fleas, which can then transmit bubonic plague. Early ancestral versions of Y. pestis did not have the ymt gene, which was only detected in a 951 calibrated BC sample. [5] [6]
The Amarna letters and the Plague Prayers of Mursili II describe an outbreak of a disease among the Hittites . [7] [8] [9] The First Book of Samuel [10] describes a possible plague outbreak in Philistia , and the Septuagint version says it was caused by a "ravaging of mice". [11]
In the second year of the Peloponnesian War (430 BC), Thucydides described an epidemic disease which was said to have begun in Ethiopia , passed through Egypt and Libya , then come to the Greek world. In the Plague of Athens , the city lost possibly one third of its population, including Pericles . Modern historians disagree on whether the plague was a critical factor in the loss of the war. Although this epidemic has long been considered an outbreak of plague, many modern scholars believe that typhus , [12] smallpox , or measles may better fit the surviving descriptions. A recent study of DNA found in the dental pulp of plague victims suggests that typhoid was actually responsible. [13]
In the first century AD, Rufus of Ephesus , a Greek anatomist, refers to an outbreak of plague in Libya , Egypt , and Syria . He records that Alexandrian doctors named Dioscorides and Posidonius described symptoms including acute fever, pain, agitation, and delirium. Buboes—large, hard, and non-suppurating—developed behind the knees, around the elbows, and "in the usual places." The death toll of those infected was very high. Rufus also wrote that similar buboes were reported by a Dionysius Curtus, who may have practiced medicine in Alexandria in the third century BC. If this is correct, the eastern Mediterranean world may have been familiar with bubonic plague at that early date. [14] [15]
In the second century, the Antonine Plague , named after Marcus Aurelius ’ family name of Antoninus and also known as the Plague of Galen, who had first hand knowledge of the disease, may in fact have been smallpox . Galen was in Rome when it struck in 166 AD, and was also present in the winter of 168–69 during an outbreak among troops stationed at Aquileia ; he had experience with the epidemic, referring to it as very long lasting, and describes its symptoms and his treatment of it, though his references are scattered and brief. According to Barthold Georg Niebuhr [16] "this pestilence must have raged with incredible fury; it carried off innumerable victims. The ancient world never recovered from the blow inflected upon it by the plague which visited it in the reign of M. Aurelius." The mortality rate of the plague was 7–10 percent; the outbreak in 165/6–168 would have caused approximately 3.5 to 5 million deaths. Otto Seek believes that over half the population of the empire perished. J. F. Gilliam believes that the Antonine plague probably caused more deaths than any other epidemic during the empire before the mid-3rd century.
The Plague of Justinian in AD 541–542 is the first known attack on record, and marks the first firmly recorded pattern of bubonic plague. This disease is thought to have originated in China. [17] It then spread to Africa from where the huge city of Constantinople imported massive amounts of grain, mostly from Egypt, to feed its citizens. The grain ships were the source of contagion for the city, with massive public granaries nurturing the rat and flea population. At its peak, Procopius said the plague was killing 10,000 people in Constantinople every day. The real number was more likely close to 5,000 a day. [18] The plague ultimately killed perhaps 40% of the city's inhabitants, and then continued to kill up to a quarter of the human population of the eastern Mediterranean.
In AD 588 a second major wave of plague spread through the Mediterranean into what is now France. It is estimated that the Plague of Justinian killed as many as 100 million people across the world. [19] [20] It caused Europe's population to drop by around 50% between 541 and 700. [21] It also may have contributed to the success of the Arab conquests . [22] [23] An outbreak of it in the AD 560s was described in AD 790 as causing "swellings in the glands ... in the manner of a nut or date" in the groin "and in other rather delicate places followed by an unbearable fever". While the swellings in this description have been identified by some as buboes, there is some contention as to whether the pandemic should be attributed to the bubonic plague, Yersinia pestis , known in modern times. [24]
From 1331 to 1351, the Black Death , a massive and deadly pandemic originating in China, spread along the Silk Road and swept through Asia, Europe and Africa. [17] It may have reduced the world's population from 450 million to between 350 and 375 million . [25] China lost around half of its population, from around 123 million to around 65 million ; Europe around one third of its population, from about 75 million to about 50 million ; and Africa approximately 1 ⁄ 8 of its population, from around 80 million to 70 million (mortality rates tended to be correlated with population density so Africa, being less dense overall, had the lowest death rate). This makes the Black Death the largest death toll from any known non-viral epidemic. Although accurate statistical data does not exist, it is thought that 1.4 million died in England ( 1 ⁄ 3 of England's 4.2 million people), while an even higher percentage of Italy's population was likely wiped out. On the other hand, north-eastern Germany, Bohemia, Poland and Hungary are believed to have suffered less, and there are no estimates available for Russia or the Balkans. It is conceivable that Russia may not have been as affected due to its very cold climate and large size, hence often less close contact with the contagion.
The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. [26] According to Biraben, plague was present somewhere in Europe in every year between 1346 and 1671. [27] The Second Pandemic was particularly widespread in the following years: 1360–1363; 1374; 1400; 1438–1439; 1456–1457; 1464–1466; 1481–1485; 1500–1503; 1518–1531; 1544–1548; 1563–1566; 1573–1588; 1596–1599; 1602–1611; 1623–1640; 1644–1654; and 1664–1667; subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). [28] According to Geoffrey Parker, " France alone lost almost a million people to plague in the epidemic of 1628–31." [29]
In England, in the absence of census figures, historians propose a range of pre-incident population figures from as high as 7 million to as low as 4 million in 1300, [30] and a postincident population figure as low as 2 million. [31] By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–62, 1369, 1379–83, 1389–93, and throughout the first half of the 15th century. [32] An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–80 could have been as high as 20%. [33] The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665. [34]
In 1466, perhaps 40,000 people died of plague in Paris. [35] During the 16th and 17th centuries, plague visited Paris for almost one year out of three. [36] The Black Death ravaged Europe for three years before it continued on into Russia, where the disease hit somewhere once every five or six years from 1350 to 1490. [37] Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, [38] reducing its population by 10 to 30% during those years. [39] Over 10% of Amsterdam 's population died in 1623–1625, and again in 1635–1636, 1655, and 1664. [40] There were 22 outbreaks of plague in Venice between 1361 and 1528. [41] The plague of 1576–1577 killed 50,000 in Venice, almost a third of the population. [42] Late outbreaks in central Europe included the Italian Plague of 1629–1631 , which is associated with troop movements during the Thirty Years' War , and the Great Plague of Vienna in 1679. Over 60% of Norway's population died from 1348 to 1350. [43] The last plague outbreak ravaged Oslo in 1654. [44]
In the first half of the 17th century, the Great Plague of Milan claimed some 1.7 million victims in Italy, or about 14% of the population. [45] In 1656, the plague killed about half of Naples ' 300,000 inhabitants. [46] More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain . [47] The plague of 1649 probably reduced the population of Seville by half. [48] In 1709–1713, a plague epidemic that followed the Great Northern War (1700–1721, Sweden v. Russia and allies) [49] killed about 100,000 in Sweden, [50] and 300,000 in Prussia. [48] The plague killed two-thirds of the inhabitants of Helsinki , [51] and claimed a third of Stockholm 's population. [52] Western Europe's last major epidemic occurred in 1720 in Marseilles , [43] in Central Europe the last major outbreaks happened during the plague during the Great Northern War , and in Eastern Europe during the Russian plague of 1770–72 .
The Black Death ravaged much of the Islamic world . [53] Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. [54] Plague repeatedly struck the cities of North Africa. Algiers lost 30,000–50,000 to it in 1620–1621, and again in 1654–1657, 1665, 1691, and 1740–1742. [55] Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, 37 larger and smaller epidemics were recorded in Constantinople , and 31 between 1751 and 1800. [56] Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out. [57]
The Third Pandemic began in China's Yunnan province in 1855, spreading plague to all inhabited continents and ultimately killing more than 12 million people in India and China alone. Casualty patterns indicate that waves of this pandemic may have come from two different sources. The first was primarily bubonic and was carried around the world through ocean-going trade, transporting infected persons, rats, and cargoes harboring fleas. The second, more virulent, strain was primarily pneumonic in character, with a strong person-to-person contagion. This strain was largely confined to Manchuria and Mongolia . Researchers during the "Third Pandemic" identified plague vectors and the plague bacterium (see above), leading in time to modern treatment methods. [ citation needed ]
Plague occurred in Russia in 1877–1889 in rural areas near the Ural Mountains and the Caspian Sea . Efforts in hygiene and patient isolation reduced the spread of the disease, with approximately 420 deaths in the region. Significantly, the region of Vetlianka in this area is near a population of the bobak marmot , a small rodent considered a very dangerous plague reservoir. The last significant Russian outbreak of Plague was in Siberia in 1910 after sudden demand for marmot skins (a substitute for sable ) increased the price by 400 percent. The traditional hunters would not hunt a sick Marmot and it was taboo to eat the fat from under the arm (the axillary lymphatic gland that often harboured the plague) so outbreaks tended to be confined to single individuals. The price increase, however, attracted thousands of Chinese hunters from Manchuria who not only caught the sick animals but also ate the fat, which was considered a delicacy. The plague spread from the hunting grounds to the terminus of the Chinese Eastern Railway and then followed the track for 2,700 km. The plague lasted 7 months and killed 60,000 people. [ citation needed ]
The bubonic plague continued to circulate through different ports globally for the next fifty years; however, it was primarily found in Southeast Asia. The 1894 Hong Kong plague had particularly high death rates, 90%. [58] As late as 1897, medical authorities in the European powers organized a conference in Venice , seeking ways to keep the plague out of Europe. Mumbai plague epidemic struck the city of Bombay (Mumbai) in 1896. The disease reached the Territory of Hawaii in December 1899, and the Board of Health's decision to initiate controlled burns of select buildings in Honolulu 's Chinatown turned into an uncontrolled fire which led to the inadvertent burning of most of Chinatown on January 20, 1900. [59] Shortly thereafter, plague reached the continental US, initiating the San Francisco plague of 1900–1904 . Plague persisted in Hawaii on the outer islands of Maui and Hawaii (The Big Island) until it was finally eradicated in 1960. [60]
Research done by a team of biologists from the Institute of Pasteur in Paris and Johannes Gutenberg University Mainz in Germany by analyzing the DNA and proteins from plague pits, published in October 2010, reported beyond doubt that all 'the three major plagues' were due to at least two previously unknown strains of Yersinia pestis and originated from China. A team of medical geneticists led by Mark Achtman of University College Cork in Ireland reconstructed a family tree of the bacterium and concluded in an online issue of Nature Genetics published on 31 October 2010 that all three of the great waves of plague originated from China. [61] [62]
Plague cases were massively reduced during the second half of the 20th century, but outbreaks still occurred, especially in developing countries. Between 1954 and 1997, human plague was reported in 38 countries, making the disease a re-emerging threat to human health. [63] Between 1987 and 2001, 36,876 confirmed cases of plague with 2,847 deaths are reported to the World Health Organization . [64]
In the 21st century, fewer than 200 people die of the plague worldwide each year, mainly due to lack of treatment. [65] Plague is considered to be endemic in 26 countries around the world, with most cases found in remote areas of Africa . [66] The three most endemic countries are Madagascar , the Democratic Republic of the Congo and Peru . [67] Outbreaks with dozens of deaths occurred in Madagascar in 2014 and 2017 , in India in 1994 , and Congo in 2006.
During 1995, plague was confirmed in the United States from nine western states. [68] Currently, five to 15 people in the United States are estimated to catch the disease each year — typically in western states. The reservoir is thought to be mice. [69] [70] In the U.S., about half of all fatal cases of plague since 1970 have occurred in New Mexico . There were two plague deaths in the state in 2006, the first fatalities in 12 years. [71] In New Mexico, four people were diagnosed with the plague in 2015; one died. In 2016, four were diagnosed and all were treated with success. Three others were diagnosed by late June in 2017. Vegetation such as pinyon and juniper trees are thought to support rodents such as the prairie dog and rock squirrel, with their fleas, according to Paul Ettestad of the New Mexico public health department. [72] As well, pets can bring back fleas from dead rodents, he said. The CDC indicates that over the past century, plague in the U.S. has been most common in the areas of northern New Mexico, northwestern Arizona and southern Colorado. [73]

In 2000, typhoid fever caused an estimated 21.7 million illnesses and 217,000 deaths. [1] It occurs most often in children and young adults between 5 and 19 years old. [2] In 2013, it resulted in about 161,000 deaths – down from 181,000 in 1990. [3] Infants, children, and adolescents in south-central and Southeast Asia experience the greatest burden of illness. [4] Outbreaks of typhoid fever are also frequently reported from sub-Saharan Africa and countries in Southeast Asia. [5] [6] [7] In the United States, about 400 cases occur each year, and 75% of these are acquired while traveling internationally. [8] [9]
Historically, before the antibiotic era, the case fatality rate of typhoid fever was 10–20%. Today, with prompt treatment, it is less than 1%. [10] However, about 3–5% of individuals who are infected develop a chronic infection in the gall bladder. [11] Since S. e. subsp. enterica is human-restricted, these chronic carriers become the crucial reservoir, which can persist for decades for further spread of the disease, further complicating the identification and treatment of the disease. [12] Lately, the study of S. e. subsp. enterica associated with a large outbreak and a carrier at the genome level provides new insights into the pathogenesis of the pathogen. [13] [14]
In industrialized nations, water sanitation and food handling improvements have reduced the number of cases. [15] Developing nations, such as those found in parts of Asia and Africa, have the highest rates of typhoid fever. These areas have a lack of access to clean water, proper sanitation systems, and proper health-care facilities. For these areas, such access to basic public-health needs is not in the near future. [16]
In 430 BCE, a plague , which some believe to have been typhoid fever, killed one-third of the population of Athens , including their leader Pericles . Following this disaster, the balance of power shifted from Athens to Sparta , ending the Golden Age of Pericles that had marked Athenian dominance in the Greek ancient world. The ancient historian Thucydides also contracted the disease, but he survived to write about the plague. His writings are the primary source of information on this outbreak, and modern academics and medical scientists consider typhoid fever the most likely cause. In 2006, a study detected DNA sequences similar to those of the bacterium responsible for typhoid fever in dental pulp extracted from a burial pit dated to the time of the outbreak. [17]
The cause of the plague has long been disputed and other scientists have disputed the findings, citing serious methodologic flaws in the dental pulp-derived DNA study. [18] The disease is most commonly transmitted through poor hygiene habits and public sanitation conditions; during the period in question related to Athens above, the whole population of Attica was besieged within the Long Walls and lived in tents. [ citation needed ]
A pair of epidemics struck the Mexican highlands in 1545 and 1576 , causing an estimated 7 to 17 million deaths. [19] A study published in 2018 suggests that the cause was typhoid fever. [20] [21]
Some historians believe that the English colony of Jamestown, Virginia , died out from typhoid. Typhoid fever killed more than 6,000 settlers in the New World between 1607 and 1624. [22]
A long-held belief is that 9th US President William Henry Harrison died of pneumonia , but recent studies suggest he likely died from typhoid.
This disease may also have been a contributing factor in the death of 12th US President Zachary Taylor due to the unsanitary conditions in Washington, DC, in the mid-19th century. [23] [24]
During the American Civil War , 81,360 Union soldiers died of typhoid or dysentery , far more than died of battle wounds. [25] In the late 19th century, the typhoid fever mortality rate in Chicago averaged 65 per 100,000 people a year. The worst year was 1891, when the typhoid death rate was 174 per 100,000 people. [26]
During the Spanish–American War , American troops were exposed to typhoid fever in stateside training camps and overseas, largely due to inadequate sanitation systems. The Surgeon General of the Army, George Miller Sternberg , suggested that the War Department create a Typhoid Fever Board. Major Walter Reed , Edward O. Shakespeare, and Victor C. Vaughan were appointed August 18, 1898, with Reed being designated the president of the board. The Typhoid Board determined that during the war, more soldiers died from this disease than from yellow fever or from battle wounds. The board promoted sanitary measures including latrine policy, disinfection, camp relocation, and water sterilization, but by far the most successful antityphoid method was vaccination, which became compulsory in June 1911 for all federal troops. [27]
In 1902, guests at mayoral banquets in Southampton and Winchester, England, became ill and four died, including the Dean of Winchester , after consuming oysters. The infection was due to oysters sourced from Emsworth , where the oyster beds had been contaminated with raw sewage. [28] [29]
The most notorious carrier of typhoid fever, but by no means the most destructive, was Mary Mallon , also known as Typhoid Mary. In 1907, she became the first carrier in the United States to be identified and traced. She was a cook in New York, who was closely associated with 53 cases and three deaths. [30] Public-health authorities told Mary to give up working as a cook or have her gall bladder removed, as she had a chronic infection that kept her active as a carrier of the disease. Mary quit her job, but returned later under a false name . She was detained and quarantined after another typhoid outbreak. She died of pneumonia after 26 years in quarantine. [ citation needed ]
A notable outbreak occurred in Aberdeen , Scotland , in 1964, due to contaminated tinned meat sold at the city's branch of the William Low chain of stores. No fatalities resulted. [ citation needed ]
In 2004–05 an outbreak in the Democratic Republic of Congo resulted in more than 42,000 cases and 214 deaths. [2] Since November 2016, Pakistan has had an outbreak of extensively drug-resistant (XDR) typhoid fever. [31]
Epizootiology , epizoology , or veterinary epidemiology is the study of disease patterns within animal populations. [1]

This veterinary medicine –related article is a stub . You can help Wikipedia by expanding it .

Eradication is the reduction of an infectious disease 's prevalence in the global host population to zero. [1] It is sometimes confused with elimination, which describes either the reduction of an infectious disease's prevalence in a regional population to zero or the reduction of the global prevalence to a negligible amount. Further confusion arises from the use of the term eradication to refer to the total removal of a given pathogen from an individual (also known as clearance of an infection ), particularly in the context of HIV and certain other viruses where such cures are sought.
The selection of infectious diseases for eradication is based on rigorous criteria, as both biological and technical features determine whether a pathogenic organism is (at least potentially) eradicable. The targeted organism must not have a non-human reservoir (or, in the case of animal diseases, the infection reservoir must be an easily identifiable species, as in the case of rinderpest ), and/or amplify in the environment. This implies that sufficient information on the life cycle and transmission dynamics is available at the time an eradication initiative is programmed. An efficient and practical intervention (such as a vaccine or antibiotic) must be available to interrupt transmission of the infective agent. Studies of measles in the pre-vaccination era led to the concept of the critical community size , the size of the population below which a pathogen ceases to circulate. [2] The use of vaccination programs before the introduction of an eradication campaign can reduce the susceptible population. The disease to be eradicated should be clearly identifiable, and an accurate diagnostic tool should exist. Economic considerations, as well as societal and political support and commitment, are other crucial factors that determine eradication feasibility. [3] [4]
Two infectious diseases have successfully been eradicated: smallpox and rinderpest . There are also four ongoing programs, targeting poliomyelitis , yaws , dracunculiasis , and malaria . Five more infectious diseases have been identified as of April 2008 [update] as potentially eradicable with current technology by the Carter Center International Task Force for Disease Eradication— measles , mumps , rubella , lymphatic filariasis and cysticercosis . [5]
So far, two diseases have been successfully eradicated—one specifically affecting humans ( smallpox ), and one affecting a wide range of ruminants ( rinderpest ).
Smallpox was the first disease, and so far the only infectious disease of humans, to be eradicated by deliberate intervention. [8] It became the first disease for which there was an effective vaccine in 1798 when Edward Jenner showed the protective effect of inoculation ( vaccination ) of humans with material from cowpox lesions. [9]
Smallpox (variola) occurred in two clinical varieties: variola major , with a mortality rate of up to 40 percent, and variola minor , also known as alastrim , with a mortality rate of less than one percent. The last naturally occurring case of Variola major was diagnosed in October 1975 in Bangladesh . The last naturally occurring case of smallpox ( Variola minor ) was diagnosed on 26 October 1977, on Ali Maow Maalin , in the Merca District, of Somalia . The source of this case was a known outbreak in the nearby district of Kurtuware. All 211 contacts were traced, revaccinated, and kept under surveillance. [10]
After two years' detailed analysis of national records, the global eradication of smallpox was certified by an international commission of smallpox clinicians and medical scientists on 9 December 1979, and endorsed by the General Assembly of the World Health Organization on 8 May 1980. [8] However, there is an ongoing debate regarding the continued storage of the smallpox virus by labs in the US and Russia, as any accidental or deliberate release could create a new epidemic in people born since the late 1980s due to the cessation of vaccinations against the smallpox virus.
During the twentieth century, there were a series of campaigns to eradicate rinderpest , a viral disease which infected cattle and other ruminants and belonged to the same family as measles , primarily through the use of a live attenuated vaccine. [11] The final, successful campaign was led by the Food and Agriculture Organization of the United Nations . On 14 October 2010, with no diagnoses for nine years, the Food and Agriculture Organization announced that the disease had been completely eradicated, [12] making this the first (and so far the only) disease of livestock to have been eradicated by human undertakings.
A dramatic reduction of the incidence of poliomyelitis in industrialized countries followed the development of a vaccine in the 1950s. In 1960, Czechoslovakia became the first country certified to have eliminated polio.
In 1988, the World Health Organization (WHO), Rotary International , the United Nations Children's Fund (UNICEF), and the United States Centers for Disease Control and Prevention (CDC) passed the Global Polio Eradication Initiative . Its goal was to eradicate polio by the year 2000. The updated strategic plan for 2004–2008 expects to achieve global eradication by interrupting poliovirus transmission, using the strategies of routine immunization , supplementary immunization campaigns, and surveillance of possible outbreaks. The WHO estimates that global savings from eradication, due to forgone treatment and disability costs, could exceed one billion U.S. dollars per year. [22]
The following world regions have been declared polio-free:
The lowest annual wild polio prevalence seen so far was in 2017, with only 22 reported cases, although there were more total reported cases (including circulated vaccine-derived cases) than in 2016, mainly due to reporting of circulated vaccine-derived cases in Syria, where it likely had already been circulating, [25] but gone unreported, presumably due to the civil war. Only two or three countries remain in which poliovirus transmission may never have been interrupted: Pakistan , Afghanistan , and perhaps Nigeria . [26] [27] (There have been no cases caused by wild strains of poliovirus in Nigeria since August 2016, [28] though cVDPV2 was detected in environmental samples in 2017. [29] )  Nigeria was removed from the WHO list of polio-endemic countries in September 2015 but added back in 2016, and India was removed in 2014 [30] after no new cases were reported for one year. [31]
On 20 September 2015, the World Health Organization announced that wild poliovirus type 2 had been eradicated worldwide, as it has not been seen since 1999. On 24 October 2019, the World Health Organization announced that wild poliovirus type 3 had also been eradicated worldwide.  This leaves only wild poliovirus type 1 and circulating vaccine-derived polio circulating in a few isolated pockets, with all wild polio cases after August 2016 in Afghanistan and Pakistan.
Dracunculiasis , also called Guinea worm disease, is a painful and disabling parasitic disease caused by the nematode Dracunculus medinensis . It is spread through consumption of drinking water infested with copepods hosting Dracunculus larvae. The Carter Center has led the effort to eradicate the disease, along with the CDC, the WHO, UNICEF, and the Bill and Melinda Gates Foundation .
Unlike diseases such as smallpox and polio, there is no vaccine or drug therapy for guinea worm. Eradication efforts have been based on making drinking water supplies safer (e.g. by provision of borehole wells, or through treating the water with larvicide), on containment of infection and on education for safe drinking water practices. These strategies have produced many successes: two decades of eradication efforts have reduced Guinea worm's global incidence to 22 cases in 2015, after which cases rose to 25 cases in 2016, and 30 cases in 2017, but this is still down from an estimated 3.5 million in 1986.  Success has been slower than was hoped—the original goal for eradication was 1995. The WHO has certified 180 countries free of the disease, and only three countries— South Sudan , Ethiopia , and Chad —reported cases of guinea worm in 2016, [33] [37] [38] and only two— Ethiopia and Chad —in 2017. As of 2010 [update] , the WHO predicted it would be "a few years yet" before eradication is achieved, on the basis that it took 6–12 years for the countries that have so far eliminated guinea worm transmission to do so after reporting a similar number of cases to that reported by Sudan in 2009. [39] The number of cases in 2019 (54) was less than 2% of the number in 2009, so real progress has been made towards this prediction. Nonetheless, the last 1% may be the hardest, [37] and cases have increased from 2015 (22) to 2019 (54).  The worm is able to infect dogs , domestic cats and baboons as well as humans, complicating eradication efforts. [40]
Yaws is a rarely fatal but highly disfiguring disease caused by the spiral-shaped bacterium ( spirochete ) Treponema pallidum pertenue , a close relative of the syphilis bacterium Treponema pallidum pallidum , spread through skin to skin contact with infectious lesions.  The global prevalence of this disease and the other endemic treponematoses, bejel and pinta , was reduced by the Global Control of Treponematoses (TCP) programme between 1952 and 1964 from about 50 million cases to about 2.5 million (a 95% reduction).  However, following the cessation of this program these diseases remained at a low prevalence in parts of Asia, Africa and the Americas with sporadic outbreaks. According to a 2012 official WHO roadmap, the elimination should be achievable by 2020. [41] [42] Yaws is currently targeted by the South-East Asian Regional Office of the WHO for elimination from the remaining endemic countries in this region ( India , Indonesia and East Timor ) by 2010, and so far, this appears to have met with some success, since no cases have been seen in India since 2004. [43] [44] The discovery that oral antibiotic azithromycin can be used instead of the previous standard, injected penicillin , was tested on Lihir Island from 2013 to 2014; [45] a single oral dose of the macrolide antibiotic reduced disease prevalence from 2.4% to 0.3% at 12 months. [46] The campaign was in an early stage in 2013, still gathering data on disease incidence and planning initial large-scale treatment campaigns in Cameroon, Ghana, Indonesia, Papua New Guinea, the Solomon Islands, and Vanuatu. [47]
Malaria has been eliminated from most of Europe , North America , Australia , North Africa and the Caribbean , and parts of South America , Asia and Southern Africa . [48] The WHO defines elimination as having no domestic transmission for the past three years. They also define an "elimination stage" when a country is on the verge of eliminating malaria, as being less than one case per 1000 people at risk per year. As of 2019 [update] , 38 countries are certified as having eliminated malaria. [49] As of 2018, 21 countries were seeking to eliminate malaria by 2020. [50] The pre-elimination stage entails fewer than 5 cases per 1000 people at risk per year.
In 1955 the WHO launched the Global Malaria Eradication Program (GMEP). Support waned, and the program was suspended in 1969. [51] Since 2000, support for eradication has increased, although some people in the global health community remain sceptical. [52] According to the WHO's World Malaria Report 2015, the global mortality rate for malaria fell by 60% between 2000 and 2015. The WHO aims to achieve a further 90% reduction between 2015 and 2030. [53] Bill Gates believes that global eradication is possible by 2040. [54]
A major challenge to malaria elimination is the persistence of malaria in border regions, making international cooperation crucial. [55]
Some diseases have already been eliminated from large regions of the world, and/or are currently being targeted for regional elimination.  This is sometimes described as "eradication", although technically the term only applies when this is achieved on a global scale. [56] Even after regional elimination is successful, interventions often need to continue to prevent a disease becoming re-established.  Three of the diseases here listed (lymphatic filariasis, measles, and rubella) are among the diseases believed to be potentially eradicable by the International Task Force for Disease Eradication, and if successful, regional elimination programs may yet prove a stepping stone to later global eradication programs. This section does not cover elimination where it is used to mean control programs sufficiently tight to reduce the burden of an infectious disease or other health problem to a level where they may be deemed to have little impact on public health , such as the leprosy , neonatal tetanus , or obstetric fistula campaigns.
In North American countries, such as the United States , elimination of hookworm had been attained due to scientific advances. Despite the United States declaring that it had eliminated hookworm decades ago, a 2017 study showed it was present in Lowndes County, Alabama . [57]
The Rockefeller Foundation 's hookworm campaign in the 1920s was supposed to focus on the eradication of hookworm infections for those living in Mexico and other rural areas. However, the campaign was politically influenced, causing it to be less successful, and regions such as Mexico still deal with these infections from parasitic worms . This use of health campaigns by political leaders for political and economic advantages has been termed the science-politics paradox. [58]
Lymphatic filariasis is an infection of the lymph system by mosquito-borne microfilarial worms which can cause elephantiasis .  Studies have demonstrated that transmission of the infection can be broken when a single dose of combined oral medicines is consistently maintained annually for approximately seven years. [59] The strategy for eliminating transmission of lymphatic filariasis is mass distribution of medicines that kill the microfilariae and stop transmission of the parasite by mosquitoes in endemic communities. [59] In sub-Saharan Africa , albendazole is being used with ivermectin to treat the disease, whereas elsewhere in the world albendazole is used with diethylcarbamazine . [60] Using a combination of treatments better reduces the number of microfilariae in blood. [59] Avoiding mosquito bites, such as by using insecticide -treated mosquito bed nets , also reduces the transmission of lymphatic filariasis. [59] [61] In the Americas, 95% of the burden of lymphatic filariasis is on the island of Hispaniola (comprising Haiti and the Dominican Republic). An elimination effort to address this is currently under way alongside the malaria effort described above; both countries intend to eliminate the disease by 2020. [62]
As of October 2008 [update] , the efforts of the Global Programme to Eliminate LF are estimated to have already prevented 6.6 million new filariasis cases from developing in children, and to have stopped the progression of the disease in another 9.5 million people who have already contracted it. Overall, of 83 endemic countries, mass treatment has been rolled out in 48, and elimination of transmission reportedly achieved in 21. [63]
Five out of six WHO regions have goals to eliminate measles , and at the 63rd World Health Assembly in May 2010, delegates agreed to move towards eventual eradication, although no specific global target date has yet been agreed. [64] [65] [66] The Americas set a goal in 1994 to eliminate measles and rubella transmission by 2000, and successfully achieved regional measles elimination in 2002, although there have been occasional small outbreaks from imported cases since then. [67] Europe had set a goal to eliminate measles transmission by 2010, but were hindered by the MMR vaccine controversy and by low uptake in certain groups, [ which? ] and despite achieving low levels by 2008, European countries have since experienced a small resurgence in cases.  They have set a new target of 2015. [68] [ needs update ] The Eastern Mediterranean also had goals to eliminate measles by 2010 (later revised to 2015), the Western Pacific aims to eliminate the disease by 2012, and in 2009 the regional committee for Africa agreed a goal of measles elimination by 2020.  As of May 2010 [update] , only the South-East Asian region has yet to set a target date for elimination of measles transmission. [64]
In 2005, a global target was agreed for a 90% reduction in measles deaths by 2010 from the 757,000 deaths in 2000; estimates for 2008 show a 78% decline so far to 164,000 deaths. [69] However, some have been pushing to attempt global eradication. [70] This was updated at the 2010 World Health Assembly to a targeted 95% reduction in mortality by 2015, alongside specific vaccination and structural targets, [64] [65] and in a meeting in November 2010, the Strategic Advisory Group of Experts on Immunization "concluded that measles can and should be eradicated". [71] A study of the costs of eradicating measles compared to the costs of maintaining indefinite control was commissioned in 2009 by the WHO and the Bill and Melinda Gates Foundation . [72] In 2013, measles deaths globally were down to 145,700. [73]
As of mid-2013, measles elimination in many areas is stalling.  "This year, measles and rubella outbreaks are occurring in many areas of the world where people have no immunity to these viruses.  The reasons people are unvaccinated range from lack of access to vaccines in areas of insecurity, to poor performing health systems, to vaccine refusals.  We need to address each of these challenges if we’re going to meet global measles and rubella elimination goals," said Dr. Myrna Charles of the American Red Cross , as reported in a post in the Measles and Rubella Initiative's blog. [74] A look at the WHO's epidemiological graph of measles over time from 2008-2013 show that, with little more of two years to go to 2015, measles cases in 2013 are moving in the wrong direction, with more cases this year than at the same point in 2012 or 2011. [75]
During 2014 there were 23 outbreaks of measles in the United States and over 600 individual cases, which is the highest seen in decades.  In 2015 the US has had one major outbreak of measles originating from an amusement park in California of a variant of the virus circulating in the Philippines in 2014.  From this there have been 113 individual measles cases and one death (out of the total of 189 cases in the US in 2015). [76] [77]
The WHO region of the Americas declared on 27 September 2016 it had eliminated measles. [78] The last confirmed endemic case of measles in the Americas was in Brazil in July 2015. [79] May 2017 saw a return of measles to the US after an outbreak in Minnesota among unvaccinated children. [80] Another outbreak occurred in the state of New York between 2018 and 2019, causing over 200 confirmed measles cases in mostly ultra-Orthodox Jewish communities. [81] [82] Subsequent outbreaks occurred in New Jersey and Washington state with over 30 cases reported in the Pacific Northwest. [83] [84]
Four out of six WHO regions have goals to eliminate rubella, with the WHO recommending using existing measles programmes for vaccination with combined vaccines such as the MMR vaccine . The number of reported cases dropped from 670 thousand in the year 2000 to below 15 thousand in 2018, and the global coverage of rubella vaccination was estimated at 69% in 2018 by the WHO. [85] The WHO region of the Americas declared on 29 April 2015 it had eliminated rubella and congenital rubella syndrome. [86] The last confirmed endemic case of rubella in the Americas was in Argentina in February 2009. [86] [87] Australia achieved eradication in 2018. [88] The WHO European region missed its elimination target of 2010 due to undervaccination in Central and Western Europe; it has set a new goal of 2015. [89] [ needs update ] The disease remains problematic in other regions; the WHO regions of Africa and South-East Asia have the highest rates of congenital rubella syndrome [85] and a 2013 outbreak of rubella in Japan resulted in 15,000 cases. [86]
Onchocerciasis (river blindness) is the world's second leading cause of infectious blindness .  It is caused by the nematode Onchocerca volvulus , which is transmitted to people via the bite of a black fly .  Elimination of this disease is under way in the region of the Americas, where this disease was endemic to Brazil , Colombia , Ecuador , Guatemala , Mexico and Venezuela .  The principal tool being used is mass ivermectin treatment. If successful, the only remaining endemic locations would be in Africa and Yemen . [90] In Africa, it is estimated that greater than 102 million people in 19 countries are at high risk of onchocerciasis infection, and in 2008, 56.7 million people in 15 of these countries received community-directed treatment with ivermectin.  Since adopting such treatment measures in 1997, the African Programme for Onchocerciasis Control reports a reduction in the prevalence of onchocerciasis in the countries under its mandate from a pre-intervention level of 46.5% in 1995 to 28.5% in 2008. [91] Some African countries, such as Uganda , [92] are also attempting elimination and successful elimination was reported in 2009 from two endemic foci in Mali and Senegal . [93]
On 29 July 2013, the Pan American Health Organization (PAHO) announced that after 16 years of efforts, Colombia had become the first country in the world to eliminate the parasitic disease onchocerciasis. [94] It has also been eliminated in Ecuador (2014), Mexico (2015), and Guatemala (2016). [95]
Following an epidemic of variant Creutzfeldt–Jakob disease (vCJD) in the UK in the 1990s, there have been campaigns to eliminate bovine spongiform encephalopathy (BSE) in cattle across the European Union and beyond which have achieved large reductions in the number of cattle with this disease. [96] Cases of vCJD have also fallen since then, from an annual peak of 29 cases in 2000 to five in 2008 and none in 2012. Two cases were reported in both 2013 and 2014: two in France; one in the United Kingdom and one in the United States. [97] [98]
Following the ongoing eradication effort, only seven cases of BSE were reported worldwide in 2013: three in the United Kingdom, two in France, one in Ireland and one in Poland. This is the lowest number of cases since at least 1988. [99] [100] In 2015 there were at least six reported cases (three of the atypical H-type [ clarification needed ] ). [101]
In 2015, Cuba became the first country in the world to eliminate mother-to-child syphilis . [102] In 2017 the WHO declared that Antigua and Barbuda , Saint Kitts and Nevis and four British Overseas Territories — Anguilla , Bermuda , Cayman Islands , and Montserrat —have been certified that they have ended transmission of mother-to-child syphilis and HIV . Nevertheless eradication of syphilis by all transmission methods remains unresolved and many questions about the eradication effort remain to be answered.
Early planning by the WHO for the eradication of African trypanosomiasis , also known as sleeping sickness , is underway as the rate of reported cases continues to decline and passive treatment is continued. The WHO aims to completely eliminate transmission of the Trypanosoma brucei gambiense parasite by 2030, though it acknowledges that this goal "leaves no room for complacency." [103] However expert opinion is that eradication - no infections worldwide - is impossible due to continuing zoonosis [F 1] and inherent ineffectiveness of vaccines, [F 2] and the goal is instead elimination - no infections in an area - with even that being ambitious. [F 3]
Because the rabies virus is almost always caught from animals, rabies eradication has focused on reducing the population of wild and stray animals, controls and compulsory quarantine on animals entering the country, and vaccination of pets and wild animals. Many island nations , including Iceland , Ireland , Japan , Malta , and the United Kingdom , managed to eliminate rabies during the twentieth century, [104] and more recently much of continental Europe has been declared rabies-free. [105]
Chagas disease is caused by Trypanosoma cruzi and is endemic to South America . Eradication is hampered by the large chronically infected population in the endemic area in humans, [F 4] and animals, [F 1] and emigration of infected humans to non-endemic areas. [F 5] Production of effective vaccines is inherently difficult. [F 6]
As far as animal diseases are concerned, now that rinderpest has been stamped out, many experts believe peste des petits ruminants (PPR) is the next disease amenable to global eradication. Also known as goat plague or ovine rinderpest , PPR is a highly contagious viral disease of goats and sheep characterized by fever, painful sores in the mouth, tongue and feet, diarrhea, pneumonia and death, especially in young animals. It is caused by a virus of the genus Morbillivirus that is related to rinderpest, measles and canine distemper . [106]
Public upheaval by means of war, famine, political means, and infrastructure destruction can disrupt or eliminate eradication efforts altogether. [107]
Essence is an abbreviation/ acronym for the United States Department of Defense 's Electronic Surveillance System for the Early Notification of Community-based Epidemics . Essence's goal is to monitor health data as it becomes available and discover epidemics and similar health concerns before they get out of control. [1] The program was created and developed in 1999 by Dr. Michael Lewis, MD , MPH, when he was a resident in the Preventive Medicine residency training program at the Walter Reed Army Institute of Research in Silver Spring, Maryland. [2]
Though the program was originally intended for early detection of bioterrorism attacks in the Washington, DC, area in the wake of the September 11 attacks , the U.S. Army Surgeon General , LTG James Peake, MD, ordered Jay Mansfield, the information technology specialist responsible for the IT development of ESSENCE, to expand ESSENCE to look globally at the entire DoD Military Healthcare System as designed. Subsequently, ESSENCE has been adopted and adapted by the Centers for Disease Control and Prevention , Johns Hopkins University , and numerous health departments around the United States and other countries.

This United States government –related article is a stub . You can help Wikipedia by expanding it .
This article about a medical organization or association is a stub . You can help Wikipedia by expanding it .
The European Programme for Intervention Epidemiology Training (EPIET) Fellowship provides training and practical experience in intervention epidemiology at the national centres for surveillance and control of communicable diseases in the European Union . [1] The fellowship is aimed at EU medical practitioners, public-health nurses, microbiologists, veterinarians and other health professionals with previous experience in public health and a keen interest in epidemiology.
The aims of the programme are: [2]
The EPIET Fellowship lasts two years. Ten percent of this time is taken up by formal training courses and the remainder by a placement at a training site in a European country. The fellowship starts with a three-week introductory course in infectious disease epidemiology . This course provides basic knowledge of intervention epidemiology, including outbreak investigation, 
surveillance and applied research.
Following the introductory course, fellows spend 23 months at a training site in an EU member state , Norway , Switzerland , the WHO or at the European Centre for Disease Prevention and Control (ECDC). During the training period, fellows will:
In addition to the introductory course, 4-5 one-week modules are organised throughout the fellowship. The modules focus on one or several specific public health topics, such as: computer tools in outbreak investigations; multivariable regression; time series analysis; vaccinology ; laboratory methods for epidemiologists.
The Fellowship is funded by the European Centre for Disease Prevention and Control (ECDC) and the EU member states . The ECDC took over the coordination of the programme on November 1, 2007, as the European Commission funded project components ended in 2007.
Evolutionary epidemiology consists in simultaneously analysing how parasites spread and evolve. [1]
Mortality displacement is a phenomenon where a period of excess deaths (i.e., more deaths than expected) is followed by a period of mortality deficit (i.e., fewer deaths than expected). It is also known as " harvesting ". [1] [2] It is usually attributable to environmental phenomena such as heat waves , cold spells , epidemics and pandemics , especially influenza pandemics , famine or war .
During heat waves, for instance, there are often additional deaths observed in the population, affecting especially older adults and those who are sick.  After some periods with excess mortality, however, there has also been observed a decrease in overall mortality during the subsequent weeks. Such short-term forward shift in mortality rate is also referred to as harvesting effect . The subsequent, compensatory reduction in mortality suggests that the heat wave especially affected those whose health was already so compromised that they "would have died in the short-term anyway". [3]
Different institutions and initiatives offer weekly data to monitor excess mortality. Significant efforts to capture short term mortality data have been made along 2020 due to the pandemic of the Coronavirus disease 2019 (COVID-19) and its worldwide effects . Eurostat launched in April 2020 a collection of weekly death data that provide for most of the EU countries weekly death data series by 5-year age groups and sex in NUTS3 regions within the countries starting from the year 2000. [4] This temporary data collection was established in order to support the policy and research efforts related to the Covid-19 pandemic. Data are transmitted by the National Statistical Institutes on voluntary basis and it is being updated, depending on the country, weekly. [5]
The Human Mortality Database project launched in May 2020 a new data series, the Short-term Mortality Fluctuation series (STMF) , offering freely available weekly death counts by age and sex for a growing number of countries (34 in October 2020), as well as a visualization tool that captures the excess mortality in a weekly basis. The STMF was established to provide data for scientific analysis of all-cause mortality fluctuations by week within each calendar year in standard formats. As part of the HMD project, is a joint project of two teams based in the Laboratory of Demographic Data at the Max Planck Institute for Demographic Research (MPIDR) and at the Department of Demography of the University of California, Berkeley (UCB).
The collaborative network EuroMOMO (European mortality monitoring activity), monitors mortality across 24 European countries in order to detect and measure excess deaths related to seasonal influenza, pandemics and other public health threats. EuroMOMO is hosted and maintained by the Department of Infectious Disease Epidemiology and Prevention of Copenhagen , Denmark . They offer regular reports (weekly bulletins), graphs and maps showing the present levels of mortality but the network does not publish openly data. Individual partners may decide to share openly some selected national data, like for instance, MoMo-Spain . The study centre at the Statens Serum Institut in Copenhagen publishes a weekly situation report and regular scientific articles. Periods of high excess mortality have also been described for the United States. [6]
In epidemiology and biostatistics , the experimental event rate (EER) is a measure of how often a particular statistical event (such as response to a drug, adverse event or death) occurs within the experimental group (non-control group) of an experiment. [1]
This value is very useful in determining the therapeutic benefit or risk to patients in experimental groups, in comparison to patients in placebo or traditionally treated control groups.
Three statistical terms rely on EER for their calculation: absolute risk reduction , relative risk reduction and number needed to treat .
The control event rate (CER) is identical to the experimental event rate except that is measured within the scientific control group of an experiment. [2]
In a trial of hypothetical drug "X" where we are measuring event "Z", we have two groups. Our control group (25 people) is given a placebo , and the experimental group (25 people) is given drug "X".
Event "Z" in control group : 4 in 25 people Control event rate : 4/25
Event "Z" in experimental group : 12 in 25 people
Experimental event rate : 12/25
Another worked example is as follows:



This statistics -related article is a stub . You can help Wikipedia by expanding it .
Farr's laws is a law formulated by Dr. William Farr when he made the observation that epidemic events rise and fall in a roughly symmetrical pattern. The time-evolution behavior could be captured by a single mathematical formula that could be approximated by a bell-shaped curve . [1]
In 1840, Farr submitted a letter to the Annual Report of the Registrar General of Births, Deaths and Marriages in England . In that letter, he applied mathematics to the records of deaths during a recent smallpox epidemic, proposing that:
"If the latent cause of epidemics cannot be discovered, the mode in which it operates may be investigated. The laws of its action may be determined by observation, as well as the circumstances in which epidemics arise, or by which they may be controlled." [2]
He showed that during the smallpox epidemic, a plot of the number of deaths per quarter followed a roughly bell-shaped or " normal curve ", [3] [4] and that recent epidemics of other diseases had followed a similar pattern. [5]
This virus -related article is a stub . You can help Wikipedia by expanding it .
This medical article is a stub . You can help Wikipedia by expanding it .
The fetal origins hypothesis (differentiated from the developmental origins of health and disease hypothesis, which emphasizes environmental conditions both before and immediately after birth) proposes that the period of gestation has significant impacts on the developmental health and wellbeing outcomes for an individual ranging from infancy to adulthood. The effects of fetal origin are marked by three characteristics: latency, wherein effects may not be apparent until much later in life; persistency, whereby conditions resulting from a fetal effect continue to exist for a given individual; and genetic programming, which describes the 'switching on' of a specific gene due to prenatal environment. [1] Research in the areas of economics, epidemiology, and epigenetics offer support for the hypothesis. [2]
The fetus was once believed to be a "perfect parasite", [3] immune to harmful environmental toxins passed from the mother via the placenta.  Stemming from this belief, pregnant women of the early to mid 20th century freely drank alcohol, ingested medications, smoked cigarettes, and were largely ignorant of any nutritional needs for a developing fetus.  This easy going attitude about pregnancy was challenged, however, by findings relating substances ingested by a mother to tragic outcomes for a fetus.  The birth defects crisis due to the medication thalidomide in the 1960s, where thousands of children were born with defects ranging from brain damage to truncated and missing arms and legs is an example of how a seemingly miracle medication supposed to prevent morning sickness instead had disastrous consequences. [4] Similarly, in 1971, a drug known as DES, diethylstilbestrol , when taken by pregnant women, was found to be causing an incredibly rare vaginal cancer known as clear-cell adenocarcinoma in young girls when the cancer was traditionally only found to affect those of post-menopausal age. [2] This finding, in particular, demonstrates that events occurring during gestation are capable of impacting future health into adulthood.  As perhaps the most well-known fetal risk, It wasn't until 1973 that fetal alcohol syndrome was first formally diagnosed, and not until 1989 that the United States government began requiring warning labels directed at pregnant women to be in place on all alcoholic beverages for sale. [2] While the risks associated with certain substances have been well documented during pregnancy, the fetal origins hypothesis goes beyond medical substances to expand upon the effects of maternal stress, obesity, influenza, nutrition, and pollution on a developing fetus. [2]
Epidemiologist David Barker was the earliest proponent of the theory of fetal origins of adult disease, prompting the theory to be denoted as "Barker's hypothesis". In 1986, Barker published findings proposing a direct link between prenatal nutrition and late-onset coronary heart disease. [5] He had noticed that the poorest areas of England were the same areas with the highest rates of heart disease, unearthing the predictive relationship between low birth weight and adult disease.  His findings were met with criticism, mainly because at the time heart disease was considered to be predominantly determined by lifestyle and genetic factors. Since Barker's initial findings, the results have been replicated in diverse populations of Europe, Asia, North American, Africa, and Australia. [2] In explanation of such findings, Barker suggests that fetuses learn to adapt to the environment they expect to enter into once outside of the womb.  Essentially, all transmissions entering the placenta act as "postcards" giving the fetus clues as to the outside world, preparing its physiology appropriately. [2] This can be an adaptive mechanism, when fetal conditions accurately represent the world of birth; alternatively, it can be a harmful mechanism, when fetal conditions of plenitude or scarcity do not match the world of birth and the child has been physiologically predisposed to inhabit an environment where expected resources are drastically different from reality.
The thrifty phenotype hypothesis proposes that a low availability of nutrients during the prenatal stage followed by an improvement in nutritional availability in early childhood causes an increase risk of metabolic disorders, including Type II diabetes, as a result of permanent changes in the metabolic processing of glucose-insulin determined in utero. [6] This predominantly affects poor communities, where maternal malnutrition may be rampant, in turn causing fetuses to be biologically programmed to expect sparse nutritional environments.  But, once in the world, the readily accessible processed foods consumed are unable to be processed efficiently by individuals who had their metabolic systems pre-set to expect scarcity.  This difference between expected nutritional deficits and actual food surplus results in obesity and eventually Type II Diabetes. [7] Janet Rich-Edwards, an epidemiologist at Harvard Medical School, initially set out to disprove the fetal origins theory with her database of over 100,000 nurses. Instead, she found that the results hold: a strong relationship exists between low birth weight and later coronary heart disease and stroke. [8]
Pregnancy outcomes can impact the wellbeing of a society.  Comparisons between the children who were in gestation during the 1918 flu pandemic and those in gestation immediately before or after the health crisis show marked differences between the two groups on census data.  Across all socioeconomic measures, those who were fetuses during the crisis attained lower educational achievement, income, and socioeconomic status.  Specifically, individuals affected were 15% less likely to graduate high school, 15% more likely to be poor, and 20% more likely to be disabled as adults.  Even federal welfare payments were higher for the gestational cohort than those born before or after the flu hit. [2] The same economic researcher, Douglas Almond, has investigated other historical situations affecting particular cohorts of fetuses: children born during or immediately following the Chernobyl nuclear disaster explosion, and China's Great Leap Forward (which resulted in a deadly famine).  Both prenatally exposed groups suffered lower cognitive abilities and reduced employment levels. [2] Such outcomes can have lasting impacts on the productivity and economic security of a society for an entire generation of individuals, and perhaps even continue to affect future descendants through changes in gene expression.
Epigenetics refers to the study of the behavior of genes, and how gene expression can be altered by the environment without changes made in DNA.  This is believed to be particularly possible during prenatal development, and both stress and diet have been known to causes changes to a fetus. [9] Findings linking maternal exposure to pollution with poor health outcomes for children are possibly linked to the altering of gene expression. [2] Additionally, studies focusing on maternal weight show gene altering may be occurring.  Women who are overweight at the time of pregnancy have children that are more likely to be overweight themselves.  This could be due to the genetic heritability of genes related to obesity.  But, siblings born to these same women after they had weight reduction surgery were no more likely to be overweight than the rest of the general population.  The metabolic nature of the children was completely different, despite being born to the same mother, supporting the idea that the gestational environment strongly influences future outcomes. [2] In discussing the epigenetics findings of fetal origins, Princeton University's Janet Currie says, "The long-vaunted distinction between nature and nurture is therefore outdated and unhelpful.  Poor nurture during pregnancy can worsen the hand that nature has dealt." [10]
Epidemiological research, or the study of the health and disease patterns of certain populations, allow for controls not possible in other research avenues.  When a significant situation, disaster, or event occurs across a given population, it can be assumed that the entire population is affected, thus generalizing findings across all demographics in a given group. Certain historical events provide epidemiological support for the developmental origins of health and disease, including the Dutch Hunger Winter and the Holocaust.
During the 1918 flu pandemic, an estimated 20% of the world’s population became infected and 50 million of those infections proved to be fatal. In the United States, the average lifespan dropped by 12 years per person. [11] The disease struck indiscriminately by class but was often fatal for those that were in their 20s and 30s while having a particularly strong effect on pregnant women, infecting one third of all American women that were pregnant between 1918 and 1919. [1] Children who were born in 1919 and had mothers who were infected during gestation experienced many handicaps later in life. Those born in 1919 experienced a 5% or more wage drop and were often found to have lower educational achievement overall. These children were also 20% more likely to be disabled than other comparable cohorts (early 1918 and late 1919) who did not experience in utero exposure. [1] In a study conducted in 2008 it was found that in utero exposure to the pandemic led to higher chances of developing coronary heart disease and kidney disease later in life. The study concluded an 11.8% increased chance of coronary heart disease for those born in the first quarter of 1919. and a 51% increased chance of developing kidney disease for those born in the fourth quarter in 1918 as compared to those born in early 1918 and late 1919. It is also notable that those who were already born but young (between the ages of 1 and 5) during exposure did not have a noticeable increase in coronary heart disease or kidney disease. [12] In Italy, one of the countries most affected by the pandemic, there was a drop in educational attainment for those in utero during exposure to the pandemic. Being exposed to the pandemic while in utero would lead to an average loss of 0.3–0.4 years of schooling. These effects were much higher or lower depending on the district of Italy. [13] The possibility that maternal exposure to influenza during gestation may be linked to increased rates of schizophrenia later in life for the child. In a recent [ when? ] study conducted in California they were able to predict schizophrenia in adult offspring by analyzing the influenza antibodies of pregnant women in 1959–1966. It has been hypothesized that a definite link exists between influenza-induced stress on the fetus and schizophrenia. [14]
During World War II, a Nazi barricade resulted in a severe famine in the Western Netherlands.  Where food was previously plentiful, supplies immediately were cut off in November 1944, resulting in a period of starvation that lasted until spring of 1945.  The Dutch people survived on as little as 30% of their daily needed caloric intake, and tens of thousands of people died. Analyses of the orderly health records from this time period allow for a systematic comparison of the effects of fetal starvation. Individuals who were in utero during the Hunger Winter were subject to different outcomes depending on the period of time in which they were conceived.  Those who were in the first trimester during the three-month siege were likely to be born normal size, having caught up with typical development.  However, these normal size babies developed high blood pressure, diabetes, and obesity.  Contrary to this group, those who were in the third trimester during the siege, who presumably had been well nourished up until the last few months of gestation, were born small.  But, these small babies stayed small their entire lives, and did not develop higher rates of obesity or disease. Surprisingly, effects continued to be seen in the offspring of the individuals who were fetuses at the time of the famine. [15]
During Ramadan (of the lunar cycle) many Muslims around the world participate in a fast during the daylight hours of the lunar month. This fasting usually entails abstaining from food or drink for the daylight hours of the month.   There are groups that are automatically exempt from having to participate such as the young, sick and old but the list of exemption does not officially include pregnant women (though they are most often allowed exemption). The majority of pregnant women however, choose to participate despite the hardship due to cultural and personal pressure. [16] In several recent studies on the effects of fasting during Ramadan and Fetal Origins Hypothesis they have found many negative outcomes on children who were in utero during the fast. These outcomes were as numerous as a change in birth weight to the long term health of the affected. The studies were conducted primarily in Uganda and Iraq but had some smaller sections in Michigan and other places for control groups or specific studies. The effects on birth weight are negatively correlated with Ramadan fasting. Arab Muslim pregnancies that overlap with the Ramadan fast experienced a lower birth weight of 18 grams per child. The effect was slightly larger at a lower birth weight of 20-25 grams if Ramadan fell somewhere in the first or second trimester of the pregnancy. [16] In utero exposure to Ramadan fasting has a negative effect on male birth rate causing a skewed sex ratio for total births. When exposure to the Ramadan fast takes place a month after conception it is correlated with a 13% decline in total births. The effects on exposed males and females is drastically different where the male birth rate drops by 26% the female birth rate only drops by 2.5% leading to the assumption that “male vulnerability” may be to blame. [16] In a study conducted in Uganda and Iraq on the levels of disability among those exposed to the fast while in utero they concluded that disability rates were much higher for those exposed when controlling for outside factors. Though the measure for disability differs by country the effect is still noticeable. For those born 9 months after Ramadan the likelihood of disability is higher than the surrounding population. The mean rate of disability in Uganda is 3.8% for the country but for those exposed the number is drastically higher at 22% mean disability rate. A similar effect can be observed in Iraq where the mean rate of disability is 1.5% but the disability rate of those exposed is 23%. [16] In Uganda the recorded number of blind and deaf can be specifically recorded allowing one to see the effect on this specific disability to expose. Those born 9 months after Ramadan were 33% more likely to be blind and 64% more likely to be deaf than those not exposed in utero. The effects of exposure to the Ramadan fast can even be observed in mental disorders. In a study conducted in Uganda it was concluded that exposure to the fast, early in a pregnancy effectively doubles the likelihood of a person having a cognitive disorder of some kind. A similar discovery was made in Iraq where 63% higher likelihood of a cognitive disorder relative to the mean was discovered for all those exposed. [16] Certain specific health effects have been observed for those exposed to in utero fasting. The reported signs of Anemia among the old were higher for those exposed during mid gestation, all other points in the gestation period were found to be insignificant. Anemia is caused by damage to the kidneys so the findings are consistent that the effect is noticeable during mid gestation when the kidneys are being developed. [17]
The offspring of Holocaust survivors have been found to have an epigenetic 'tag' change in their DNA similar to those of their parents, individuals affected directly by the Holocaust.  This finding shows that gene expressions can be altered via stressful experiences and then passed down to children through prenatal conditions.  While the children of the Holocaust survivors had not themselves experienced Nazi inflicted trauma, they experienced the physiological and emotional trauma as if they had.  When compared to Jewish families who were living outside of affected areas of Europe, the findings continued to stand: "The gene changes in the children could only be attributed to Holocaust exposure in the parents.” [18]
Pollution may affect the health of the mother, or cross over the placenta and enter the developing fetus.  Beate Ritz, a professor at UCLA, found significantly higher rates of heart malformations and valve defects in the children born to women living in highly polluted areas of Los Angeles.
Maternal stress has been linked to a number of negative outcomes for the developing fetus.  Pregnant women who firsthand experienced the devastation of the World Trade Center attack on September 11, 2001 were studied to observe the effects of PTSD (post-traumatic stress disorder) on their child's future health.  Of the women studied, those who developed PTSD following the attacks had lower basal cortisol levels than a control group. Their children, also, had lower basal cortisol levels than those not exposed to extreme prenatal stressors. The finding was strongest for the women who were in their third trimester during 9/11.  Based on the findings that there was a trimester distinction in strength, conclusions can be drawn that the development of a vulnerability to stress was due at least in part to environment in utero. [2] 9/11 is also correlated with lower birth weights of children born to women with Arabic sounding names following the attacks; this could possibly be due to fear of retaliation or stereotyping associations with the attackers. [19] Stress has also been linked to preterm birth, as shown by research studies conducted following the Tarapaca, Chile earthquake in 2005, as well as the Northridge, California earthquake in 1994. [20] Similar findings have been replicated for stressful life experiences and fetal outcomes in the Hurricane Katrina population of 2005.  Women in New Orleans at the time who reported enduring multiple severe disaster experiences also had a significantly higher chance of delivering early or low birth weight children. [21] Experiencing loss during pregnancy also influences postnatal outcomes.  Women who experienced the death of a close family member, friend, or spouse, or were pregnant during a wartime conflict, were more likely to have children prematurely, and the children of these women were significantly more likely than the general population to suffer from schizophrenia in adulthood. [19] [22] Besides birth weight, mental health, and reduced cortisol levels, effects of stress during pregnancy have also been linked to impaired cognitive development in children as seen in the maternal population exposed to a severe snowstorm in Canada. [23] Women who experienced the most stressful storm related events had children with detriments in cognitive, language, behavioral, and attention outcomes. [23] Shockingly, the poorer performance by these children has persisted until the age of ten. [23] Even job-related stress has been found to be associated with low birth weight and preterm birth.  Working long hours, having temporarily employment, or reporting physically demanding job tasks showed "significant and strong" associations with poorer later birth outcomes. [24] Findings for the job stress-birth association have been replicated by obstetricians at Cedars-Sinai Medical Center in Los Angeles. [2] However, some research has found that moderate amounts of stress and cortisol passed on to a developing fetus are actually beneficial, perhaps acting to give organs a "workout" prior to birth. [2] Further cementing the theory that maternal emotional state can impact child development are the sound research findings that women who are clinically or slightly depressed during pregnancy are more likely to have children with low birth weight, putting them at risk for future health concerns of their own. [25] On May 12, 2008, a raid took place in Postville, Iowa, where 389 workers at a meat processing factory were arrested and held for questioning. Out of the 389 workers detained, 270 served sentences and most were deported to primarily Mexico and Guatemala. [26] Among those arrested 98% were Latino as they were suspected of being illegal immigrants. Latino families feared future deportations and future raids creating psychological stress on Latinos in the area. In relation to maternal stress a study was conducted which found that Latino babies born 37 weeks after the event experienced a 24% greater risk of lower birth (about 5.5 pounds) weight than babies born in other years. [27] The risk for preterm births was also higher for Latina women when compared to non-Latina White women.
Criticism of the fetal origins hypothesis can be aimed at the limitations of the research.  Confounds abound due to the intertwined nature of environment before and after birth, as well as the correlational factors associated with poverty outcomes. Additionally, the use of historical and longitudinal data raises the question of reliability. [28] Also, some critics maintain that despite the compelling relationship documented between low birth weight and later disease, it is too soon to begin to mandate interventions aimed at increasing birth weight.  Such interventions could instead have increased negative effects, [29] until the specific mechanisms and processes are more deeply understood by which birth and early childhood weight determine development.  As stated in "Killing Me Softly: The Fetal Origins Hypothesis", "Such pre-emptive targeting would constitute a radical departure from current policies that steer nearly all healthcare resources to the sick, i.e. the “pound of cure” approach. That said, the existing evidence is not sufficient to allow us to rank the cost-effectiveness of interventions targeted at women against more traditional interventions targeted at children, adolescents, or adults. For example, broadening the target population to women who might get pregnant would reduce the set of policies which are cost effective." [1]
The implications of the developmental origins of health and disease hypothesis are akin to changing the focus of public health intervention from childhood to in utero.  Because the demonstrated effects range from dramatic to subtle in the wide spread areas of educational achievement, emotional stability, career trajectory, life expectancy, disease prognosis, and psychological disorders, interventions addressing the gestational period could potentially have significant impact on individual and societal levels. Proposed and in effect interventions include the following:
Field Epidemiology is the application of epidemiologic methods to unexpected health problems when a rapid on-site investigation is necessary for timely intervention. [1] A more expansive definition is: The practice of Epidemiology in the field. Work is done in communities often as a public health service and as part of government or a closely allied institution. Field epidemiology is how epidemics and outbreaks are investigated, and is used to implement measures to protect and improve the health of the public. Field epidemiologists must deal with unexpected, sometimes urgent problems that demand immediate solution.  Its methods are designed to answer specific epidemiologic questions in order to plan, implement, and/or evaluate public health interventions.  These studies consider the needs of those who will use the results.  The task of a field epidemiologist is not complete until the results of a study have been clearly communicated in a timely manner to those who need to know, and an intervention made to improve the health of the people. [2]
Field Epidemiology Training Programs ( FETPs ) are two-year applied public health training programs modeled after the U.S. Centers for Disease Control and Prevention 's (CDC) Epidemic Intelligence Service (EIS). FETPs are established within host country ministries of health to enhance the epidemiologic capacity of the public health workforce and increase the use of science and data to appropriately respond to public health threats. By developing the skills of the workers and reinforcing the health systems in which they work, FETPs also help countries to meet their core capacity requirements for surveillance and response under the revised International Health Regulations (IHR, 2005).
The guiding principle of the FETP training model is “learning through doing,” a concept that is analogous to a medical residency (in which physicians acquire on-the-job experience by learning and practicing the necessary skills to become capable clinicians); many FETP programs are however open to a wide range of health professional backgrounds, not only physicians. FETP trainees, or “residents,” spend approximately 25 percent of their time in the classroom, learning the principles of epidemiology, disease surveillance, outbreak investigation, and biostatistics. The other 75 percent of their time is spent in field placements, where residents "learn by doing," by participating in outbreak investigations, helping to establish and evaluate disease surveillance systems, designing and conducting studies on problems of public health concern in their country, and training other healthcare workers.  Field work is typically conducted under the supervision and guidance of an experienced mentor.
FETP residents have been involved in initiatives to prevent and control infectious diseases of global health importance, including polio, cholera, tuberculosis, HIV, malaria, and emerging infectious diseases of animal origin (e.g., SARS, Nipah virus, and avian influenza). Many residents have also worked to reduce the burden of non-communicable diseases , such as heart disease, cancer, and diabetes, or environmental or occupational health problems.
Since launching the Epidemic Intelligence Service by the US Centers for Disease Control and Prevention in 1951, the development of field epidemiology has been promoted internationally and globally. [3] The first FETP outside of the United States was established by Canada in 1975. [4] In 1980, the government of Thailand requested CDC’s assistance to establish its own program, [5] with funding initially contributed by the United States Agency for International Development (USAID). Since then, CDC has helped to establish FETPs in 41 countries worldwide, which have produced more than 2,500 graduates from 61 countries. [6] Over 80% of residents stay in their home countries, and many become leaders within their public health system.
Many of the countries which participate in an FETP collaborate with the Training Programs in Epidemiology and Public Health Interventions Network (TEPHINET) , a global network of Field and Applied Epidemiology Training Programs, to share resources and best practices. Regional FETP networks also exist, including: the African Field Epidemiology Network ( AFENET ), the Eastern Mediterranean Public Health Network ( EMPHNET ), RedSur (the network of Latin American FETPs), and the South Asia Field Epidemiology and Technology Network (SAFETYNET).
The European Centre for Disease Prevention and Control (ECDC) has created the European Programme for Intervention Epidemiology Training (EPIET) in 1995. Its purpose was to create a network of highly trained field epidemiologists in the European Union, thereby strengthening the public health epidemiology workforce at EU Member States and EEA level. Current EPIET alumni are providing expertise in response activities and strengthening capacity for communicable disease surveillance and control inside and beyond the EU. In 2006 EPIET was integrated into the core activities of ECDC. The European Public Health Microbiology Training Programme (EUPHEM) was initiated by ECDC in 2008. The EUPHEM program a unique program. The objective of the public health microbiology path ( EUPHEM ) is to provide state-of-the-art training in public health microbiology enabling its fellows to apply microbiological and epidemiological methods to a wide range of public health problems in Europe.
In 2016, EPIET and EUPHEM became the ECDC Fellowship Programme, consolidating in this way the alignment of administrative processes and core curricular aspects.
EPIET has a very active alumni network (the EPIET Alumni Network ; EAN) that was created in 2000 to help develop and maintain a network of European public health epidemiologists that have participated in the European Programme for Intervention Epidemiology Training (EPIET); it now also includes alumni from the European Programme for Public Health Microbiology Training (EUPHEM) and other European Field Epidemiology Training Programmes (FETP). As well as alumni of training programmes, the EAN also has some ‘external’ members who, through their work, meet similar objectives to the EAN. As the regular member surveys show, "having a network of professionals that know each other, speak the same 'language', and can easily access each other's expertise, represents an important resource for European and global public health, which should be nurtured by encouraging more collaborations devoted to professional development." [7]
Public Health
The five-year survival rate is a type of survival rate for estimating the prognosis of a particular disease, normally calculated from the point of diagnosis. [1] Lead time bias from earlier diagnosis can affect interpretation of the five-year survival rate. [2]
There are absolute and relative survival rates, but the latter are more useful and commonly used.
Five-year relative survival rates are more commonly cited in cancer statistics. [3] Five-year absolute survival rates may sometimes also be cited. [4]
The fact that relative survival rates above 100% were estimated for some groups of patients appears counterintuitive on first view. It is unlikely that occurrence of prostate cancer would increase chances of survival, compared to the general population. A more plausible explanation is that the pattern reflects a selection effect of PSA screening , as screening tests tend to be used less often by socially disadvantaged population groups, who, in general, also have higher mortality. [5]
Five-year survival rates can be used to compare the effectiveness of treatments. Use of five-year survival statistics is more useful in aggressive diseases that have a shorter life expectancy following diagnosis, such as lung cancer , and less useful in cases with a long life expectancy, such as prostate cancer .
Improvements in rates are sometimes attributed to improvements in diagnosis rather than to improvements in prognosis. [6]
To compare treatments independently from diagnostics, it may be better to consider survival from reaching a certain stage of the disease or its treatment.
Analysis performed against the Surveillance, Epidemiology, and End Results database (SEER) facilitates calculation of five-year survival rates. [7] [8]

Flattening the curve is a public health strategy to slow down the spread of the SARS-CoV-2 virus during the COVID-19 pandemic . The curve being flattened is the epidemic curve , a visual representation of the number of infected people needing health care over time. During an epidemic, a health care system can break down when the number of people infected exceeds the capability of the health care system's ability to take care of them. Flattening the curve means slowing the spread of the epidemic so that the peak number of people requiring care at a time is reduced, and the health care system does not exceed its capacity. Flattening the curve relies on mitigation techniques such as hand washing , use of face masks and social distancing .
A complementary measure is to increase health care capacity, to "raise the line". [4] As described in an article in The Nation , "preventing a health care system from being overwhelmed requires a society to do two things: 'flatten the curve'—that is, slow the rate of infection so there aren't too many cases that need hospitalization at one time—and 'raise the line'—that is, boost the hospital system's capacity to treat large numbers of patients." [5] As of April 2020 [update] , in the case of the COVID-19 pandemic, two key measures are to increase the numbers of available ICU beds and ventilators , which are in systemic shortage . [2] [ needs update ]
Warnings about the risk of pandemics were repeatedly made throughout the 2000s and the 2010s by major international organisations including the World Health Organization (WHO) and the World Bank , especially after the 2002–2004 SARS outbreak . [6] Governments, including those in the United States and France , both prior to the 2009 swine flu pandemic , and during the decade following the pandemic, both strengthened their health care capacities and then weakened them. [7] [8] At the time of the COVID-19 pandemic, health care systems in many countries were functioning near their maximum capacities. [4] [ better source needed ]
In a situation like this, when a sizable new epidemic emerges, a portion of infected and symptomatic patients create an increase in the demand for health care that has only been predicted statistically, without the start date of the epidemic nor the infectivity and lethality known in advance. [4] If the demand surpasses the capacity line in the infections per day curve, then the existing health facilities cannot fully handle the patients, resulting in higher death rates than if preparations had been made. [4]
An influential UK study showed that an unmitigated COVID-19 response in the UK could have required up to 46 times the number of available ICU beds. [9] One major public health management challenge is to keep the epidemic wave of incoming patients needing material and human health care resources supplied in a sufficient amount that is considered medically justified. [4]
Non-pharmaceutical interventions such as hand washing , social distancing , isolation and disinfection [4] reduce the daily infections, therefore flattening the epidemic curve. A successfully flattened curve spreads health care needs over time and the peak of hospitalizations under the health care capacity line. [2] Doing so, resources, be it material or human, are not exhausted and lacking. In hospitals, it for medical staff to use the proper protective equipment and procedures, but also to separate contaminated patients and exposed workers from other populations to avoid intra-hospital spread. [4]
Along with the efforts to flatten the curve is the need for a parallel effort to "raise the line", to increase the capacity of the health care system. [2] Healthcare capacity can be raised by raising equipment, staff, providing telemedicine , home care and health education to the public. [4] Elective procedures can be cancelled to free equipment and staffs. [4] Raising the line aims to provide adequate medical equipment and supplies for more patients. [10]
The concept was popular during the early months of the COVID-19 pandemic. [12]
According to Vox , in order to move away from social distancing and return to normal, the US needs to flatten the curve by isolation and mass testing, and to raise the line. [13] Vox encourages building up health care capability including mass testing, software and infrastructures to trace and quarantine infected people, and scaling up cares including by resolving shortages in personal protection equipment , face masks . [13]
According to The Nation , territories with weak finances and health care capacity such as Puerto Rico face an uphill battle to raise the line, and therefore a higher imperative pressure to flatten the curve. [5]
In March 2020, UC Berkeley Economics and Law professor Aaron Edlin commented that ongoing massive efforts to flatten the curve supported by trillions dollars emergency package should be matched by equal efforts to raise the line and increase health care capacity. [14] Edlin called for an activation of the Defense Production Act to order manufacturing companies to produce the needed sanitizers , personal protective equipment , ventilators , and set up hundreds thousands to millions required hospital beds . [14] Standing in March 2020 estimates, Edlin called for the construction of 100-300 emergency hospitals to face what he described as "the largest health catastrophe in 100 years" and to adapt health care legislation preventing emergency practices needed in time of pandemics. [14] Edlin pointed out proposed stimulus package as oriented toward financial panics, while not providing sufficient funding for the core issue of a pandemic: health care capability. [14]
In early May, the senior contributor on healthcare from Forbes posted, " Tenet Healthcare said its more than 60 hospitals are 'not being overwhelmed' by patients sickened by the Coronavirus strain COVID-19, the latest sign the U.S. healthcare system may be effectively coping with the pandemic," suggesting that the goal of flattening the curve to a point below health care capacity had met with initial success. [15]
Focal infection theory is the historical concept that many chronic diseases, including systemic and common ones, are caused by focal infections.  In present medical consensus, a focal infection is a localized infection, often asymptomatic, that causes disease elsewhere in the host, but focal infections are fairly infrequent and limited to fairly uncommon diseases. [1] (Distant injury is focal infection's key principle, whereas in ordinary infectious disease, the infection itself is systemic, as in measles , or the initially infected site is readily identifiable and invasion progresses contiguously, as in gangrene .) [2] [3] Focal infection theory, rather, so explained virtually all diseases, including arthritis, atherosclerosis, cancer, and mental illnesses. [4] [5] [6] [7]
An ancient concept that took modern form around 1900, focal infection theory was widely accepted in medicine by the 1920s. [3] [7] [8] [9] In the theory, the focus of infection might lead to secondary infections at sites particularly susceptible to such microbial species or toxin. [3] Commonly alleged foci were diverse—appendix, urinary bladder, gall bladder, kidney, liver, prostate, and nasal sinuses —but most commonly were oral.  Besides dental decay and infected tonsils , both dental restorations and especially endodontically treated teeth were blamed as foci. [3] [7] The putative oral sepsis was countered by tonsillectomies and tooth extractions, including of endodontically treated teeth and even of apparently healthy teeth, newly popular approaches—sometimes leaving individuals toothless—to treat or prevent diverse diseases. [7]
Drawing severe criticism in the 1930s, focal infection theory—whose popularity zealously exceeded consensus evidence—was discredited in the 1940s by research attacks that drew overwhelming consensus of this sweeping theory's falsity.  Thereupon, dental restorations and endodontic therapy became again favored. [3] [7] Untreated endodontic disease retained mainstream recognition as fostering systemic disease. [3] [7] [10] [11] But only alternative medicine and later biological dentistry continued highlighting sites of dental treatment—still endodontic therapy, but, more recently, also dental implant , and even tooth extraction, too—as foci of infection causing chronic and systemic diseases. [12] In mainstream dentistry and medicine, the primary recognition of focal infection is endocarditis , if oral bacteria enter blood and infect the heart, perhaps its valves . [2]
Entering the 21st century, scientific evidence supporting general relevance of focal infections remained slim, yet evolved understandings of disease mechanisms had established a third possible mechanism—altogether, metastasis of infection, metastatic toxic injury, and, as recently revealed, metastatic immunologic injury—that might occur simultaneously and even interact. [2] [13] Meanwhile, focal infection theory has gained renewed attention, as dental infections apparently are widespread and significant contributors to systemic diseases, although mainstream attention is on ordinary periodontal disease , not on hypotheses of stealth infections via dental treatment . [14] [15] [16] Despite some doubts renewed in the 1990s by conventional dentistry's critics, dentistry scholars maintain that endodontic therapy can be performed without creating focal infections. [3] [7]
In ancient Greece, Hippocrates reported cure of an arthritis case by tooth extraction. [3] Yet modern focal infection theory awaited Robert Koch 's establishment of medical bacteriology in the late 1870s to early 1880s. [6] [17] In 1890, Willoughby D Miller attributed a set of oral diseases to infections, and a set of general diseases—as of lung , stomach, brain abscesses , and other conditions—to those infectious oral diseases. [6] [18] [19] [20] In 1894, Miller became the first to reveal existence of bacteria in samples of dental pulp . [21] [22] Miller advised root canal therapy . [3] [6] Yet focal infection theory met a cultural climate where ancient and folk ideas, long entrenched via Galenic humoral medicine , found new outlets through bacteriology—a pillar of the new "scientific medicine". [23]
Emigrating from Russia in 1886, international scientific celebrity Elie Metchnikoff —discoverer of phagocytes , mediating innate immunity —was embraced in Paris by Louis Pasteur , who granted him an entire floor for research once the Pasteur Institute , the globe's first biomedical institute, opened in 1888. [24] Later the Institute 's director and 1908 Nobelist , Metchnikoff believed, as did his rival Paul Ehrlich —theorist on antibody , mediating acquired immunity —and as did Pasteur, that nutrition influenced immunity. [24] [25] Sharing Pasteur's view of science as a means to suppress the problems plaguing humankind, Metchnikoff brought into France its first cultures of yogurt for probiotic microorganisms to foster health and longevity by suppressing the colon's putrefactive microorganisms alleged to foster the colon 's toxic seepage, autointoxication . [23] [24] [26]
As the 20th century opened, British surgeons were still knife-happy, and called for "surgical bacteriology". [23] Surgical pioneer Sir Arbuthnot Lane , famed for an emergency appendectomy performed on England's royalty, drew from Metchnikoff and clinical observation to issue dire warnings about "chronic intestinal stasis"—that is, constipation—its "flooding of the circulation with filthy material" and causing autointoxication, [23] which Lane then treated with colon bypass and colectomy . [26] [27] In America, alleged bowel sepsis wreaking degeneration and disease had been targeted since 1875 by John Harvey Kellogg in Michigan at his huge Battle Creek Sanitarium —he coined the term sanitarium —yearly receiving several thousand patients, including US Presidents and celebrities, and advertising itself as the "University of Health". [23] When embracing focal infection theory, however, American medical doctors sided against alleged "health faddists" like Kellogg as well as Sylvester Graham , and endorsed the academic tradition of German "scientific medicine". [23]
In 1900, British surgeon William Hunter blamed many disease cases on oral sepsis . [6] [28] [29] In 1910, lecturing in Montreal at McGill University , he claimed, "The worst cases of anemia, gastritis , colitis , obscure fevers, nervous disturbances of all kinds from mental depression to actual lesions of the cord , chronic rheumatic infections , kidney diseases are those which owe their origin to or are gravely complicated by the oral sepsis produced by these gold traps of sepsis." [6] He apparently indicted dental restorations . [18] Incriminating their execution, rather, his American critics lobbied for stricter dental licensing requirements. [6] Still, Hunter's lecture—as later recalled—"ignited the fires of focal infection". [30] Ten years later, he proudly accepted that credit. [8] And yet, read carefully, his lecture asserts a sole cause of the sepsis: dentists who instruct patients to never remove partial dentures. [30] [31]
Focal infection theory's modern era really began with physician Frank Billings , based in Chicago, and his case reports of tonsillectomies and tooth extractions claimed to have cured infections of distant organs. [30] Replacing Hunter's term oral sepsis with focal infection , [7] Billings in November 1911 lectured at the Chicago Medical Society, and published it in 1912 as an article for the American medical community. [32] In 1916, Billings lectured in California at Stanford University Medical School, this time printed in book format. [33] Billings thus popularized intervention by tonsillectomy and tooth extraction. [6] A pupil of Billings, Edward Rosenow held that extraction alone was often insufficient, and urged teamwork by dentistry and medicine. [18] Rosenow developed the principle elective localization , whereby microorganisms have affinities for particular organs, and also espoused extreme pleomorphism . [30] [34] [35]
Since 1889, in American state Minnesota, brothers William Mayo and Charles Mayo had built an international reputation for surgical skill at their Mayo Clinic , by 1906 performing some 5,000 surgeries a year, over 50% intra-abdominal, a tremendous number at the time, with unusually low mortality and morbidity. [23] [36] Though originally distancing themselves from routine medicine and skeptical of laboratory data, they later recruited Rosenow from Chicago to help improve Mayo Clinic's diagnosis and care and to enter basic research via experimental bacteriology. [23] [36] Rosenow influenced Charles Mayo, [23] who by 1914 published to support focal infection theory alongside Billings and Rosenow. [37] [38] [39]
At Johns Hopkins University 's medical school, launched in 1894 as America's first to teach "scientific medicine", the eminent Sir William Osler was succeeded as professor of medicine by Llewellys Barker , [40] who became a prominent proponent of focal infection theory. [23] Although many of Hopkins' medical faculty remained skeptics, Barker's colleague William Thayer [41] cast support. [23] As Hopkins' chief physician, Barker was a pivotal convert propelling the theory to the center of American routine medical practice. [23] Russell Cecil , [42] famed author of Cecil's Essentials of Medicine , too, lent support. [30] In 1921, British surgeon Hunter announced that oral sepsis was "coming of age". [8]
Although physicians had already interpreted pus within a bodily compartment as a systemic threat, pus from infected tooth roots often drained into the mouth and thereby was viewed as systemically inconsequential. [43] Amid focal infection theory, it was concluded that that was often the case—while immune response prevented dissemination from the focus—but that immunity could fail to contain the infection, that dissemination from the focus could ensue, and that systemic disease, often neurological , could result. [43] By 1930, excision of focal infections was considered a "rational form of therapy" undoubtedly resolving many cases of chronic diseases. [5] Its inconsistent effectiveness was attributed to unrecognized foci—perhaps inside internal organs—that the clinicians had missed. [5]
In 1923, upon some 25 years of researches, dentist Weston Andrew Price of Cleveland, Ohio, published a landmark book, [3] [44] then a related article in the Journal of the American Medical Association in 1925. [45] Price concluded that after root canal therapy , teeth routinely host bacteria producing potent toxins. [3] Transplanting the teeth into healthy rabbits, Price and his researchers duplicated heart and arthritic diseases. [3] Although Price noted often seeing patients "suffering more from the inconvenience and difficulties of mastication and nourishment than they did from the lesions from which their physician or dentist had sought to give them relief", [46] his 1925 debate with John P Buckley was decided in favor of Price's position: "practically all infected pulpless teeth should be extracted". [47] As chairman of the American Dental Association 's research section, Price was the individual who, perhaps beyond any other, shaped the dentistry profession's opinion. [48] Textbook authors relied on Price's 1923 treatise into the late 1930s. [49]
Unsuspected periapical disease was first revealed by dental X-ray in 1911, the year that Frank Billings lectured on focal infection to the Chicago Medical Society. [30] Introduced by C Edmund Kells , [50] the technology became used to feed the "mania of extracting devitalized teeth". [51] Even Price was cited as an authoritative source espousing conservative intervention at focal infections. [52] Kells, too, advocated conservative dentistry. [50] Many dentists were "100 percenters", extracting every tooth exhibiting either necrotic pulp or endodontic treatment , and extracted apparently healthy teeth, too, as suspected foci, leaving many persons toothless. [3] [7] A 1926 report published by several authors in Dental Cosmos —a dentistry journal where Willoughby Miller had published in the 1890s—advocated extraction of known healthy teeth to prevent focal infection. [53] Endodontics nearly vanished from American dental education. [3] [7] Some dentists held that root canal therapy should be criminalized and penalized with six months of hard labor . [7]
Besides heredity, focal infection and autointoxication was psychiatry's predominant explanation of schizophrenia near the turn of the 20th century. [54] Henry Andrews Cotton , the director starting in 1907 of the psychiatric asylum at Trenton State Hospital in the American state of New Jersey, [55] identified focal infections as the main causes of dementia praecox (now schizophrenia ) and manic depression (now bipolar disorder ), [55] drawing influence from the medical popularity of focal infection theory. [23] Cotton routinely prescribed surgery to clean the nasal sinuses and to extract the tonsils and dentition . [55] Seeking to clean the entire body of focal infections, Cotton frequently prescribed surgical removal of the appendix, gall bladder, spleen, stomach, colon, cervix, ovaries, testicles, and thereby claimed up to 85% cure rate. [55]
Despite the death rate of some 30%, Cotton's fame rapidly spread through America and Europe, and the asylum drew influx of paying patients. [55] The New York Times praised his accomplishments and heralded "high hope". [55] Cotton made a European lecture tour, [55] and Princeton University Press and Oxford University Press simultaneously published his book in 1922. [56] Despite scepticism within his profession, psychiatrists were under pressure to match Cotton's treatments, as patients would ask why they were being denied successful intervention. [55] Other patients, ostensibly for their own good, were pressured or compelled into treatment without their own consent. [57] Cotton had his two sons' teeth extracted as preventive healthcare—although each later committed suicide. [55] By the 1930s, however, focal infection fell from psychiatry as an explanation. [54]
Addressing the Eastern Medical Society in December 1918, New York City physician Robert Morris had explained that focal infection theory had drawn much interest but that understanding was incomplete, while the theory was earning disrepute through overzealousness of some advocates. [58] Morris called for facts and explanation from scientists before physicians continued investing so steeply in it, already triggering vigorous disputes and embittering division among clinicians as well as uncertainty among patients. [58]
In 1919, in New Orleans, at the annual meeting of the National Dental Association (forerunner of the American Dental Association ), dental X-ray originator and pioneer C Edmund Kells [50] delivered a lecture, published in 1920 in the Association's journal, [59] largely discussing focal infection theory, which Kells condemned as a "crime". [51] Kells stressed that X-ray technology is to improve dentistry, not to enhance the "mania of extracting devitalized teeth". [51] Kells urged dentists to reject physicians' prescriptions of tooth extractions. [60]
Focal infection theory's elegance suggested simple application, but the applications brought meager "cure" rate, occasional disease worsening, and inconsistent experimental results, [6] although the lack of controlled clinical trials , among present criticism, [6] was standard at the time—except in New York City. [55] Around 1920, at Henry Cotton 's claims of up to 85% success treating schizophrenia and manic depression, Cotton's major critic was George Kirby , director of the New York State Psychiatric Institute on Ward's Island . [61] Two researchers, bacteriologist Nicolas Kopeloff and psychiatrist Clarence Cheney, ventured from the New York State Psychiatric Institute to Trenton, New Jersey, to investigate Cotton's practice. [55]
In two controlled clinical trials with alternate allocation of patients, Kopeloff, Cheney, and Kirby found no effectiveness of Cotton 's psychiatric surgeries, as patients who improved already had that prognosis and others did so without surgeries. [55] [62] They presented their findings at the American Psychiatric Association 's 1922 and 1923 annual meetings, and published two papers. [55] [63] Most of Cotton's data were questioned at Johns Hopkins University by Phyllis Greenacre , who later helped steer American psychiatry into psychoanalysis . [55] Colectomy for psychosis vanished except in Trenton until Cotton—who used publicity and word of mouth, kept the 30% death rate unpublicized, and passed a 1925 investigation by New Jersey Senate—died by heart attack in 1933. [55]
As early as 1927, Weston Price 's researches were criticized for "faulty bacterial technique". [64] In the 1930s and 1940s, researchers and editors dismissed the studies of Edward Rosenow and of Price as flawed by insufficient controls, massive doses of bacteria, and contamination of endontically treated teeth during extraction. [3] In 1938, Cecil and Angevine reported 200 cases of rheumatoid arthritis , but no consistent cures by tonsillectomies or tooth extractions. [3] [65] They noted that, "Focal infection is a splendid example of a plausible medical theory which is in danger of being converted by its enthusiastic supporters into the status of an accepted fact." [6] Newly a critic, Cecil alleged that foci were "anything readily accessible to surgery". [30]
In 1939, E W Fish implanted bacteria into guinea pigs' jaws and reported [66] that four zones develop. [3] The first zone was the zone of infection, whereas the other three zones—surrounding the zone of infection—revealed immune cells or other host cells but no bacteria. [3] Fish theorized that by removing the infectious nidus, dentists would permit recovery from the infection, and Fish's reasoning and conclusion became the basis for successful root canal treatment. [3] Still, endodontic therapy of the era indeed posed substantial risk of failure, and fear of focal infection crucially motivated endontologists to develop new and improved technology and techniques. [7]
The review and "critical appraisal" by Hobart A Reimann and W Paul Havens, published in January 1940, [31] was perhaps the most influential criticism of focal infection theory. [6] Recasting Hunter 's views of 30 years earlier as widely misinterpreted, they summarized that "the removal of infectious dental focal infections in the hope of influencing remote or general symptoms of disease must still be regarded as an experimental procedure not devoid of hazard". [53] By 1940, Louis I Grossman's textbook Root Canal Therapy flatly rejected the methods and conclusions made earlier by Price and especially by Rosenow. [67] Amid improvements in endodontics and medicine, including release of sulfa drugs and antibiotics , a backlash to the "orgy" of tooth extractions and tonsillectomies ensued. [6]
Easlick's 1951 review in the Journal of the American Dental Association notes, "Many authorities who formerly felt that focal infection was an important etiologic factor in systemic disease have become skeptical and now recommend less radical procedures in the treatment of such disorders". [68] A 1952 editorial in Journal of the American Medical Association tolled the era's end by stating that "many patients with diseases presumably caused by foci of infection have not been relieved of their symptoms by removal of the foci, many patients with these same systemic diseases have no evidence focus of infection, foci of infection are as common in apparently healthy persons as in those with disease". [69] [70] Some support extended into the late 1950s, [71] [72] yet focal infection vanished as the primary explanation of chronic, systemic diseases, [15] and was generally abandoned in the 1950s. [73]
Despite the general theory's demise, focal infection remained a formal, if rare, diagnosis, as in idiopathic scrotal gangrene [74] and angioneurotic edema . [75] Meanwhile, by way of continuing case reports claiming cures of chronic diseases like arthritis after extraction of infected or root-filled teeth, and despite lack of scientific evidence, "dental focal infection theory never died". [7] In fact, severe endodontic disease resembles classic focal infection theory. [7] [70] In 1986, it was noted that, "in spite of a decline in recognition of the focal-infection theory, the association of decayed teeth with systemic disease is taken very seriously". [10] Eventually, the theory of focal infection drew reconsideration. [73] Conversely, attribution of endocarditis to dentistry has entered doubt via case-control study, as the species usually involved is present throughout the human body. [76]
With the 1950s introduction of antibiotics, attempts to explain unexplained diseases via bacterial etiology seemed all the more unlikely. [77] By the 1970s, however, it was established that antibiotics could trigger bacteria's switch to their L phase . [78] Eluding detection by traditional methods of medical microbiology , bacterial L forms and the similar mycoplasma —and, later, viruses —became the entities expected in the theory of focal infection. [77] [78] Yet until the 1980s, such researchers were scarce, largely via scarce funding for such investigations. [77]
Despite the limited funding, research established that L forms can adhere to red blood cells and thereby disseminate from foci within internal organs such as the spleen , [79] or from oral tissues and the intestines, especially during dysbiosis . [80] [81] Perhaps some of Weston Price 's identified "toxins" in endodontically treated teeth were L forms, [82] thought nonexistent by bacteriologists of his time and widely overlooked into the 21st century. [83] Apparently, dental infections, including by uncultured or cryptic microorganisms, contribute to systemic diseases. [84] [85] [86] [87] [82] [81]
At the 1990s' emergence of epidemiological associations between dental infections and systemic diseases, American dentistry scholars have been cautious, [73] some seeking successful intervention to confirm causality. [3] [88] Some American sources emphasized epidemiology's inability to determine causality, categorized the phenomena as progressive invasion of local tissues, and distinguished that from focal infection theory—which they assert was evaluated and disproved by the 1940s. [3] Others have found focal infection theory's scientific evidence still slim, but have conceded that evolving science might establish it. [2] Yet select American authors affirm the return of a modest theory of focal infection. [89] [90]
European sources find it more certain that dental infections drive systemic diseases, at least by driving systemic inflammation, and probably, among other immunologic mechanisms, by molecular mimicry resulting in antigenic crossreaction with host biomolecules, [16] [91] [92] while some seemingly find progressive invasion of local tissues compatible with focal infection theory. [92] Acknowledging that beyond epidemiological associations, successful intervention is needed to establish causality, they emphasize that biological explanation is needed atop both, and the biological aspect is thoroughly established already, such that general healthcare, as for cardiovascular disease , must address prevalent periodontal disease , [91] [93] a stance matched in Indian literature. [94] Thus, there has emerged the concept periodontal medicine . [16] [73]
Amid continuing research interest, [13] Indian textbooks find focal infection theory established, if in more modest form than originally. [15] [95] Akshata et al have attacked the stigma and posed focal infection theory as a correct theory earlier misapplied and thereby discredited yet later refined as knowledge grew over time. [87] In a paper winning an Indian prize, they clarify "that the oral cavity can act as the site of origin for dissemination of pathogenic organisms to distant body sites, especially in immunocompromised hosts", especially those "suffering from malignancies, diabetes, rheumatoid arthritis", or "undergoing other immunosuppressive treatment", and that "uncontrolled advanced periodontitis" "presents a substantial infectious burden for the entire body by releasing bacteria, bacterial toxins, and other inflammatory mediators into the bloodstream that then affect the other parts of the body", this altogether "a paradigm shift in thinking about the directionality of oral and systemic associations". [87]
During the 1980s, dentist Hal Huggins , sparking severe controversy, spawned biological dentistry , which claims that conventional tooth extraction routinely leaves within the tooth socket the periodontal ligament that often becomes gangrenous , then, forming a jawbone cavitation seeping infectious and toxic material. [12] Sometimes forming elsewhere in bones after injury or ischemia , [96] jawbone cavitations are recognized as foci also in osteopathy [96] and in alternative medicine, [97] but conventional dentists generally conclude them to be nonexistent. [96] Although the International Academy of Oral Medicine & Toxicology claims that the scientific evidence establishing the existence of jawbone cavitations is overwhelming, and even published in textbooks, the diagnosis and related treatment remain controversial, [98] and allegations of quackery persist. [99]
Huggins and many biological dentists also espouse Weston Price 's findings on endodontically treated teeth routinely being foci of infection, [12] although these dentists have been accused of quackery . [100] Conventional belief is that microorganisms within inaccessible regions of a tooth's roots are rendered harmless once entrapped by the filling material, although little evidence supports this. [101] A H Rogers in 1976 [102] and E H Ehrmann in 1977 [103] had dismissed any relation between endodontics and focal infection. [53] At dentist George Meinig 's 1994 book, Root Canal Cover-Up , discussing researches of Rosenow and of Price, some dentistry scholars reasserted that the claims were evaluated and disproved by the 1940s. [104] [105] Yet Meinig was but one of at least three authors who in the early 1990s independently renewed the concern. [53]
Boyd Haley and Curt Pendergrass found especially high levels of bacterial toxins in root-filled teeth. [106] [82] Although such possibility appears especially likely amid compromised immunity —as in individuals cirrhotic , asplenic , elderly, rheumatoid arthritic , or using steroid drugs—there remained a lack of carefully controlled studies definitely establishing adverse systemic effects. [53] Conversely, some if few studies have investigated effects of systemic disease on root-canal therapy's outcomes, which tend to worsen with poor glycemic control, perhaps via impaired immune response, a factor largely ignored until recently, but now recognized as important. [53] Still, even by 2010, "the potential association between systemic health and root canal therapy has been strongly disputed by dental governing bodies and there remains little evidence to substantiate the claims". [53]
The traditional root-filling material is gutta-percha , whereas a new material, Biocalex, drew initial optimism even in alternative dentistry, but Biocalex-filled teeth were later reported by Boyd Haley to likewise seep toxic byproducts of anaerobic bacterial metabolism. [107] [108] Seeking to sterilize the tooth interior, some dentists, both alternative and conventional, have applied laser technology. [108] [109] Although endodontic therapy can fail and eventually often does, [101] [110] dentistry scholars maintain that it can be performed without creating focal infections. [3] And even by 2010, molecular methods had rendered no consensus reports of bacteremia traced to asymptomatic endodontic infection. [7] In any event, the predominant view is that shunning endodonthic therapy or routinely extracting endodontically treated teeth to treat or prevent systemic diseases remains unscientific and misguided. [3] [105] [111]
A focus of infection is a place containing whatever epidemiological factors are needed for transmission of an infection. Any focus of infection will have a source of infection, and other common traits of such a place include a human community, a vector population, and environmental characteristics adequate for spreading infection. [1]
In 1854 London physician John Snow discovered that people who drank from a particular water pump contracted cholera , and proposed that drinking this water from this pump was the cause of the illness. [2] At the time people did not readily believe germ theory of disease , instead favoring miasma theory . [2] The discovery of the water pump as a source of infection set a precedent which helped establish epidemiology as a science. [2]
A gay bathhouse is a place where men who have sex with men meet for sex. In the 1980s at the advent of HIV/AIDS many men who used bathhouses for sex developed AIDS as a consequence of their having sex without using safe sex practices for the prevention of HIV/AIDS . [3] Consequently, public health policies found bathhouses to be a place to target for public health intervention. [3]
Childcare infection is the spread of infection during childcare , typically because of contact among children in daycare or school. [4] This happens when groups of children meet in a childcare environment, and there any individual with an infectious disease may spread it to the entire group. Commonly spread diseases include influenza-like illness and enteric illnesses, such as diarrhea among babies using diapers. It is uncertain how these diseases spread, but hand washing reduces some risk of transmission and increasing hygiene in other ways also reduces risk of infection. [5] [6]
A fomite ( / ˈ f oʊ m aɪ t / ) or fomes ( / ˈ f oʊ m iː z / ) is any inanimate object that, when contaminated with or exposed to infectious agents (such as pathogenic bacteria , viruses or fungi ), can transfer disease to a new host . [1] In the 21st century, the role of fomites in disease transfer is higher than ever in human history because of the indoor lifestyle. [2]
A fomite is any inanimate object (also called passive vector) that, when contaminated with or exposed to infectious agents (such as pathogenic bacteria , viruses or fungi ), can transfer disease to a new host . [1] Contamination can occur when one of these objects comes into contact with bodily secretions, like nasal fluid, vomit, or feces. Many common objects can sustain a pathogen until a person comes in contact with the pathogen, increasing the chance of infection. The likely objects are different in a hospital environment than at home or in a workplace.
For humans, common hospital fomites are skin cells , hair, clothing, and bedding. [3]
Fomites are associated particularly with hospital-acquired infections (HAIs), as they are possible routes to pass pathogens between patients. Stethoscopes and neckties are common fomites associated with health care providers . [4] It worries epidemiologists and hospital practitioners because of the growing selection of microbes resistant to disinfectants or antibiotics (so-called antimicrobial resistance phenomenon).
Basic hospital equipment, such as IV drip tubes, catheters , and life support equipment, can also be carriers, when the pathogens form biofilms on the surfaces. Careful sterilization of such objects prevents cross-infection. [5] Used syringes, if improperly handled, are particularly dangerous fomites.
In addition to objects in hospital settings, other common fomites for humans are cups, spoons, pencils, bath faucet handles, toilet flush levers, door knobs, light switches, handrails, elevator buttons, television remote controls, pens, touch screens, common-use phones, keyboards and computer mice, coffeepot handles, countertops, drinking fountains, and any other items that may be frequently touched by different people and infrequently cleaned.
Cold sores, hand–foot–mouth disease, and diarrhea are some examples of illnesses easily spread by contaminated fomites. [6] The risk of infection by these diseases and others through fomites can be greatly reduced by simply washing one's hands. [6] When two children in one household have influenza, more than 50% of shared items are contaminated with virus. In 40–90% cases, adults infected with rhinovirus have it on their hands. [7]
Researchers have discovered that smooth (non-porous) surfaces like door knobs transmit bacteria and viruses better than porous materials like paper money because porous, especially fibrous, materials absorb and trap the contagion, making it harder to contract through simple touch. [2] Nonetheless, fomites may include soiled clothes, towels, linens, handkerchiefs, and surgical dressings. [8] [9]
SARS-CoV-2 was found to be viable on various surfaces from 4 to 72 hours under laboratory conditions. [10] However, fomite transmission of SARS-CoV-2 in real healthcare settings is yet to be determined. [11]
The 2007 research showed that influenza virus was still active on stainless steel 24 hours after contamination. Though on hands it survives only for five minutes, the constant contact with a fomite almost certainly means catching the infection. [12] Transfer efficiency depends not only on surface, but mainly on pathogen type. For example, avian influenza survives on both porous and non-porous materials for 144 hours. [2]
Contaminated needles are the most common fomite that transmits HIV . [13]
The Italian scholar and physician Girolamo Fracastoro appears to have first used the Latin word fomes , meaning " tinder ", in this sense in his essay on contagion, De Contagione et Contagiosis Morbis , published in 1546: [14] "By fomes I mean clothes, wooden objects, and things of that sort, which though not themselves corrupted can, nevertheless, preserve the original germs of the contagion and infect by means of these". [15]
English usage of fomes , pronounced / ˈ f oʊ m iː z / , is documented since 1658. [16] The English word fomite , which has been in use since 1859, is a back-formation from the plural fomites (originally borrowed from the Latin plural fōmĭtēs [ˈfoːmɪteːs] of fōmĕs [ˈfoːmɛs] ). [17] [18] Over time, the English-language pronunciation of the plural fomites changed from / ˈ f oʊ m ɪ t iː z / ) to / ˈ f oʊ m aɪ t s / , which led to the creation of a new singular fomite , pronounced / ˈ f oʊ m aɪ t / . [18] [19] [20] The French fomite , Italian fomite , Spanish fómite and Portuguese fómite or fômite , however, are derived directly from the Latin accusative singular fōmĭtēm , as usually happens with Latin common nouns.

In epidemiology , force of infection (denoted λ {\displaystyle \lambda } ) is the rate at which susceptible individuals acquire an infectious disease . [1] Because it takes account of susceptibility it can be used to compare the rate of transmission between different groups of the population for the same infectious disease, or even between different infectious diseases. That is to say, λ {\displaystyle \lambda } is directly proportional to β {\displaystyle \beta } ; the effective transmission rate.
Such a calculation is difficult because not all new infections are reported, and it is often difficult to know how many susceptibles were exposed. However, λ {\displaystyle \lambda } can be calculated for an infectious disease in an endemic state if homogeneous mixing of the population and a rectangular population distribution (such as that generally found in developed countries) is assumed. In this case, λ {\displaystyle \lambda } is given by:
where A {\displaystyle A} is the average age of infection. In other words, A {\displaystyle A} is the average time spent in the susceptible group before becoming infected. The rate of becoming infected ( λ {\displaystyle \lambda } ) is therefore 1 / A {\displaystyle 1/A} (since rate is 1/time). The advantage of this method of calculating λ {\displaystyle \lambda } is that data on the average age of infection is very easily obtainable, even if not all cases of the disease are reported.
The discipline of forensic epidemiology (FE) is a hybrid of principles and practices common to both forensic medicine and epidemiology . FE is directed at filling the gap between clinical judgment and epidemiologic data for determinations of causality in civil lawsuits and criminal prosecution and defense. [1] [2] [3] [4]
Forensic epidemiologists formulate evidence-based probabilistic conclusions about the type and quantity of causal association between an antecedent harmful exposure and an injury or disease outcome in both populations and individuals. The conclusions resulting from an FE analysis can support legal decision-making regarding guilt or innocence in criminal actions, and provide an evidentiary support for findings of causal association in civil actions.
Applications of forensic epidemiologic principles are found in a wide variety of types of civil litigation, including cases of medical negligence, toxic or mass tort, pharmaceutical adverse events, medical device and consumer product failures, traffic crash-related injury and death, person identification and life expectancy.
The term Forensic Epidemiology was first associated with the investigation of bioterrorism in 1999, and coined by Dr. Ken Alibek, the former chief deputy of the Soviet bioweapons program. The scope of FE at that time was confined to the investigation of epidemics that were potentially man-made. After the US Anthrax attacks of 2001 the CDC defined forensic epidemiology as a means of investigating possible acts of bioterrorism.
At the present time FE is more widely known and described as the systematic application of epidemiology to disputed issues of causation that are decided in (primarily) civil, but also criminal courts. The use of epidemiologic data and analysis as a basis for assessing general causation in US courts, particularly in toxic tort cases, has been described for more than 30 years, beginning with the investigation of the alleged relationship between exposure to the Swine Flu vaccine in 1976 and subsequent cases of Guillain–Barré syndrome. [1]
More recently FE has also been described as an evidence-based method of quantifying the probability of specific causation in individuals. The approach is particularly helpful when a clinical differential diagnosis approach to causation is disputed. Examples covering a wide variety of applications of FE are listed below under Examples of Investigative Questions Addressed by Forensic Epidemiologists.
The metric of a case-specific FE analysis of cause is the comparative risk ratio (CRR).  The CRR is a unique metric to FE; it allows for the comparison of probabilities applicable to the investigated circumstances of an individual injury or disease.  Because a CRR is based on the unique circumstances surrounding the injury or disease of an individual, it may or may not be derived from a population-based relative risk (RR) or odds ratio (OR). An example of an RR analysis that could be used as a CRR  is as follows: for an unbelted driver who was seriously injured in a traffic crash, an important causal question might be what role the failure to use a seat belt played in causing his injury. A relevant RR analysis would consist of the examination of the frequency of serious injury in 1000 randomly selected unbelted drivers exposed to a 20 mph frontal collision versus the frequency of serious injury in 1000 randomly selected restrained drivers exposed to the same collision severity and type. If the frequency of serious injury in the group exposed to the presumptive hazard (failure to use a seat belt) was 0.15 and the frequency in the unexposed (belted) group was 0.05, then the CRR would be the same thing as the RR of 0.15/0.05. The RR design of the analysis dictates that the populations that the numerator and denominator of the CRR  are substantially similar in all respects, with the exception of the exposure to the investigated hazard, which was the failure to use a seat belt in the example.
In some instances encountered in a legal setting, however, the numerator and denominator risk must be derived from dissimilar populations in order to fit the circumstances of an investigated injury or disease. In such a case the CRR cannot be derived from either an RR or OR. An example of such a situation occurs when the numerator is a per event risk, and the denominator is a per-time risk (also known as a cumulative risk). An example of this type of analysis would be the investigation of a pulmonary embolism (PE) that occurred a week after a patient sustained a lower extremity fracture in a traffic crash. Such complications often result from blood clots forming in the legs and then traveling to the lungs. If the patient had a history of deep vein thrombosis (DVT) in the lower extremities prior to the crash, then a CRR might consist of the comparison between the risk of a PE following a lower extremity fracture (a per event rate) and  the 1-week risk of PE in a patient with DVT (a time-dependent probability).
Another example of a CRR based on dissimilar populations is when there are only a limited number of potential causes to be compared. An example is the investigation of the cause of an adverse reaction in a person who took two different drugs at the same time, both of which could have caused the reaction (and which, for the example, do not interact with each other). In such a situation, the CRR applicable to the unique circumstances experienced by the individual could be estimated by comparing the adverse reaction rate for the two drugs.
The attributable proportion under the exposed (AP e ) is an indication of the proportion of patients who were exposed to the potential cause and got sick because of this exposure. It can only be used if the RR >1 and can be calculated by [(RR-1)/RR X 100%]. When the CRR is based on an RR, these formulae also apply to the CRR. The result of the analysis, given as an RR, CRR, or AP e , meets the legal standard of what is “ more likely true than not ,”  when the RR or CRR is >2.0 (with a 95% confidence interval lower boundary of >1.0), or the AP e is >50%. The AP e is also known as the " Probability of Causation (PC)" a term that is defined in the US Code of Federal Regulations ( Federal Register / Vol. 67, No. 85 / Thursday, May 2, 2002 / Rules and Regulations p. 22297 ) and elsewhere.
Analysis of causation, particularly for injury or other conditions with a relatively short latency period between exposure and outcome, is accomplished using a 3-step approach, as follows: [5]
The 3-step methodology was challenged in United States District Court for the District of Colorado in Etherton v Auto-Owners Insurance Company . [2] The defendant challenged, among other things, the reliability and fit of the methods described by the expert. After an extensive examination and discussion of the 3-step process used by the expert, the court found that the methodology appropriately fit the specific facts of the case, and that a population-based (epidemiologic) approach was an appropriate part of the causal methodology. The court denied the defendant’s motion to strike the expert’s testimony in the order, which was entered on 3/31/14.
The Defendant appealed the ruling from the District Court, and in July 2016, the Tenth Circuit U.S. Court of Appeals affirmed the 3-step causal methodology as generally accepted and well established for assessing injury causation, under the Daubert standard .  See Etherton v. Auto-Owners Insurance Company , No. 14-1164 (10th Cir, 7/19/16) [3] .
Plausibility of an investigated association can be assessed in an FE investigation, in part, via application of the Hill criteria , named for a 1965 publication by Sir Austin Bradford-Hill, in which he described nine “viewpoints” by which an association described in an epidemiologic study could be assessed for causality. [6] Hill declined to call his viewpoints “criteria”  lest they be considered a checklist for assessing causation. The term “ Hill criteria”  is used widely in the literature, however, and for convenience is used in the present discussion. Of the nine criteria, there are seven that have utility for assessing the plausibility of an investigated specific causal relationship, as follows:
Subsequent authors have added the feature of Challenge/ Dechallenge/ Rechallenge for circumstances when the exposure is repeated over time and there is the ability to observe the associated outcome response, as might occur with an adverse reaction to a medication. Additional considerations when assessing an association are the potential impact of confounding and bias in the data, which can obscure a true relationship. Confounding refers to a situation in which an association between an exposure and outcome is all or partly the result of a factor that affects the outcome but is unaffected by the exposure. Bias refers to a form of error that may threaten the validity of a study by producing results that are systematically different from the true results. Two main categories of bias in epidemiologic studies are selection bias , which occurs when study subjects are selected as a result of another unmeasured variable that is associated with both the exposure and outcome of interest; and information bias , which is systematic error in the assessment of a variable. While useful when assessing a previously unexplored association, there is no combination or minimal number of these criteria that must be met in order to conclude that a plausible relationship exists between a known exposure and an observed outcome.
In many FE investigations there is no need for a causal plausibility analysis if a general causal relationship is well established. In large part, plausibility of a relationship is entertained once implausibility has been rejected. The two remaining Hill criteria are temporality  and strength of association. While both criteria have utility in assessing specific causation, temporality is the feature of an association that must be present, at least with regard to sequence ( i.e. , the exposure must precede the outcome), in order to consider a relationship causal. Temporal proximity can also be useful in some specific causation evaluations, as the closer the investigated exposure and the outcome are in time the less opportunity there is for an intervening cause to act. Another feature of temporality that may have a role in a specific causation evaluation is latency. An outcome may occur too soon or too long after an exposure to be considered causally related. As an example, some food borne illnesses must incubate for hours or days after ingestion, and thus an illness that begins directly following a meal, and which is later found to be caused by a food borne microorganism that requires >12 h incubation, was not caused by the investigated meal, even if an investigation were to reveal the microorganism in the ingested food. Strength of association is the criterion that is used in general causation to assess the impact of the exposure on the population, and is often quantified in terms of RR. In a specific causation evaluation the strength of the association between the exposure and the outcome is quantified by the CRR, as described above.
Test accuracy investigation is a standard practice in clinical epidemiology. In this setting, a diagnostic test is scrutinized to determine by various measures how often a test result is correct. In FE the same principles are used to evaluate the accuracy of proposed tests leading to conclusions that are central to fact finder determinations of guilt or innocence in criminal investigations, and of causality in civil matters. The utility of a test is highly dependent on its accuracy, which is determined by a measure of how often a positive or negative test result truly represents the actual status that is being tested. For any test or criterion there are typically four possible results: (1) a true positive (TP), in which the test correctly identifies tested subjects with the condition of interest; (2) a true negative (TN), in which the test correctly identifies test subjects who do not have the condition of interest; (3) a false positive (FP), in which the test is positive even though condition is not present, and; (4) a false negative (FN) in which the test is negative even though the condition is present. Fig. 3.19 is a contingency table illustrating the relationships between test results and condition presence, as well as the following test accuracy parameters:
Probability is used to characterize the degree of belief in the truth of an assertion. The basis for such a belief can be a physical system that produces outcomes at a rate that is uniform over time, such as a gaming device like a roulette wheel or a die. With such a system, the observer does not influence the outcome; a fair six-sided die that is rolled enough times will land on any one of its sides 1/6th of the time. An assertion of a probability based in a physical system is easily tested with sufficient randomized experimentation. Conversely, the basis for a high degree of belief in an asserted claim may be a personally held perspective that cannot be tested. This does not mean that the assertion is any less true than one that can be tested. As an example, one might truthfully assert that “if I eat a banana there is a high probability that it will make me nauseous” based upon experience unknown to anyone but one’s self. It is difficult to test such assertions, which are evaluated through collateral evidence of plausibility and analogy, often based on similar personal experience. In forensic settings, assertions of belief are often characterized as probabilities, that is, what is most likely , for a given set of facts. For circumstances in which a variety of conditions exist that may modify or “ condition”  the probability of a particular outcome or scenario, a method of quantifying the relationship between the modifying conditions and the probability of the outcome employs Bayesian reasoning , named for Bayes’ Theorem or Law upon which the approach is based. Most simply stated, Bayes’ Law allows for a more precise quantification of the uncertainty in a given probability. As applied in a forensic setting, Bayes’ Law tells us what we want to know given what we do know. Although Bayes’  Law is known in forensic sciences primarily for its application to DNA evidence, a number of authors have described the use of Bayesian reasoning for other applications in forensic medicine, including identification and age estimation.
The post-test probability is a highly useful Bayesian equation that allows for the calculation of the probability that a condition is present when the test is positive, conditioned by the pretest prevalence of the condition of interest. This equation is given in box to the right:
The equation results in a positive predictive value for a given pre-event or pretest prevalence. In a circumstance in which the pretest prevalence is considered “indifferent” the prevalence and (1-prevalence) values cancel out, and the calculation is a simplified to a positive predictive value.

The French paradox is a catchphrase first used in the late 1980s, that summarizes the apparently paradoxical epidemiological observation that French people have a relatively low incidence of coronary heart disease (CHD), while having a diet relatively rich in saturated fats , [1] in apparent contradiction to the widely held belief that the high consumption of such fats is a risk factor for CHD .  The paradox is that if the thesis linking saturated fats to CHD is valid, the French ought to have a higher rate of CHD than comparable countries where the per capita consumption of such fats is lower.
It has also been suggested that the French paradox is an illusion, created in part by differences in the way that French authorities collect health statistics, as compared to other countries, and in part by the long-term effects, in the coronary health of French citizens, of changes in dietary patterns which were adopted years earlier. [2] Nevertheless the French diet may be considered to be a type of Mediterranean diet - that is, one consisting largely of fruits, vegetables, whole grains, legumes and olive oil, with limited amounts of lean protein from fish and poultry - which has fairly well established benefits. [3]
The term "French Paradox" was first used in The Letter , the newsletter of the International Organisation of Vine and Wine , in 1986. In 1989, theatre Professor George Riley Kernodle of the University of Arkansas used the term in a chapter in his book Theatre In History , later republished as a separate academic paper. [4]
In 1991, Serge Renaud, a scientist from Bordeaux University , France—considered today the father of the phrase—presented the results of his scientific study into the term and actual scientific data behind the perception of the phrase. [5] This was followed by a public documentary broadcast on the American CBS News television channel, 60 Minutes .
Renaud's observations regarding the apparent disconnect between French patterns of high saturated fat consumption and their low rates of cardiovascular disease can be quantified using data from the Food and Agriculture Organization of the United Nations. [6] [ non-primary source needed ] In 2002, the average French person consumed 108 grams per day of fat from animal sources, while the average American consumed only 72 grams. The French eat four times as much butter, 60 percent more cheese and nearly three times as much pork. Although the French consume only slightly more total fat (171 g/d vs 157 g/d), they consume much more saturated fat because Americans consume a much larger proportion of fat in the form of vegetable oil, with most of that being soybean oil. [7] However, according to data from the British Heart Foundation , [8] in 1999, rates of death from coronary heart disease among males aged 35–74 years were 115 per 100,000 people in the U.S. but only 83 per 100,000 in France.
In 1991, Renaud extended his studies in partnership with then junior researchers, cardiologist Michel de Lorgeril and dietician Patricia Salen. The three enhanced Renaud's study, with their paper concluding that: a diet based on southwestern Mediterranean cuisine ; which is high in omega-3 oils, antioxidants and includes "moderate consumption" of red wine ; created lower cases of cancer , myocardial infarction and cardiovascular disease ; partly through increasing HDL cholesterol whilst reducing LDL cholesterol. [5]
In 1999, Malcolm Law and Nicholas Wald published a study in the British Medical Journal , [2] using data from a 1994 study of alcohol and diet to explain how the French paradox might actually be an illusion, caused by two statistical distortions.
First, Law and Wald attributed about 20% of the difference in the observed rates of CHD between France and the United Kingdom to the under-certification of CHD in France, relative to the UK.
Second, Law and Wald presented a time-lag hypothesis: if there were a delay in serum cholesterol concentrations increasing and a subsequent increase in ischaemic heart disease mortality, then the current rate of mortality from CHD is more likely to be linked to past levels of serum cholesterol and fat consumption than to current serum cholesterol levels and patterns of fat consumption. They wrote,
We propose that the difference is due to the time lag between increases in consumption of animal fat and serum cholesterol concentrations and the resulting increase in mortality from heart disease—similar to the recognised time lag between smoking and lung cancer. Consumption of animal fat and serum cholesterol concentrations increased only recently in France but did so decades ago in Britain.
Evidence supports this explanation: mortality from heart disease across countries, including France, correlates strongly with levels of animal fat consumption and serum cholesterol in the past (30 years ago)....
In addition, the French population has become increasingly overweight. A study published by the French Institute of Health and Medical Research (INSERM) revealed an increase in obesity from 8.5% in 1997 to 14.5% in 2009, with women showing a greater tendency toward obesity than men. [9]
The overall impact of the popular perception, in the English-speaking world, that the French paradox is a real phenomenon, has been to give added credibility to health claims associated with specific French dietary practices.
This was seen most dramatically when, in 1991, an early account of the then-novel concept of the French paradox was aired in the United States on 60 Minutes . The broadcast left the impression    that France's high levels of red wine consumption accounted for much of the country's lower incidence of cardiac disease. Within a year, the consumption of red wine in the United States had increased by 40% [10] and some wine sellers began promoting their products as " health food ." [11]
The cultural impact of the French paradox can be seen in the large number of book titles in the diet-and-health field which purport to give the reader access to the secrets behind the paradox:
Other books sought to boost their credibility by reference to the French paradox.  The American edition of The Dukan Diet , written by Pierre Dukan, a Paris-based doctor, is marketed with the subtitle, “The real reason the French stay thin.”
The existence of the French paradox has caused some researchers to speculate that the link between dietary consumption of saturated fats and coronary heart disease might not be as strong as had previously been imagined. This has resulted in a review of the earlier studies which had suggested this link.
Some researchers have thrown into question the entire claimed connection between natural saturated fat consumption and cardiovascular disease. In 2006 this view received some indirect support from the results of the Nurses' Health Study run by the Women's Health Initiative . After accumulating approximately 8 years of data on the diet and health of 49,000 post-menopausal American women, the researchers found that the balance of saturated versus unsaturated fats did not appear to affect heart disease risk, whereas the consumption of trans fat was associated with significantly increased risk of cardiovascular disease. [12]
Similarly, the authors of a 2009 review of dietary studies concluded that there was insufficient evidence to establish a causal link between consumption of saturated fats and coronary heart disease risk. [13]
It has been suggested that France's high red wine consumption is a primary factor in the trend. This hypothesis was expounded in a 60 Minutes broadcast in 1991. [14] The program catalysed a large increase in North American demand for red wines from around the world. It is believed that one of the components of red wine potentially related to this effect is resveratrol ; [15] however, the authors of a 2003 study concluded that the amount of resveratrol absorbed by drinkers of red wine is small enough that it is unlikely to explain the paradox. [16]
The difference between U.S. annual per capita alcohol consumption (9.2 litres per year) [17] and French consumption (12.2 litres per year) is only 3 litres per year. [18]
There is a lack of medical consensus about whether moderate consumption of beer, wine, or distilled spirits has a stronger association with longevity. Of ten major studies, three found stronger evidence for wine, three for beer, three for liquor, and one study found no difference between alcoholic beverages. [19]
Wines, particularly red wines, are a source of low levels of resveratrol . However, there is no high-quality evidence that resveratrol improves lifespan or has an effect on any human disease. [20] [21]
Oligomeric procyanidins have been proposed to offer protection to human vascular cells, [22] with other research indicating that red wine polyphenols reduce absorption of malondialdehyde which is implicated in elevating levels of low-density lipoprotein in the onset of arteriosclerosis . [23]
However, once digested and metabolized, the role of polyphenols to assert possible health effects in humans cannot be determined. [24] Although polyphenols are speculated to be part of the health-promoting effects of wine consumption, no evidence exists to date that ingesting polyphenols from red wine or food sources actually provides health benefits. [25]
The French diet is based on natural saturated fats such as butter, cheese and cream that the human body finds easy to metabolize, because they are rich in shorter saturated fatty acids ranging from the 4-carbon butyric acid to the 16-carbon palmitic acid . But the American diet includes greater amounts of saturated fats made via hydrogenating vegetable oils which include longer 18- and 20-carbon fatty acids.  In addition, these hydrogenated fats include small quantities of trans fats which may have associated health risks. [26] [27] [28]
In his 2003 book, The Fat Fallacy: The French Diet Secrets to Permanent Weight Loss , Will Clower suggests the French paradox may be narrowed down to a few key factors, namely:
Clower tends to downplay the common beliefs that wine consumption and smoking are greatly responsible for the French paradox. While a higher percentage of French people smoke, this is not greatly higher than the U.S. (35% in France vs. 25% in U.S.) and is unlikely to account for the weight difference between countries.
Mireille Guiliano , author of the 2006 bestseller French Women Don't Get Fat , [29] agrees that the weight differences are not due to French smoking habits.  She points out that the smoking rates for women in France and the U.S. are virtually identical. [30] Guiliano explains the key factors to the French woman's ability to stay slim as:
In his 2008 book In Defense of Food , Michael Pollan suggests the explanation is not any single nutrient, nor the amount of carbohydrates or fats or proteins, but the whole length and breadth of nutrients found in "natural" as opposed to "processed" foods. [31]
It has been suggested that a higher intake of fruit and vegetables in the diet may lower the risk of cardiovascular diseases. [32]
One proposed explanation of the French paradox regards possible effects ( epigenetic or otherwise) of dietary improvements in the first months and years of life, exerted across multiple generations. Following defeat in the Franco-Prussian War in 1871, the French government introduced an aggressive nutritional program providing high quality foods to pregnant women and young children with the aim of fortifying future generations of soldiers (the program was implemented about three decades prior to an analogous initiative in England in response to the Boer War ). It has been suggested that the particular timing of this historical intervention  might help explain the relatively low rates of obesity and heart disease found in France. [33]
Functional Molecular Infection Epidemiology ( FMIE ) [1] is an emerging area of medicine that entails the study of pathogen genes and genomes in the context of their functional association with the host niches (adhesion, invasion, adaptation) and the complex interactions they trigger within the host immune system ( cell signaling , apoptosis ) to culminate in varied outcomes of the infection. This can also be defined as the correlation of genetic variations in a pathogen or its respective host with a unique function that is important for disease severity, disease progression, or host susceptibility to a particular pathogen. Functional epidemiology implies not only  descriptive host-pathogen genomic associations, but rather the interplay between pathogen and host genomic variations to functionally demonstrate the role of the genetic variations during infection.
Functional Molecular Infection Epidemiology differs from classical Molecular Infection Epidemiology mainly in that the latter deals with the tagging and tracking of the infectious agent without much concern for the functional/phenotypic characteristics of the agent being tracked. Functional molecular epidemiology, on the other hand, lays more emphasis on genotypic and phenotypic correlates of host-pathogen interaction, adaptation or homeostasis. Furthermore, classical molecular epidemiology largely uses “neutral” markers, such as insertion sequences and intergenic elements, while functional molecular epidemiology harnesses functionally relevant markers such as SNPs and genome co-ordinates with putative roles in infection biology – both on the pathogen and the host side. Many studies have been conducted which fit the theme of FMIE - for example, acquisition and transmission of the Mycobacterium avium subsp. paratuberculosis and its role in the development of Type-1 diabetes mellitus when human gene SLC11A1 undergoes particular mutations in a susceptible host. [2]
The concept of FMIE has become potentially relevant in the aftermath of multiple genome sequencing and resequencing of important bacterial pathogens from many different host/patient populations. [3]
A consortium of scientists in India and Germany is (Project BRIDGE) already formed under the aegis of the Freie University in Berlin and the University of Hyderabad to explore and investigate the application of FMIE in public health and Veterinary arena.

The Glasgow effect refers to the low life expectancy and poor health of residents of Glasgow , Scotland , compared to the rest of the United Kingdom and Europe. [1] [2] Defined as "[e]xcess mortality in the West of Scotland (Glasgow) after controlling for deprivation", [3] the Glasgow effect is part of the "Scottish effect"; the country as a whole has health disparities compared to the rest of Europe, and indeed it has been called "the sick man of Europe". The disparities are greatest in Glasgow. [4]
Although lower income levels are generally associated with poor health and shorter lifespan, epidemiologists have argued that poverty alone does not appear to account for the disparity found in Glasgow. [2] [5] [6] [7] [8] [4] Equally deprived areas of the UK such as Liverpool and Manchester have higher life expectancies, and the wealthiest ten percent of the Glasgow population have a lower life expectancy than the same group in other cities. [9] One in four men in Glasgow will die before his sixty-fifth birthday. [10]
Several hypotheses have been proposed to account for the ill health, including the practice in the 1960s and 1970s of offering young, skilled workers in Glasgow social housing in new towns , leaving behind a demographically "unbalanced population". [11] [12] Other suggested factors have included a high prevalence of premature and low birthweight births, land contaminated by toxins, a high level of derelict land, more deindustrialisation than in comparable cities, poor social housing, religious sectarianism , lack of social mobility , soft water , [13] vitamin D deficiency , cold winters, higher levels of poverty than the figures suggest, adverse childhood experiences and childhood stress, high levels of stress in general, and social alienation. [14]
The city's mortality gap was not apparent until 1950 and seems to have widened since the 1970s. [9] The Economist wrote in 2012: "It is as if a malign vapour rises from the Clyde at night and settles in the lungs of sleeping Glaswegians." [15]
The mortality rates are the highest in the UK and among the highest in Europe. As of 2016, life expectancy in Scotland was lower for both females and males than anywhere else in western Europe, and was not improving as quickly as in other western European countries. [16] With a population of 1.2 million in greater Glasgow, life expectancy at birth is 71.6 years for men, nearly seven years below the national average of 78.2 years, and 78 years for women, over four years below the national average of 82.3. [14] [17] [18]
According to the World Health Organization in 2008, the male life expectancy at birth in the Calton area of Glasgow at that time was 54 years. [a] A local doctor attributed this to alcohol and drug abuse, and to a violent gang culture . [21] According to Bruce Whyte of the Glasgow Centre for Population Health, writing in 2015, the estimate was based on deaths in 1998–2002 in an area comprising 2,500 people, and the figures may have been affected by the presence of hostels for adults with alcohol, drug and mental-health problems. The 2008–2012 estimate for Calton and nearby Bridgeton together, by then more ethnically diverse and with fewer hostels, was 67.8 years for males and 76.6 years for females. [22]
Research led by David Walsh of the Glasgow Centre for Population Health in 2010 concluded that the deprivation profiles of Glasgow, Liverpool and Manchester are almost identical, but premature deaths in Glasgow are over 30 percent higher, and all deaths around 15 percent higher, across almost the entire population. [9] The higher mortality is fueled by stroke, respiratory disease, cardiovascular disease and cancer, along with deaths caused by alcohol, drugs, violence and suicide. [23] According to a 2016 study, 43 percent of adults are classified as either disabled or chronically ill. Suicide rates are higher than they were in 1968, and the all-cause mortality rate in the 15–44 age group is 142.4 deaths per 100,000. [4] Drug-related deaths in Scotland more than doubled between 2006 and 2016. [24]
The Glasgow Centre for Population Health (GCPH) was established in 2004 to study the causes of Glasgow's ill health; the centre's partners are NHS Greater Glasgow and Clyde , Glasgow City Council and the University of Glasgow . [25] In a publication introducing the GCPH, the Chief Medical Officer for Scotland , Harry Burns , referred to research suggesting that chronically activated stress responses, especially in children, affect the structure of parts of the frontal lobes of the brain, and that these determine the physical reaction to stress, which could result in chronic ill health. The ability to attain good health, he suggested, depends in part on whether people feel in control of their lives, and whether they see their environments as threatening or supportive. [26]
A GCPH report in 2016 concluded that certain historical processes and policy decisions had left the city more vulnerable to deprivation. [27] [28] [29] [12] Factors include the "lagged effects" of overcrowding and the former practice, in the 1960s and 1970s, of offering young, skilled workers social housing in new towns outside Glasgow; this, according to a 1971 government document, threatened to leave behind an "unbalanced population with a very high proportion of the old, the very poor and the almost unemployable". [30]
Other hypotheses have included a higher prevalence of premature and low-birthweight births; [31] land contaminated by toxins such as chromium ; [32] a high level of derelict land, leading to a "negative physical environment"; [33] more deindustrialisation than in comparable cities; [34] and low-quality housing estates. [35] [12] Social deficits and sources of social dysfunction have been suggested: religious sectarianism ; [36] a low " sense of coherence "; [37] low social capital ; [38] lack of social mobility ; [39] and a culture of alienation and pessimism. [14] [15] Soft water (with lower levels of magnesium and calcium ) has been mentioned as a possible factor, [40] as have cold winters; vitamin D deficiency ; higher levels of poverty than the figures suggest; and adverse childhood experiences . [14] [15]
Walsh, D.; Bendel, N; Jones, R.; Hanlon, P. (2010). "It's not 'just deprivation': Why do equally deprived UK cities experience different health outcomes?" . Public Health . 124 (9): 487–495. doi : 10.1016/j.puhe.2010.02.006 . PMID 20223487 .
Media related to Glasgow effect at Wikimedia Commons
The genomic epidemiological database for global identification of microorganisms or global microbial identifier [1] is a platform for storing whole genome sequencing data of microorganisms , for the identification of relevant genes and for the comparison of genomes to detect and track-and-trace infectious disease outbreaks and emerging pathogens . [2] The database holds two types of information: 1) genomic information of microorganisms , linked to, 2) metadata of those microorganism such as epidemiological details. The database includes all genera of microorganisms: bacteria , viruses , parasites and fungi .
For genotyping of microorganisms for medical diagnosis , or other purposes, scientists may use a wide variety of DNA profiling techniques, such as polymerase chain reaction , pulsed-field gel electrophoresis or multilocus sequence typing . A complication of this broad variety of techniques is the difficulty to standardize between techniques, laboratories and microorganisms, which may be overcome using the complete DNA code of the genome generated by whole genome sequencing. [3] For straightforward diagnostic identification, the whole genome sequencing information of a microbiological sample is fed into a global genomic database and compared using BLAST procedures to the genomes already present in the database. [4] In addition, whole genome sequencing data may be used to back calculate to the different pre-whole genome sequencing genotyping methods, so previous collected valuable information is not lost. [5] [6] For the global microbial identifier the genomic information is coupled to a wide spectrum of metadata about the specific microbial clone and includes important clinical and epidemiological information such as the global finding places, treatment options and antimicrobial resistance , making it a general microbiological identification tool. This makes personalized treatment of microbial disease possible as well as real-time tracing systems for global surveillance of infectious diseases for food safety and serving human health .
The initiative for building the database arose in 2011 and when several preconditions were met: 1) whole genome sequencing has become mature and serious alternative for other genotyping techniques, [7] [8] 2) the price of whole genome sequencing has started falling dramatically and in some cases below the price of traditional identifications, 3) vast amounts of IT resources and a fast Internet have become available, and 4) there is the idea that via a cross sectoral and One Health approach infectious diseases may be better controlled. [9] [10]
Starting the second millennium, many microbiological laboratories, as well as national health institutes, started genome sequencing projects for sequencing the infectious agents collections they had in their biobanks . [11] [12] Thereby generating private databases and sending model genomes to global nucleotide databases such as GenBank of the National Center for Biotechnology Information [13] or the nucleotide database of the EMBL . [14] This created a wealth of genomic information and independent databases for eukaryotic as well as prokaryotic genomes. [15] [16] [17] The need to further integrate these databases and to harmonize data collection, and to link the genomic data to metadata for optimal prevention of infectious diseases, was generally recognized by the scientific community. [18] In 2011, several infectious disease control centers and other organizations took the initiative of a series of international scientific- and policy-meetings, to develop a common platform and to better understand the potentials of an interactive microbiological genomic database. The first meeting was in Brussels, September 2011, [19] [20] followed by meetings in Washington (March 2012) and Copenhagen [21] (February 2013). In addition to experts from around the globe, Intergovernmental Organizations have been included in the action, notably the World Health Organization and the World Organization for Animal Health .
A detailed roadmap [22] for the development of the database was set up with the following general timeline:
Current members:
Former members:
The Global Outbreak Alert and Response Network ( GOARN ) is a network composed of numerous technical and public health institutions, laboratories, NGOs, and other organizations that work to observe and respond to threatening epidemics. [1] GOARN works closely with and under the World Health Organization (WHO), which is one of its most notable partners. Its goals are to: examine and study diseases, evaluate the risks that certain diseases pose, and improve international capability to deal with diseases. [2]
The World Health Organization realized at the start of the 21st century that it did not have the resources required to adequately respond to and prevent epidemics around the world. Thus, a "Framework for Global Outbreak and Response" was created by the Department of Communicable Diseases Surveillance and Response, and Regional Offices. This framework was put then forth in a meeting in Geneva from April 26–28, 2000. In this meeting, which was attended by 121 representatives from 67 institutions, the decision was made to form GOARN to contribute resources, coordination, surveillance, and technical assistance towards combating diseases. [3]
It was decided that GOARN would be directed by a steering committee made of 20 representatives of GOARN partners and an operational support team (OST) based in WHO. The steering committee oversees and plans the activities of GOARN, and the OST is composed of a minimum of 5–6 WHO staff. Task forces and groups were established to deal with specific issues. [4] GOARN resources are primarily coordinated by the World Health Organization. [3]
The WHO's guiding principles are to standardize "epidemiological, laboratory, clinical management, research, communication, logistics, support, security, evacuation, and communication systems" and coordinative international resources to support local efforts by GOARN partners to combat outbreaks. It also focuses on improving long term ability to provide technical assistance to affected areas. [4]
GOARN has grown to now have over 600 partners in the form of public health institutions, networks, laboratories, and United Nations and non-governmental organizations. Technical institutions, networks, and organizations that have the ability to improve GOARN's capabilities are eligible for partnership. [5] Through its partners, GOARN is staffed by a variety of individuals who specialize in public health, such as "doctors, nurses, infection control specialists, logisticians, laboratory specialists; communication, anthropology and social mobilization experts, emergency management and public health professionals among others." [6]
As its biggest partner, WHO plays a large role in GOARN. Alongside coordinating its resources to combat outbreaks, WHO provides much of the staffing and assistance for GOARN, though as will be covered later, does not fund GOARN directly. Since the network is primarily led by the WHO, there is some uncertainty as to whether WHO should be considered a partner in GOARN or if the network should be considered a WHO initiative.
Another notable partner is the Center for Disease Control , which sends technical resources and staff to GOARN. The CDC also has a history of resource sharing and cooperation with WHO in order to combat disease. [7]
The WHO does not directly fund GOARN. Instead, GOARN members and outside fundraising that is carried out each time there is a new incidence is used to support the GOARN response. The Nuclear Threat Initiative provides GOARN with US$500,000 as a revolving fund, meant to be used for quickly mobilizing response teams. This is known as the WHO-NTI Global Emergency Response Fund, and must be repaid after withdrawal. The GOARN is effective at operating from a fairly small budget. [8]
GOARN has responded to over 120 occurrences in 85 countries and has deployed over 2,300 experts into the field. [6] Some examples of deployments are the SARS outbreak in Asia in 2003, Rift Valley fever, and the nipah virus around the Indian subcontinent. [9]
Since its creation, GOARN cooperated with various other organizations to control outbreaks and improve national capacity to respond to diseases. A brief history of GOARN's work against international diseases is as follows. In 2000–2003, GOARN primarily responded to outbreaks of diseases such as cholera, meningitis, and yellow fever in Africa. It supported field investigation and outbreak containment. In 2003, GOARN helped to deploy international teams and helped to coordinate the response against SARS. In 2004, the network was one of the first to deploy against H5N1 influenza. In April–July 2005, GOARN helped to control Marburg Hemorrhagic fever in Angola. It carried out some "risk assessment and preparedness missions" in 2006, along with some response to human bird flu. In 2008/2009, GOARN responded to cholera in Zimbabwe. [10]
GOARN played a role in containing the 2003 SARS outbreak in Asia. The network sent teams of experts in epidemiology, microbiology/virology, and infection control to Hanoi, Vietnam on March 14, 2003 and then Beijing China on March 25, 2003. GOARN assisted during this outbreak to not only study the outbreak and provide assistance, but also facilitate communication between the Department of Health (Hong Kong) and the WHO. [11]
Earliest signs of the outbreak in China were reported February 11–24, when multiple people were reported to contract the disease. WHO was notified February 28 and then directly notified GOARN March 13. The first members of a WHO/GOARN outbreak control team arrived in Hong Kong March 14, followed by another 5-person GOARN team 12 days later. This second team transitioned to Guangdong, where they investigated the earliest cases of SARS and conducted interviews with health staff. The outbreak was declared by WHO to be contained by July 5. [12]
Worldwide, GOARN carried out many of the operations for the initial response to SARS through the mobilization of field teams. Also through GOARN, the WHO developed many international networks to create tools and standards for containing the epidemic. These networks communicated data by teleconference and use of secure websites for sharing of information.
Besides these networks and field teams, GOARN also assisted nations by directly providing assistance to affected areas and improving their capacity to respond to such threats in the future. GOARN's role in the outbreak was recognized by the World Health Assembly during the 56th Assembly in resolution WHA56.29. [13]
On March 23, 2014, the first reports of Ebola in Guinea were reported by WHO's Regional Office in Africa. Five days later, the first GOARN team was sent to Guinea. This team found the situation to be quite severe and its findings were discussed in a press conference in Geneva April 8. [14]
In the third week of April, WHO collaborated with GOARN to send a new medical team trained in infection prevention/control and intensive care to Guinea's principal hospital, Donka Hospital. Two weeks later, on May 5, WHO deployed experts, thirty three of whom were from GOARN, to West Africa to assist in the response to the outbreak. The outbreak was detected to have spread to Sierre Leone later in the month. [14]
On June 23, a GOARN steering committee session sent a message to WHO requesting for WHO to lead the response more strongly because it was the only agency with the resources and staff to do so. [14]
Over the course of the outbreak, the network deployed 895 experts, including "doctors, nurses, infection control specialists, logisticians, laboratory specialists; communication, anthropology and social mobilisation experts, emergency management and public health professionals." The network is still involved in the response to Ebola. [15]
In Sierre Leone, GOARN has sent case management and laboratory experts from the International Centre for Diarrhoeal Disease Research, Bangladesh to help train the response capacity of health care and laboratory workers case management and diagnosis. [16]
In Northern Iraq, the Syrian Civil War displaced many refugees into the Kurdistan. The refugee camps suffer from poor sanitation, which has led to cholera outbreaks in the region in 2007 and 2012. GOARN, as per the request of the Ministry of Health of Iraq for support and training in outbreak response, deployed a multidisciplinary team of six experts to the North-Iraqi Dohuk and Erbil camps to assist with assessing the risk of cholera and other diseases as well as assisting MoH to prepare for response to the diseases. [17]
GOARN supported countries and various other outbreak control organizations to fight against the H1N1 outbreak in the US and Mexico. The GOARN alert and request for assistance started in Mexico April 24, 2009. Over the course of the outbreak, GOARN helped the Pan American Health Organization coordinate and exchange information with the CDC and Public Health Agency of Canada . It was provided with support and training from the Regional Office for the Western Pacific Response so that it could support regional offices in Manila and carry out field missions in Malaysia and Mongolia. The network carried out a joint training course with the Regional Office for the Eastern Mediterranean in Cairo. [18]
All in all, GOARN carried out 188 missions to 27 countries to strengthen international coordination between these organizations and to improve international capacity to respond to threats. Its activities consisted of assessment of the situation, communication between partners, infection control, laboratory diagnostic, and transportation of specimens. [18]
The Global Public Health Intelligence Network (GPHIN) is an electronic public health early warning system developed by Canada's Public Health Agency , and is part of the World Health Organization's (WHO) Global Outbreak Alert and Response Network (GOARN). This system monitors internet media, such as news wires and websites, in nine languages in order to help detect and report potential disease or other health threats around the world. [1] The system has been credited with detecting early signs of the 2009 swine flu pandemic in Mexico , Zika in West Africa , H5N1 in Iran , MERS and Ebola .
The system came to greater public awareness after it was revealed that Canada's Federal Government effectively shut it down in May 2019, ultimately preventing the system from providing an early warning of COVID-19 . In August 2020, the system began issuing alerts again.
Ronald St. John, then a government epidemiologist, created GPHIN in 1994 as a way to improve Canada's intelligence surrounding outbreaks. [2] Growing in parallel with ProMED-mail , [3] GPHIN was Canada's major contribution to the World Health Organization (WHO), which at one point credited the system with supplying 20 per cent of its "epidemiological intelligence" and described the system as "the foundation" of a global pandemic early warning system. [4] [5]
After the 2003 SARS outbreak , the system became central to Canada's pandemic preparedness. [4] The system, which eventually fell under the Centre for Emergency Preparedness and Response in the PHAC, [6] detected early signs of the 2009 swine flu pandemic in Mexico , Zika in West Africa , H5N1 in Iran , MERS and Ebola . [5] [4]
A July 2020 investigation by The Globe and Mail revealed that Canada's Federal Government effectively shutdown GPHIN in May 2019, ultimately preventing the system from providing an early warning of COVID-19 which originated in Wuhan , China . After the government directed for a more domestic focus, the Public Health Agency of Canada (PHAC) assigned employees to different tasks in the department. [5] The shutdown was gradual: in 2009, there were 877 alerts; 198 in 2013; and only 21 in 2018. The final alert came on May 24, 2019. [7] In August 2020, more than 400 days after going silent, the system began issuing alerts again. [8]
In early 2020 before COVID-19 was declared a pandemic , scientists at PHAC "were told to focus on official information coming out of China, rather than unofficial intelligence. Some said they struggled to convey urgent information up the chain of command." [2] Internal PHAC emails obtained by The Globe indicate that Sally Thornton, vice-president of the Health Security Infrastructure Branch, and Jim Harris, director-general of the Centre for Emergency Preparedness and Response, oversaw the decision that curtailed alerts. [9]
– Auditor General of Canada Karen Hogan , March 2021
Following The Globe and Mail's report, Canada's Auditor-General began an investigation into why the program was curtailed. [11] Released in March 2021, the Auditor-General's report described PHAC as ill-prepared for the pandemic. The report focused primarily on the silencing of GPHIN and the inaccurate risk assessments that replaced it. [10] In September 2020, Canada's Health Minister Patty Hajdu ordered an independent federal review to look into both the shutdown of the system along with allegations that some scientist's voices were marginalized. [4] Former national security adviser Margaret Bloodworth ; former deputy chief public health officer Paul Gully; and Mylaine Breton, Canada Research Chair in Clinical Governance on Primary Health Care led Hajdu's review. [12] [13] Canada's Chief Public Health Officer Theresa Tam announced her support of the review, [14] while Prime Minister Justin Trudeau issued blame to funding cuts made prior to 2015 by the previous Conservative government under Stephen Harper . [15]
PHAC's Centre for Emergency Preparedness and Response (CEPR) manages GPHIN. [16] [9] In October 2020, Jim Harris was director-general CEPR. [9]
In September 2020 Brigitte Diogo replaced Sally Thornton as vice-president of the Health Security Infrastructure Branch, the bureaucratic division overseeing GPHIN among other operations. [17] A week later, President Tina Namiesniowski announced her resignation, with Trudeau nominating Iain Stewart as her successor. [18]
In medicine and statistics , a gold standard test is usually the diagnostic test or benchmark that is the best available under reasonable conditions . [1] Other times, a gold standard is the most accurate test possible without restrictions .
Both meanings are different because for example, in medicine, dealing with conditions that would require an autopsy to have a perfect diagnosis, the gold standard test would be the best one that keeps the patient alive instead of the autopsy.
"Gold standard" can refer to the criteria by which scientific evidence is evaluated. For example, in resuscitation research, the "gold standard" test of a medication or procedure is whether or not it leads to an increase in the number of neurologically intact survivors that walk out of the hospital. [2] Other types of medical research might regard a significant decrease in 30-day mortality as the gold standard.
The AMA Style Guide has preferred the phrase Criterion Standard instead of "gold standard." Other journals have also issued mandates in their instructions for contributors.  For instance, Archives of biological Medicine and Rehabilitation specifies this usage. [3] When the criterion is a whole clinical testing procedure it is usually referred to as clinical case definition .
In practice however, the uptake of this term by authors, as well as enforcement by editorial staff, is notably poor, at least for AMA journals. [4]
A hypothetical ideal "gold standard" test has a sensitivity of 100% with respect to the presence of the disease (it identifies all individuals with a well defined disease process; it does not have any false-negative results) and a specificity of 100% (it does not falsely identify someone with a condition that does not have the condition; it does not have any false-positive results).  In practice, there are sometimes no true gold standard tests. [5]
As new diagnostic methods become available, the "gold standard" test may change over time.  For instance, for the diagnosis of aortic dissection , the gold standard test used to be the aortogram , which had a sensitivity as low as 83% and a specificity as low as 87%.  Since the advancements of magnetic resonance imaging , the magnetic resonance angiogram (MRA) has become the new gold standard test for aortic dissection, with a sensitivity of 95% and a specificity of 92%.  Before widespread acceptance of any new test, the former test retains its status as the "gold standard".
Because tests can be incorrect (yielding a false-negative or a false-positive ), results should be interpreted in the context of the history, physical findings, and other test results in the individual being tested. It is within this context that the sensitivity and specificity of the "gold standard" test is determined.
When the gold standard is not a perfect one, its sensitivity and specificity must be calibrated against more accurate tests or against the definition of the condition. [6] This calibration is especially important when a perfect test is available only by autopsy.
It is important to emphasize that a test has to meet some interobserver agreement, to avoid some bias induced by the study itself. [7]
Calibration errors can lead to misdiagnosis . [8]
Sometimes "gold standard test" refers to the best performing test available. In these cases, there is no other criterion against which it can be compared and it is equivalent to a definition. When referring to this meaning, gold standard tests are normally not performed at all. This is because the gold standard test may be difficult to perform or may be impossible to perform on a living person (i.e. the test is performed as part of an autopsy or may take too long for the results of the test to be available to be clinically useful).
Other times, "gold standard" does not refer to the best performing test available, but the best available under reasonable conditions. For example, in this sense, an MRI is the gold standard for brain tumour diagnosis, though it is not as good as a biopsy. In this case the sensitivity and specificity of the gold standard are not 100% and it is said to be an "imperfect gold standard" or "alloyed gold standard". [6]
The term ground truth refers to the underlying absolute state of information; the gold standard strives to represent the ground truth as closely as possible.  While the gold standard is a best effort to obtain the truth, ground truth is typically collected by direct observations. In machine learning and information retrieval , "ground truth" is the preferred term even when classifications may be imperfect; the gold standard is assumed to be the ground truth [ citation needed ] .
Good Epidemiological Practices or Good Epidemiology Practices ( GEP ) was a set of guidelines produced by the U.S. Chemical Manufacturers Association (CMA) in 1991 to improve epidemiologic research practices. [1] It was then adopted by the tobacco industry around 1993 as part of its "sound science" program to counter criticisms of the industry on health and environmental issues such as secondhand smoke . [2] It failed to make much impact on the US and European regulators, but may have had more influence in its later manifestations in Asia and particularly China.
The complexities of creating Good Epidemiological Practices developments have been examined by University of San Francisco researchers Stanton Glantz and Elisa K. Ong in papers such as "Tobacco Industry Efforts to Subvert the IARC's Second hand smoke study" published in The Lancet . [3] They argued that these were industry programs designed to manipulate the scientific standards of proof, and an attempt to set risk-evaluation standards that could be forced through by legislation, and imposed on the agencies ( EPA , OSHA , etc.) to hamstring the agencies' ability to regulate. [2]
In February 1993, an executive of Philip Morris USA , Ellen Merlo, wrote a memo to her CEO William Campbell outlining their strategies to discredit the 1992 report by the US Environmental Protection Agency (EPA) that identified secondhand smoke as a Group A human carcinogen , and to counter what she described as " junk science ". [2] Their public relations firm, APCO Associates, launched a "sound science" coalition in the United States, and The Advancement of Sound Science Coalition (TASSC) was formed three months later in May 1993, with Garrey Carruthers the chairman. [4] [5] George L. Carlo , a science entrepreneur who worked with Federal Focus Inc., prepared one of the early draft documents. [6] [7]
At this time the legislative [ which? ] and public activist [ who? ] attacks on the tobacco industry had escalated, and they [ who? ] blamed this on the "twisted science" used by the EPA. [8] In Europe the UN's International Agency for Research on Cancer (IARC) was about to release a similar report, which the tobacco industry also needed to counter.
Helmut Reif, Tom Borelli and Mitch Ritter at Philip Morris's Science & Technology division thought the CMA's GEP could be improved to help PM develop standards for secondhand smoke epidemiological studies. This initiative to "constructively improve" epidemiology would enlist the assistance of the industry's consultants, and they could then run an "offensive strategy"—essentially another proactive effort by the industry to shape the scientific process. [9] [10]
Joanna Sullivan, Philip Morris's main PR in Europe, advised her American superiors [ who? ] (June 1994) on how GEP might be used to combat the IARC's report, since this would give United Nations weight to the claim that second-hand smoke was potentially cancerous to non-smokers.  By establishing the standards of epidemiology and by using their influence in Congress to have GEP mandated by legislation, they hoped to design a set of established procedural standards for epidemiology.
The innovation here, was that they [ who? ] aimed to attack the techniques relied on by the EPA and IARC scientists. In the past, they [ who? ] had focussed attacks on specific adverse scientific findings and reports. GEP was designed to directed an attack on the techniques of epidemiology itself. [11]
They [ who? ] may have had little hope that this would be widely accepted by the majority of epidemiologists or by the regulators [ which? ] , but it made possible a vigorous public relations campaign in the media to challenge and denigrate any adverse findings. [12]
Initially Philip Morris utilised a small panel of trusted epidemiologists [ who? ] who regularly serviced the industry when it needed to defend against some adverse finding. [13] They were requested to improve on the Chemical Manufacturer's Association's GEP. Later they extended this mandate by conducting seminars on GEP and good risk-assessment practice around the world [ where? ] .  The tobacco industry itself needed to remain invisible and so they needed the front provided by third-party think-tanks and old allies.
The main organisation hired to run the GEP program was Federal Focus Inc., a PR company set up during the second term of the Reagan Administration by two top Agency officials, Thorne Auchter , who had been the Director of the OSHA (Occupational Safety & Health Administration) and James J. Tozzi who had been Reagan's "Deregulation Czar" as head of the OIRA (Office of Information and Regulatory Affairs), at the OMB (Office of Management & Budget). These two and the head of the EPA ( Anne Gorsuch Burford ) were among the 20 or so top Agency executives forced (or encouraged) to resign over Reagan's regulatory scandals. [14]
Jim Tozzi's technique at the OIRA was to block regulators [ which? ] from regulating by denying their agencies funding unless they cancelled one regulation before adding another. (Tozzi and Auchter also ran Multinational Business Services , and the Institute for Regulatory Policy ).
The main tobacco law firm, Covington & Burling (C&B) was also involved both in promoting the idea and in recruiting scientists [ who? ] willing to participate, usually only when assured that payment would be laundered through the law firm.  C&B lawyer John Rupp initiated similar activities in Italy and Germany. The total expenses for this program in 1997 were about $300,000.
The GEP program "experts" that Auchter and Tozzi enlisted were pre-screened by Philip Morris Issues Managers and Federal Focus, and the results they produced were pre-tested at a number of conferences (which often didn't carry the GEP name). The first, known as the Lansdowne Conference, was held at a US conference center with only a few of their most trusted scientist helpers. They then widened the advisory pool and held a London Conference where Philip Morris opened the operation to British-American Tobacco and the other companies. [15] They established what became known as the London Principles. [16] A German conference soon followed.
Generally GEP failed to influence policy makers in America and Europe [ where? ] , so it was tried in Asia through conferences of the Asian WhiteCoats group (collectively known as 'ARTIST') and with other recruits in Kuala Lumpur, Hong-Kong, Yokohama,  Beijing, and then Guangzhou. [17] [18]
Ted Sanders, who ran the Scientific Affairs operations for Philip Morris in Europe, provides us [ who? ] with an excellent overview of how the Philip Morris GEP unravelled in a long letter of complaint to Cathy Ellis, his executive overseer in Brussels (Apr 3 1998). [19]
In survival analysis , the hazard ratio ( HR ) is the ratio of the hazard rates corresponding to the conditions described by two levels of an explanatory variable. For example, in a drug study, the treated population may die at twice the rate per unit time of the control population. The hazard ratio would be 2, indicating higher hazard of death from the treatment.
Hazard ratios differ from relative risks (RRs) and odds ratios (ORs) in that RRs and ORs are cumulative over an entire study, using a defined endpoint, while HRs represent instantaneous risk over the study time period, or some subset thereof. Hazard ratios suffer somewhat less from selection bias with respect to the endpoints chosen and can indicate risks that happen before the endpoint.
Regression models are used to obtain hazard ratios and their confidence intervals . [1]
The instantaneous hazard rate is the limit of the number of events per unit time divided by the number at risk, as the time interval approaches 0.
where N ( t ) is the number at risk at the beginning of an interval. A hazard is the probability that a patient fails between t {\displaystyle t} and t + Δ t {\displaystyle t+\Delta t} , given that he has survived up to time t {\displaystyle t} , divided by Δ t {\displaystyle \Delta t} , as Δ t {\displaystyle \Delta t} approaches zero. [2]
The hazard ratio is the effect on this hazard rate of a difference, such as group membership (for example, treatment or control , male or female), as estimated by regression models that treat the log of the HR as a function of a baseline hazard h 0 ( t ) {\displaystyle h_{0}(t)} and a linear combination of explanatory variables:
Such models are generally classed proportional hazards regression models; the best known being the Cox semiparametric proportional hazards model , [1] [3] and the exponential, Gompertz and Weibull parametric models.
For two groups that differ only in treatment condition, the ratio of the hazard functions is given by e β {\displaystyle e^{\beta }} , where β {\displaystyle \beta } is the estimate of treatment effect derived from the regression model. This hazard ratio, that is, the ratio between the predicted hazard for a member of one group and that for a member of the other group, is given by holding everything else constant, i.e. assuming proportionality of the hazard functions. [2]
For a continuous explanatory variable, the same interpretation applies to a unit difference. Other HR models have different formulations and the interpretation of the parameter estimates differs accordingly.
In its simplest form, the hazard ratio can be interpreted as the chance of an event occurring in the treatment arm divided by the chance of the event occurring in the control arm, or vice versa, of a study. The resolution of these endpoints are usually depicted using Kaplan–Meier survival curves . These curves relate the proportion of each group where the endpoint has not been reached. The endpoint could be any dependent variable associated with the covariate (independent variable), e.g. death, remission of disease or contraction of disease. The curve represents the odds of an endpoint having occurred at each point in time (the hazard). The hazard ratio is simply the relationship between the instantaneous hazards in the two groups and represents, in a single number, the magnitude of distance between the Kaplan–Meier plots. [5]
Hazard ratios do not reflect a time unit of the study. The difference between hazard-based and time-based measures is akin to the difference between the odds of winning a race and the margin of victory. [1] When a study reports one hazard ratio per time period, it is assumed that difference between groups was proportional. Hazard ratios become meaningless when this assumption of proportionality is not met. [5] [ page needed ]
If the proportional hazard assumption holds, a hazard ratio of one means equivalence in the hazard rate of the two groups, whereas a hazard ratio other than one indicates difference in hazard rates between groups. The researcher indicates the probability of this sample difference being due to chance by reporting the probability associated with some test statistic . [6] For instance, the β {\displaystyle \beta } from the Cox-model or the log-rank test might then be used to assess the significance of any differences observed in these survival curves. [7]
Conventionally, probabilities lower than 0.05 are considered significant and researchers provide a 95% confidence interval for the hazard ratio, e.g. derived from the standard deviation of the Cox-model regression coefficient , i.e. β {\displaystyle \beta } . [7] [8] Statistically significant hazard ratios cannot include unity (one) in their confidence intervals. [5]
The proportional hazards assumption for hazard ratio estimation is strong and often unreasonable. [9] Complications , adverse effects and late effects are all possible causes of change in the hazard rate over time. For instance, a surgical procedure may have high early risk, but excellent long term outcomes.
If the hazard ratio between groups remain constant, this is not a problem for interpretation. However, interpretation of hazard ratios become impossible when selection bias exists between groups. For instance, a particularly risky surgery might result in the survival of a systematically more robust group who would have fared better under any of the competing treatment conditions, making it look as if the risky procedure was better. Follow-up time is also important. A cancer treatment associated with better remission rates might on follow-up be associated with higher relapse rates. The researchers' decision about when to follow up is arbitrary and may lead to very different reported hazard ratios. [10]
Hazard ratios are often treated as a ratio of death probabilities. [2] For example, a hazard ratio of 2 is thought to mean that a group has twice the chance of dying than a comparison group. In the Cox-model, this can be shown to translate to the following relationship between group survival functions : S 1 ( t ) = S 0 ( t ) r {\displaystyle S_{1}(t)=S_{0}(t)^{r}} (where r is the hazard ratio). [2] Therefore, with a hazard ratio of 2, if S 0 ( t ) = 0.2 {\displaystyle S_{0}(t)=0.2} (20% survived at time t ), S 1 ( t ) = 0.2 2 = 0.04 {\displaystyle S_{1}(t)=0.2^{2}=0.04} (4% survived at t ). The corresponding death probabilities are 0.8 and 0.96. [9] It should be clear that the hazard ratio is a relative measure of effect and tells us nothing about absolute risk. [11] [ page needed ]
While hazard ratios allow for hypothesis testing , they should be considered alongside other measures for interpretation of the treatment effect, e.g. the ratio of median times (median ratio) at which treatment and control group participants are at some endpoint. If the analogy of a race is applied, the hazard ratio is equivalent to the odds that an individual in the group with the higher hazard reaches the end of the race first. The probability of being first can be derived from the odds, which is the probability of being first divided by the probability of not being first:
In the previous example, a hazard ratio of 2 corresponds to a 67% chance of an early death. The hazard ratio does not convey information about how soon the death will occur. [1]
Treatment effect depends on the underlying disease related to survival function, not just the hazard ratio. Since the hazard ratio does not give us direct time-to-event information, researchers have to report median endpoint times and calculate the median endpoint time ratio by dividing the control group median value by the treatment group median value.
While the median endpoint ratio is a relative speed measure, the hazard ratio is not. [1] The relationship between treatment effect and the hazard ratio is given as e β {\displaystyle e^{\beta }} . A statistically important, but practically insignificant effect can produce a large hazard ratio, e.g. a treatment increasing the number of one-year survivors in a population from one in 10,000 to one in 1,000 has a hazard ratio of 10. It is unlikely that such a treatment would have had much impact on the median endpoint time ratio, which likely would have been close to unity, i.e. mortality was largely the same regardless of group membership and clinically insignificant .
By contrast, a treatment group in which 50% of infections are resolved after one week (versus 25% in the control) yields a hazard ratio of two. If it takes ten weeks for all cases in the treatment group and half of cases in the control group to resolve, the ten-week hazard ratio remains at two, but the median endpoint time ratio is ten, a clinically significant difference.
Health services research ( HSR ) became a burgeoning field in North America in the 1960s, when scientific information and policy deliberation began to coalesce. [1] Also known as health systems research or health policy and systems research ( HPSR ), is a multidisciplinary scientific field that examines how people get access to health care practitioners and health care services, how much care costs, and what happens to patients as a result of this care. [2] Health Services Research utilizes all qualitative and quantitative methods across the board to ask questions of the healthcare system. It focuses on performance, quality, effectiveness and efficiency of health care services as they relate to health problems of individuals and populations, as well as health care systems. Health Services Research addresses wide-ranging topics of structure, processes, and organization of health care services; their use and people's access to services; efficiency and effectiveness of health care services; the quality of healthcare services and its relationship to health status, and; the uses of medical knowledge.
Studies in HSR investigate how social factors, health policy , financing systems, organizational structures and processes, medical technology , and personal behaviors affect access to health care , the quality and cost of health care, and quantity and quality of life . Compared with medical research , HSR is a relatively young science that developed through the bringing together of social science perspectives with the contributions of individuals and institutions engaged in delivering health services. [3]
The primary goals of health services research are to identify the most effective ways to organize, manage, finance, and deliver high quality care; reduce medical errors; and improve patient safety . [4] HSR is more concerned with delivery and high quality access to care, in contrast to medical research, which focuses on the development and evaluation of clinical treatments.
Health services researchers come from a variety of specializations, including geography , nursing , economics , political science , epidemiology , public health , medicine , biostatistics , operations , management , engineering , pharmacy , psychology , usability and user experience design . [5] While health services research is grounded in theory, its underlying aim is to perform research that can be applied by physicians , nurses , health managers and administrators , and other people who make decisions or deliver care in the health care system .  For example, the application of epidemiological methods to the study of health services by managers is a type of health services research that can be described as Managerial epidemiology .
Approaches to HSR include: [3]
Many data and information sources are used to conduct health services research, such as population and health surveys, clinical administrative records, health care program and financial administrative records, vital statistics records (births and deaths), and other special studies.
Several government, academic and non-government agencies conduct or sponsor health services research, notably the Canadian Institute for Health Information and the Canadian Institutes of Health Research (i.e. the third pillar: "research respecting health systems and services"). Others include the Institute for Clinical Evaluative Sciences (ICES) in Toronto , and the Canadian Collaborative Study of Hip Fractures .
There are some universities which train in health services research. Atlantic Regional Training Centre
Several registries are available for research use, such as Danish Twin Register or Danish Cancer Register. [6]
Public Health Research Laboratory.
Several governmental agencies exist that sponsor or support HSR, with their remits set by central and devolved governments.  These include the National Institute for Health Research (NIHR) and its constituent infrastructure (including the CLAHRC programme); [7] Healthcare Improvement Scotland; [8] Health and Care Research Wales; [9] and Health and Social Care Research and Development [10] Many universities have HSR units, a web search can find these with relative ease.
Claims data on US Medicare and Medicaid beneficiaries are available for analysis. Data is divided into public data available to any entity and research data available only to qualified researchers. The US's Centers for Medicare and Medicaid Services (CMS) delegates some data export functions to a Research Data Assistance Center. [11]
23 Claims data from various states that are not limited to any particular insurer are also available for analysis via AHRQ 's HCUP project. [12]
Colloquially, health services research departments are often referred to as "shops"; in contrast to basic science research "labs".  Broadly, these shops are hosted by three general types of institutions—government, academic, or non-governmental think tanks or professional societies.
Government Sponsored
University Sponsored
Think Tank or Professional Society Sponsored

Publications:
Conferences and events:


HealthMap is a freely accessible, automated electronic information system for monitoring, organizing, and visualizing reports of global disease outbreaks according to geography, time, and infectious disease agent. [1] [2] In operation since September 2006, and created by John Brownstein , PhD and Clark Freifeld , PhD, HealthMap acquires data from a variety of freely available electronic media sources (e.g. ProMED-mail , Eurosurveillance , Wildlife Disease Information Node ) to obtain a comprehensive view of the current global state of infectious diseases. [3] [4]
Users of HealthMap come from a variety of organizations including state and local public health agencies, the World Health Organization (WHO), the US Centers for Disease Control and Prevention , and the European Centre for Disease Prevention and Control . HealthMap is used both as an early detection system and supports situational awareness by providing current, highly local information about outbreaks, even from areas relatively invisible to traditional global public health efforts. Currently, HealthMap monitors information sources in English, Chinese, Spanish, Russian, French, Portuguese, and Arabic. [5]
In March 2014, the Healthmap software tracked early press and social media reports of a hemorrhagic fever in West Africa, subsequently identified by WHO as Ebola . The HealthMap team subsequently created a dedicated HealthMap visualization at healthmap.org/ebola . [6] [7]
The healthy user bias or healthy worker bias is a bias that can damage the validity of epidemiologic studies testing the efficacy of particular therapies or interventions.
Specifically, it is a sampling bias or selection bias : the kind of subjects that voluntarily enroll in a clinical trial and actually follow the experimental regimen are not representative of the general population. People who volunteer for a study can be expected, on average, to be healthier than people who don't volunteer, as they are concerned for their health and are predisposed to follow medical advice, [1] both factors that would aid one's health. In a sense, being healthy or active about one's health is a precondition for becoming a subject of the study, an effect that can appear under other conditions such as studying particular groups of workers.  For example, someone in ill health is unlikely to have a job as manual laborer .  As a result, studies of manual laborers are studies of people who are currently healthy enough to engage in manual labor, rather than studies of people who would do manual labor if they were healthy enough.
Herd immunity (also called herd effect , community immunity , population immunity , or mass immunity ) is a form of indirect protection from infectious disease that can occur with some diseases when a sufficient percentage of a population has become immune to an infection, whether through vaccination or previous infections, thereby reducing the likelihood of infection for individuals who lack immunity. [1] [2] [3] Immune individuals are unlikely to contribute to disease transmission , disrupting chains of infection, which stops or slows the spread of disease. [4] The greater the proportion of immune individuals in a community, the smaller the probability that non-immune individuals will come into contact with an infectious individual. [1]
Individuals can become immune by recovering from an earlier infection or through vaccination . [4] Some individuals cannot become immune because of medical conditions, such as an immunodeficiency or immunosuppression , and for this group herd immunity is a crucial method of protection. [5] [6] Once the herd immunity threshold has been reached, disease gradually disappears from a population. [6] This elimination, if achieved worldwide, may result in the permanent reduction in the number of infections to zero, called eradication . [7] Herd immunity created via vaccination contributed to the eventual eradication of smallpox in 1977 and has contributed to the reduction of other diseases. [8] Herd immunity applies only to contagious disease, meaning that it is transmitted from one individual to another. [6] Tetanus , for example, is infectious but not contagious, so herd immunity does not apply. [5]
Herd immunity was recognized as a naturally occurring phenomenon in the 1930s when it was observed that after a significant number of children had become immune to measles , the number of new infections temporarily decreased. [9] Mass vaccination to induce herd immunity has since become common and proved successful in preventing the spread of many infectious diseases. [10] Opposition to vaccination has posed a challenge to herd immunity, allowing preventable diseases to persist in or return to populations with inadequate vaccination rates. [11] [12] [13]
The exact herd immunity threshold (HIT) varies depending on the basic reproduction number of the disease. An example of a disease with a high threshold is the measles , with a HIT exceeding 95%. [14]
Some individuals either cannot develop immunity after vaccination or for medical reasons cannot be vaccinated. [15] [5] [16] Newborn infants are too young to receive many vaccines, either for safety reasons or because passive immunity renders the vaccine ineffective. [17] Individuals who are immunodeficient due to HIV/AIDS , lymphoma , leukemia , bone marrow cancer, an impaired spleen , chemotherapy , or radiotherapy may have lost any immunity that they previously had and vaccines may not be of any use for them because of their immunodeficiency. [5] [16] [17] [18]
A portion of those vaccinated may not develop long-term immunity. [1] [19] [20] Vaccine contraindications may prevent certain individuals from being vaccinated. [16] In addition to not being immune, individuals in one of these groups may be at a greater risk of developing complications from infection because of their medical status, but they may still be protected if a large enough percentage of the population is immune. [5] [16] [20] [21]
High levels of immunity in one age group can create herd immunity for other age groups. [8] Vaccinating adults against pertussis reduces pertussis incidence in infants too young to be vaccinated, who are at the greatest risk of complications from the disease. [22] [23] This is especially important for close family members, who account for most of the transmissions to young infants. [8] [20] In the same manner, children receiving vaccines against pneumococcus reduces pneumococcal disease incidence among younger, unvaccinated siblings. [24] Vaccinating children against pneumococcus and rotavirus has had the effect of reducing pneumococcus - and rotavirus -attributable hospitalizations for older children and adults, who do not normally receive these vaccines. [24] [25] [26] Influenza (flu) is more severe in the elderly than in younger age groups, but influenza vaccines lack effectiveness in this demographic due to a waning of the immune system with age. [8] [27] The prioritization of school-age children for seasonal flu immunization, which is more effective than vaccinating the elderly, however, has been shown to create a certain degree of protection for the elderly. [8] [27]
For sexually transmitted infections (STIs), high levels of immunity in one sex induces herd immunity for both sexes. [10] [28] [29] Vaccines against STIs that are targeted at one sex result in significant declines in STIs in both sexes if vaccine uptake in the target sex is high. [28] [29] [30] Herd immunity from female vaccination does not, however, extend to homosexual males. [29] If vaccine uptake among the target sex is low, then the other sex may need to be immunized so that the target sex can be sufficiently protected. [28] [29] High-risk behaviors make eliminating STIs difficult since even though most infections occur among individuals with moderate risk, the majority of transmissions occur because of individuals who engage in high-risk behaviors. [10] For these reasons, in certain populations it may be necessary to immunize high-risk persons or individuals of both sexes to establish herd immunity. [10] [29]
Herd immunity itself acts as an evolutionary pressure on pathogens, influencing viral evolution by encouraging the production of novel strains, referred to as escape mutants, that are able to evade herd immunity and infect previously immune individuals. [31] [32] The evolution of new strains is known as serotype replacement, or serotype shifting, as the prevalence of a specific serotype declines due to high levels of immunity, allowing other serotypes to replace it. [33] [34]
At the molecular level, viruses escape from herd immunity through antigenic drift , which is when mutations accumulate in the portion of the viral genome that encodes for the virus's surface antigen , typically a protein of the virus capsid , producing a change in the viral epitope . [35] [36] Alternatively, the reassortment of separate viral genome segments, or antigenic shift , which is more common when there are more strains in circulation, can also produce new serotypes . [31] [37] When either of these occur, memory T cells no longer recognize the virus, so people are not immune to the dominant circulating strain. [36] [37] For both influenza and norovirus , epidemics temporarily induce herd immunity until a new dominant strain emerges, causing successive waves of epidemics. [35] [37] As this evolution poses a challenge to herd immunity, broadly neutralizing antibodies and "universal" vaccines that can provide protection beyond a specific serotype are in development. [32] [38] [39]
Initial vaccines against Streptococcus pneumoniae significantly reduced nasopharyngeal carriage of vaccine serotypes (VTs), including antibiotic-resistant types, [24] [40] only to be entirely offset by increased carriage of non-vaccine serotypes (NVTs). [24] [33] [34] This did not result in a proportionate increase in disease incidence though, since NVTs were less invasive than VTs. [33] Since then, pneumococcal vaccines that provide protection from the emerging serotypes have been introduced and have successfully countered their emergence. [24] The possibility of future shifting remains, so further strategies to deal with this include expansion of VT coverage and the development of vaccines that use either killed whole-cells , which have more surface antigens, or proteins present in multiple serotypes. [24] [41]
If herd immunity has been established and maintained in a population for a sufficient time, the disease is inevitably eliminated – no more endemic transmissions occur. [6] If elimination is achieved worldwide and the number of cases is permanently reduced to zero, then a disease can be declared eradicated. [7] Eradication can thus be considered the final effect or end-result of public health initiatives to control the spread of infectious disease. [7] [8]
The benefits of eradication include ending all morbidity and mortality caused by the disease, financial savings for individuals, health care providers, and governments, and enabling resources used to control the disease to be used elsewhere. [7] To date, two diseases have been eradicated using herd immunity and vaccination: rinderpest and smallpox . [1] [8] [42] Eradication efforts that rely on herd immunity are currently underway for poliomyelitis , though civil unrest and distrust of modern medicine have made this difficult. [1] [43] Mandatory vaccination may be beneficial to eradication efforts if not enough people choose to get vaccinated. [44] [45] [46] [47]
Herd immunity is vulnerable to the free rider problem . [48] Individuals who lack immunity, particularly those who choose not to vaccinate, free ride off the herd immunity created by those who are immune. [48] As the number of free riders in a population increases, outbreaks of preventable diseases become more common and more severe due to loss of herd immunity. [11] [12] [13] [45] [47] Individuals may choose to free ride for a variety of reasons, including the belief that vaccines are ineffective, [49] or that the risks associated with vaccines are greater than those associated with infection, [1] [12] [13] [49] mistrust of vaccines or public health officials, [50] bandwagoning or groupthinking , [45] [51] social norms or peer pressure , [49] and religious beliefs. [12] Certain individuals are more likely to choose not to receive vaccines if vaccination rates are high enough to convince a person that he or she may not need to be vaccinated, since a sufficient percentage of others are already immune. [1] [47]
Individuals who are immune to a disease act as a barrier in the spread of disease, slowing or preventing the transmission of disease to others. [4] An individual's immunity can be acquired via a natural infection or through artificial means, such as vaccination. [4] When a critical proportion of the population becomes immune, called the herd immunity threshold (HIT) or herd immunity level (HIL), the disease may no longer persist in the population, ceasing to be endemic . [6] [31]
The theoretical basis for herd immunity generally assumes that vaccines induce solid immunity, that populations mix at random, that the pathogen does not evolve to evade the immune response, and that there is no non-human vector for the disease. [1]
The critical value, or threshold, in a given population, is the point where the disease reaches an endemic steady state , which means that the infection level is neither growing nor declining exponentially . This threshold can be calculated from the effective reproduction number R e , which is obtained by taking the product of the basic reproduction number R 0 , the average number of new infections caused by each case in an entirely susceptible population that is homogeneous, or well-mixed, meaning each individual can come into contact with every other susceptible individual in the population, [10] [31] [44] and S , the proportion of the population who are susceptible to infection, and setting this product to be equal to 1:
S can be rewritten as (1 − p ), where p is the proportion of the population that is immune so that p + S equals one. Then, the equation can be rearranged to place p by itself as follows:
With p being by itself on the left side of the equation, it can be renamed as p c , representing the critical proportion of the population needed to be immune to stop the transmission of disease, which is the same as the "herd immunity threshold" HIT. [10] R 0 functions as a measure of contagiousness, so low R 0 values are associated with lower HITs, whereas higher R 0 s result in higher HITs. [31] [44] For example, the HIT for a disease with an R 0 of 2 is theoretically only 50%, whereas a disease with an R 0 of 10 the theoretical HIT is 90%. [31]
When the effective reproduction number R e of a contagious disease is reduced to and sustained below 1 new individual per infection, the number of cases occurring in the population gradually decreases until the disease has been eliminated. [10] [31] [58] If a population is immune to a disease in excess of that disease's HIT, the number of cases reduces at a faster rate, outbreaks are even less likely to happen, and outbreaks that occur are smaller than they would be otherwise. [1] [10] If the effective reproduction number increases to above 1, then the disease is neither in a steady state nor decreasing in incidence , but is actively spreading through the population and infecting a larger number of people than usual. [45] [58]
An assumption in these calculations is that populations are homogeneous, or well-mixed, meaning that every individual comes into contact with every other individual, when in reality populations are better described as social networks as individuals tend to cluster together, remaining in relatively close contact with a limited number of other individuals. In these networks, transmission only occurs between those who are geographically or physically close to one another. [1] [44] [45] The shape and size of a network is likely to alter a disease's HIT, making incidence either more or less common. [31] [44]
In heterogeneous populations, R 0 is considered to be a measure of the number of cases generated by a "typical" infectious person, which depends on how individuals within a network interact with each other. [1] Interactions within networks are more common than between networks, in which case the most highly connected networks transmit disease more easily, resulting in a higher R 0 and a higher HIT than would be required in a less connected network. [1] [45] In networks that either opt not to become immune or are not immunized sufficiently, diseases may persist despite not existing in better-immunized networks. [45]
The cumulative proportion of individuals who get infected during the course of a disease outbreak can exceed the HIT. This is because the HIT does not represent the point at which the disease stops spreading, but rather the point at which each infected person infects fewer than one additional person on average. When the HIT is reached, the number of additional infections does not immediately drop to zero. The excess of the cumulative proportion of infected individuals over the theoretical HIT is known as the overshoot . [59] [60] [61]
The primary way to boost levels of immunity in a population is through vaccination. [1] [62] Vaccination is originally based on the observation that milkmaids exposed to cowpox were immune to smallpox, so the practice of inoculating people with the cowpox virus began as a way to prevent smallpox. [43] Well-developed vaccines provide protection in a far safer way than natural infections, as vaccines generally do not cause the diseases they protect against and severe adverse effects are significantly less common than complications from natural infections. [63] [64]
The immune system does not distinguish between natural infections and vaccines, forming an active response to both, so immunity induced via vaccination is similar to what would have occurred from contracting and recovering from the disease. [65] To achieve herd immunity through vaccination, vaccine manufacturers aim to produce vaccines with low failure rates, and policy makers aim to encourage their use . [62] After the successful introduction and widespread use of a vaccine, sharp declines in the incidence of diseases it protects against can be observed, which decreases the number of hospitalizations and deaths caused by such diseases. [66] [67] [68]
Assuming a vaccine is 100% effective, then the equation used for calculating the herd immunity threshold can be used for calculating the vaccination level needed to eliminate a disease, written as V c . [1] Vaccines are usually imperfect however, so the effectiveness, E , of a vaccine must be accounted for:
From this equation, it can be observed that if E is less than (1 − 1/ R 0 ), then it is impossible to eliminate a disease, even if the entire population is vaccinated. [1] Similarly, waning vaccine-induced immunity, as occurs with acellular pertussis vaccines , requires higher levels of booster vaccination to sustain herd immunity. [1] [22] If a disease has ceased to be endemic to a population, then natural infections no longer contribute to a reduction in the fraction of the population that is susceptible. Only vaccination contributes to this reduction. [10] The relation between vaccine coverage and effectiveness and disease incidence can be shown by subtracting the product of the effectiveness of a vaccine and the proportion of the population that is vaccinated, p v , from the herd immunity threshold equation as follows:
It can be observed from this equation that, all other things being equal (" ceteris paribus "), any increase in either vaccine coverage or vaccine effectiveness, including any increase in excess of a disease's HIT, further reduces the number of cases of a disease. [10] The rate of decline in cases depends on a disease's R 0 , with diseases with lower R 0 values experiencing sharper declines. [10]
Vaccines usually have at least one contraindication for a specific population for medical reasons, but if both effectiveness and coverage are high enough then herd immunity can protect these individuals. [15] [18] [21] Vaccine effectiveness is often, but not always, adversely affected by passive immunity, [69] [70] so additional doses are recommended for some vaccines while others are not administered until after an individual has lost his or her passive immunity. [17] [21]
Individual immunity can also be gained passively, when antibodies to a pathogen are transferred from one individual to another. This can occur naturally, whereby maternal antibodies, primarily immunoglobulin G antibodies, are transferred across the placenta and in colostrum to fetuses and newborns. [71] [72] Passive immunity can also be gained artificially, when a susceptible person is injected with antibodies from the serum or plasma of an immune person. [65] [73]
Protection generated from passive immunity is immediate, but wanes over the course of weeks to months, so any contribution to herd immunity is temporary. [6] [65] [74] For diseases that are especially severe among fetuses and newborns, such as influenza and tetanus, pregnant women may be immunized in order to transfer antibodies to the child. [15] [75] [76] In the same way, high-risk groups that are either more likely to experience infection, or are more likely to develop complications from infection, may receive antibody preparations to prevent these infections or to reduce the severity of symptoms. [73]
Herd immunity is often accounted for when conducting cost–benefit analyses of vaccination programs. It is regarded as a positive externality of high levels of immunity, producing an additional benefit of disease reduction that would not occur had no herd immunity been generated in the population. [77] [78] Therefore, herd immunity's inclusion in cost–benefit analyses results both in more favorable cost-effectiveness or cost–benefit ratios, and an increase in the number of disease cases averted by vaccination. [78] Study designs done to estimate herd immunity's benefit include recording disease incidence in households with a vaccinated member, randomizing a population in a single geographic area to be vaccinated or not, and observing the incidence of disease before and after beginning a vaccination program. [79] From these, it can be observed that disease incidence may decrease to a level beyond what can be predicted from direct protection alone, indicating that herd immunity contributed to the reduction. [79] When serotype replacement is accounted for, it reduces the predicted benefits of vaccination. [78]
The term "herd immunity" was coined in 1923. [80] Herd immunity was first recognized as a naturally occurring phenomenon in the 1930s when A. W. Hedrich published research on the epidemiology of measles in Baltimore , and took notice that after many children had become immune to measles, the number of new infections temporarily decreased, including among susceptible children. [81] [9] In spite of this knowledge, efforts to control and eliminate measles were unsuccessful until mass vaccination using the measles vaccine began in the 1960s. [9] Mass vaccination, discussions of disease eradication, and cost–benefit analyses of vaccination subsequently prompted more widespread use of the term herd immunity . [1] In the 1970s, the theorem used to calculate a disease's herd immunity threshold was developed. [1] During the smallpox eradication campaign in the 1960s and 1970s, the practice of ring vaccination , to which herd immunity is integral, began as a way to immunize every person in a "ring" around an infected individual to prevent outbreaks from spreading. [82]
Since the adoption of mass and ring vaccination, complexities and challenges to herd immunity have arisen. [1] [62] Modeling of the spread of infectious disease originally made a number of assumptions, namely that entire populations are susceptible and well-mixed, which is not the case in reality, so more precise equations have been developed. [1] In recent decades, it has been recognized that the dominant strain of a microorganism in circulation may change due to herd immunity, either because of herd immunity acting as an evolutionary pressure or because herd immunity against one strain allowed another already-existing strain to spread. [35] [34] Emerging or ongoing fears and controversies about vaccination have reduced or eliminated herd immunity in certain communities, allowing preventable diseases to persist in or return to these communities. [11] [12] [13]
The epidemiology of herpes simplex is of substantial epidemiologic and public health interest. Worldwide, the rate of infection with herpes simplex virus —counting both HSV-1 and HSV-2—is around 90%. [1] Although many people infected with HSV develop labial or genital lesions ( herpes simplex ), the majority are either undiagnosed or display no physical symptoms—individuals with no symptoms are described as asymptomatic or as having subclinical herpes. [2]
In many infections, the first symptom a person will have of their own infection is the horizontal transmission to a sexual partner or the vertical transmission of neonatal herpes to a newborn at term. Since most asymptomatic individuals are unaware of their infection, they are considered at high risk for spreading HSV.  Many studies have been performed around the world to estimate the numbers of individuals infected with HSV-1 and HSV-2 by determining if they have developed antibodies against either viral species . [3]
This information provides population prevalence of HSV viral infections in individuals with or without active disease. Note that there are population subgroups that are more
vulnerable for HSV infections, such as cancer chemotherapy patients. [4]
Large differences in HSV-1 seroprevalence are seen in different European countries . HSV-1 seroprevalence is high in Bulgaria (83.9%) and The Czech Republic (80.6%), and lower in Belgium (67.4%), The Netherlands (56.7%), and Finland (52.4%). [7]
The typical age at which HSV-1 infection is acquired ranges from 5 to 9 years in Central and Eastern European countries like Bulgaria and the Czech Republic, to over 25 years of age in Northern European countries such as Finland, The Netherlands, Germany, and England and Wales . Young adults in Northern European countries are less likely to be infected with HSV-1.  European women are more likely to be HSV-1 seropositive than men. [7]
HSV-2 seropositivity is widely distributed in Europeans older than 12, although there are large differences in the percentage of the population exposed to HSV-2. Bulgaria has a high (23.9%) HSV-2 seroprevalence relative to other European countries: Germany (13.9%), Finland (13.4%),
Belgium (11.1%), The Netherlands (8.8%), the Czech Republic (6.0%), and England and Wales (4.2%). [7]
Women are more likely to be seropositive than men, and likely acquire the virus at an earlier age.  In each country of Europe, HSV-2 seropositivity becomes more common from adolescence onwards and increases in the population with age, with a decline in the older age groups in some countries. [7]
The most recent data for HSV-2 was published in March 2010, based on a National Health and Nutrition Examination Survey study performed between 2005 and 2013 by CDC . About 1 in 6 Americans (16.2%) aged 14 to 49 is infected with HSV-2. HSV-2 prevalence was nearly twice as high among women (20.9%) than men (11.5%), and was more than three times higher among blacks (39.2%) than non-Hispanic whites (12.3%). [8]
The most affected group was black women, with a prevalence rate of 48%. Prevalence increased with age and number of partners. Only 18.9% of those infected had previously been aware of their infection. [8]
African Americans and immigrants from developing countries typically have an HSV-1 seroprevalance in their adolescent population that is two or three times higher than that of Caucasian Americans . [9] Many white Americans become sexually active while seronegative for HSV-1. The absence of antibodies from a prior oral HSV-1 infection leaves these individuals susceptible to herpes whitlow, herpes gladiatorum, and HSV-1 genital infection. Primary genital infection brings with it the risk of vertical transmission to the neonate, and is highest if the mother contracts a primary infection during the third trimester of pregnancy. In the U.S. the number of genital infections caused by HSV-1 is now thought to be about 50% of first episodes of genital infection. [10] [11]
In healthy adults, HSV-2 infection occurs more frequently in the United States than in Europe.
Seroprevalence rates in the United States appeared to be increasing, rising from 16.4% in 1976 to 21.8% in 1994. [12] However, this trend seems to have reversed itself in recent years, dropping to 17.2% in 2004. [13]
The current prevalence of genital herpes caused by HSV-2 in the U.S. is roughly one in four or five adults, with approximately 50 million people infected with genital herpes and an estimated 0.5 million new genital herpes infections occurring each year. [9] African Americans appear more susceptible to HSV-2, although the presence of active genital symptoms is more likely in Caucasian Americans .  The largest increase in HSV-2 acquisition during the past few years is in white adolescents.  People with many lifetime sexual partners and those who are sexually active from a young age are also at higher-risk for the transmission of HSV-2 in the U.S. [14] [15] [16] [17]
Women are at higher risk than men for acquiring HSV-2 infection, and the chance of being infected increases with age. [9] The CDC reports that 48% of African American women in the United States are infected with the HSV-2 virus. [18] [19]
According to a study in Ontario , of people between the ages of 15 to 16, 26.9% of men, 32% of non-pregnant women, and 55% of pregnant women tested positive for HSV-1 antibodies. Between the ages of 40 to 44, 54.7% of men, 88.7% of women, and 69.2% of pregnant women tested positive for HSV-1 antibodies. The overall age-gender standardized seroprevelance for HSV-1 antibodies was 51.1%. [20] Teenagers are less likely to be seropositive for HSV-2—antibodies against this virus are only found in 0–3.8% of 15- and 16-year-olds.  However, 21% of individuals in their early forties have antibodies against HSV-2, reflecting the sexually transmitted nature of this virus. When standardising for age, HSV-2 seroprevalence in Ontario for individuals between the ages of 15 to 44 was 9.1%. Rates for Canadian people infected with HSV-2 is much lower than estimated levels of HSV-2 seroprevalence in people of a similar age range in the United States. [20]
HSV-2 seroprevalence in pregnant women between the ages of 15–44 in British Columbia is similar, with 57% having antibodies for HSV-1 and 13% having antibodies for HSV-2. [3] In British Columbia in 1999, the seroprevalence of HSV-2 antibody in leftover serum submitted for antenatal testing revealed a prevalence of 17.3%, ranging from 7.1% in women 15–19 years old to 28.2% in those 40–44 years. [21] • In attendees at an Alberta sexually transmitted infection (STI) clinic in 1994 and 1995, the seroprevalence of HSV-1 and -2 in leftover sera was 56% and 19%, respectively. [22] In Nova Scotia, 58.1% of 1,790 HSV isolates from genital lesion cultures in women were HSV-1; in men, 36.7% of 468 isolates were HSV-1 [23]
HSV-2 is more common in Sub-Saharan Africa than in Europe or the North America. Up to 82% of women and 53% of men in Sub-Saharan Africa are seropositive for HSV-2.  These are the highest levels of HSV-2 infection in the world, although exact levels vary from country to country in this continent. [24]
In most African countries, HSV-2 prevalence increases with age. However, age-associated decreases in HSV-2 seroprevalence has been observed for women in Uganda and Zambia , and in men in Ethiopia , Benin , and Uganda. [3]
Genital herpes appears less common in Northern Africa compared to Sub-Saharan Africa.  For example, only 26% of middle-aged women have antibodies for HSV-2 in Morocco . [5] Women are more likely to be infected with HSV-2 once they are over the age of 40. [5]
Children in Egypt with acute lymphoblastic leukemia are often infected with HSV from a young age—HSV-1 or HSV-2 antibodies are present in an estimated 54% of children under the age of 5, and 77% in children over 10 years of age. [25] Algerian children are also likely to acquire HSV-1 infection at a young age (under 6) and 81.25% of the population has antibodies to HSV-1 by the age of 15. [26]
Relative to rates in Europe and North America, HSV-2 seroprevalency is high in Central and South America. Infection levels are estimated at 20% to 60%. [3] [24] During the mid-1980s, HSV-2 prevalence was 33% in 25- to 29-year-old women and 45% in those aged 40 and over in Costa Rica . In the early 1990s HSV-2 prevalence was approximately 45% among women over 60 in Mexico. [3]
The highest HSV-2 prevalence in Central or South America—60%—has been found in Colombian middle-aged women, although similar HSV-2 prevalence has been observed in younger women in Haiti (54%). [3] HSV-2 infects about 30% of women over 30 years old in Colombia , Costa Rica, Mexico, and Panama . HSV-2 antibodies were found in more than 41% of women of childbearing age in Brazil . [3]
However, no increase in seroprevalence was associated with age in women over 40 years old in Brazil—HSV-2 prevalence was estimated at 50% among women aged 40–49, 33% among women 50–59, and 42% among women over 60. Women in Brazil are more likely to acquire an HSV-2 infection if their male partners had history of anal sex and had many sexual partners in his lifetime. [5] In Peru , HSV-2 prevalence is also high among women in their 30s but is lower in men. [3]
HSV-1 seroprevalence in some Asian countries is low, relative to other countries worldwide, with only 51% women in Thailand , and between 50–60% in Japan possessing antibodies. [3] [5] HSV-2 seroprevalence in developing Asian countries is comparable (10–30%) to that observed in North America and Northern Europe. [24] However, estimates of HSV-2 infectivity in Thailand are higher than observed in other Eastern Asian countries; total HSV-2 seroprevalence is approximately 37% in this country. [5] HSV-2 seroprevalence is low in women in the Philippines (9%), although commencing activity while young is associated with an increase risk of acquiring HSV-2 infection; woman starting sexual activity by the time they reach 17 are seven times more likely to be HSV-2 seropositive than those starting sexual activity when over 21. [27]
In South Korea, incidence of HSV-2 infection in those under the age of 20 is low, only 2.7% in men and 3.0% in women. [6] Seroprevalence levels increase in older South Koreans however, such that the population over 20 that has antibodies against HSV-2 is 21.7% of men and 28% of women. [6]
In India 67% of individuals were seropositive in 2005, within this figure, 33.3% were positive for HSV-1 and 16.6% are seropositive for HSV-2. Those with both HSV-1 and HSV-2 antibodies are estimated at 13.3% of the population. Indian men are more likely to be infected with HSV-2 than women, and increasing seroprevalence of this virus is associated with an increasing age. [28]
Turkey—
High levels of HSV-1 (97%) and HSV-2 (42%) were found amongst pregnant women in the city of Erzurum in Eastern Anatolia Region, Turkey . [3] In Istanbul however, lower HSV-2 seroprevalence was observed; HSV-2 antibodies were found in 4.8% of sexually active adults, while HSV-1 antibodies were found in 85.3%. [29] Only 5% of pregnant women were infected with HSV-2, and 98% were infected with HSV-1.  Prevalence of these viruses was higher in sex workers of Istanbul, reaching levels of 99% and 60% for HSV-1 and HSV-2 prevalence respectively. [29]
Jordan—
The prevalence of HSV-2 in Jordan is 52.8% for men and 41.5% for women. [30]
Israel—
HSV-1 seroprevalence is 59.8% in the population of Israel and increases with age in both genders but the adolescent seroprevalence has been declining as in most industrialized nations. [31] An estimated 9.2% of Israeli adults are infected with HSV-2. Infection of either HSV-1 or HSV-2 is higher in females; HSV-2 seroprevalence reaches 20.5% in females in their 40s. These values are similar to levels in HSV infection in Europe. [32]
Antibodies for HSV-1 or HSV-2 are also more likely to be found individuals born outside of Israel, and individuals residing in Jerusalem and Southern Israel; people of Jewish origin living in Israel are less likely to possess antibodies against herpes. [32] Among pregnant women in Israel a small scale cross sectional study found the prevalence of HSV-2 infection was 13.3% and that of HSV-1 was 94.9%. The HSV-2 infection rate was 3-fold higher among immigrants from the former Soviet Union (27.5%) than among
Israeli-born Jewish and Arab women (9%). [33] Approximately 78% of HSV-2 infections in Israel are asymptomatic. [34] HSV-1 causes 66.3% of genital herpes in the Tel Aviv area. [35]
Syria—
Genital herpes infection from HSV-2 is predicted to be low in Syria although HSV-1 levels are high. HSV-1 infections is common (95%) among healthy Syrians over the age of 30, while HSV-2 prevalence is low in healthy individuals (0.15%), and persons infected with other sexually transmitted diseases (9.5%).  High risk groups for acquiring HSV-2 in Syria, include prostitutes and bar girls; they have 34% and 20% seroprevalence respectively. [36]
In Australia, a study using data from 1999 revealed the seroprevalence of HSV-1 was at 76.5%, with significant differences associated with age, gender and Indigenous status, which were not specified. [37] An estimated 12% of Australian adults were seropositive for HSV-2, with higher prevalence in women (16%) than in men (8%). [37] Larger cities had higher HSV-2 seroprevalence (13%) than rural populations (9%). Higher prevalence was found in Indigenous Australians (18%) than non-Indigenous Australians (12%). [37]
As in the U.S., HSV-1 is increasingly identified as the cause of genital herpes in Australians ; HSV-1 was identified in the anogenital area of only 3% of the population in 1980, but had risen to 41% in 2001. [38] This was most common in females and persons under 25. [38] The number of genital herpes infections appears to be rising in New Zealand with three times more cases in 1993 compared to 1977. [39] In this country, HSV-2 affects 60% more women than men of similar age. [3]

HIV/AIDS , or human immunodeficiency virus, is considered by some authors a global pandemic . [1] However, the WHO currently uses the term 'global epidemic' to describe HIV. [2] As of 2018, approximately 37.9 million people are infected with HIV globally. [3] There were about 770,000 deaths from AIDS in 2018. [4] The 2015 Global Burden of Disease Study , in a report published in The Lancet , estimated that the global incidence of HIV infection peaked in 1997 at 3.3 million per year. Global incidence fell rapidly from 1997 to 2005, to about 2.6 million per year, but remained stable from 2005 to 2015. [5]
Sub-Saharan Africa is the region most affected. In 2018, an estimated 61% of new HIV infections occurred in this region. [3] Prevalence ratios are "In western and central Europe and North America, low and declining incidence of HIV and mortality among people infected with HIV over the last 17 years has seen the incidence:prevalence ratio fall from 0.06 in 2000 to 0.03 in 2017. Strong and steady reductions in new HIV infections and mortality among people infected with HIV in eastern and southern Africa has pushed the ratio down from 0.11 in 2000 to 0.04 in 2017. Progress has been more gradual in Asia and the Pacific (0.05 in 2017), Latin America (0.06 in 2017), the Caribbean (0.05 in 2017) and western and central Africa (0.06 in 2017). The incidence:prevalence ratios of the Middle East and North Africa (0.08 in 2017) and eastern Europe and central Asia (0.09 in 2017)". [3] South Africa has the largest population of people with HIV of any country in the world, at 7.06 million [6] as of 2017. In Tanzania, HIV/AIDS was reported to have a prevalence of 4.5% among Tanzanian adults aged 15–49 in 2017. [7]
South & South-East Asia (a region with about 2 billion people as of 2010, over 30% of the global population) has an estimated 4 million cases (12% of all people infected with HIV), with about 250,000 deaths in 2010. [8] Approximately 2.5 million of these cases are in India, where however the prevalence is only about 0.3% (somewhat higher than that found in Western and Central Europe or Canada). [9] Prevalence is lowest in East Asia at 0.1%. [8]
In 2017, approximately 1 million people in the United States had HIV ; 14% did not realize that they were infected. [10]
In 2017, 93,385 people (64,472 men and 28,877 women) living with diagnosed HIV infection received HIV care in the UK and 428 deaths. [11] 42,739 (nearly 50%) of those are gay or bisexual, a small segment of the overall population.
In Australia, as of 2017, there were about 27,545 cases. [12] In Canada as of 2016, there were about 63,110 cases. [13] [14]
A reconstruction of its genetic history shows that the HIV pandemic almost certainly originated in Kinshasa , the capital of the Democratic Republic of the Congo , around 1920. [15] [16] AIDS was first recognized in 1981, in 1983 the HIV virus was discovered and identified as the cause of AIDS, and by 2009 AIDS caused nearly 30 million deaths. [17] [18] [19]
Since the first case of HIV/AIDS reported in 1981, this virus, although rare, continues to be one of the most prevalent and deadliest pandemics worldwide. The Center for Disease Control mentions that the HIV disease continues to be a serious health issue for several parts of the world. Worldwide, there were about 1.7 million new cases of HIV reported in 2018. About 37.9 million people were living with HIV around the world in 2018, and 24.5 million of them were receiving medicines to treat HIV, called antiretroviral therapy (ART). In addition, roughly an estimated 770,000 people have died from AIDS-related illnesses in 2018. [20]
Globally, individuals suffer from HIV/AIDS; yet, there has also been a common trend as far as prevalence in cases and regions most affected by the disease. The CDC reports that areas like the Sub-Saharan Africa region is the most affected by HIV and AIDS worldwide, and accounts for approximately 61% of all new HIV infections. Other regions significantly affected by HIV and AIDS include Asia and the Pacific, Latin America and the Caribbean, Eastern Europe, and Central Asia. [21]
Worldwide there is a common stigma and discrimination surrounding HIV/AIDS. Respectively, infected patients are more subject to judgement, harassment, and acts of violence and come from marginalized areas where it is common to engage in illegal practices in exchange for money, drugs, or other exchangeable forms of currency.
AVERT , an international HIV and AIDS charity created in 1986, makes continuous efforts to prioritize, normalize, and provide the latest information and education programs on HIV and AIDS for individuals and areas most affected by this disease worldwide. AVERT suggested that, discrimination and other human rights violations may occur in health care settings, barring people from accessing health services or enjoying quality health care. [22]
Accessibility to tests have also played a significant role in the response and speed to which nations take action. Approximately 81% of people with HIV globally knew their HIV status in 2019. The remaining 19% (about 7.1 million people) still need access to HIV testing services. HIV testing is an essential gateway to HIV prevention, treatment, care and support services. [23] It is crucial to have HIV tests available for individuals worldwide since it can help individuals detect the status of their disease from an early onset, seek help, and prevent further spread through the practice of suggestive safety precautions. [ citation needed ]
There were approximately 38 million people across the globe with HIV/AIDS in 2019. Of these, 36.2 million were adults and 1.8 million were children under 15 years old. [24]
HIV/AIDS in World from 2001 to 2014 – adult prevalence rate – data from CIA World Factbook [28]
The pandemic is not homogeneous within regions, with some countries more afflicted than others. Even at the country level, there are wide variations in infection levels between different areas. The number of people infected with HIV continues to rise in most parts of the world, despite the implementation of prevention strategies, Sub-Saharan Africa being by far the worst-affected region, with an estimated 22.9 million at the end of 2010, 68% of the global total. [29]
South and South East Asia have an estimated 12% of the global total. [30] The rate of new infections has fallen slightly since 2005 after a more rapid decline between 1997 and 2005. [29] Annual AIDS deaths have been continually declining since 2005 as antiretroviral therapy has become more widely available. [ citation needed ]
Sub-Saharan Africa remains the hardest-hit region. HIV infection is becoming endemic in sub-Saharan Africa, which is home to just over 12% of the world's population but two-thirds of all people infected with HIV. [29] The adult HIV prevalence rate is 5.0% and between 21.6 million and 24.1 million total are affected. [29] However, the actual prevalence varies between regions. Presently, Southern Africa is the hardest hit region, with adult prevalence rates exceeding 20% in most countries in the region, and 30% in Swaziland and Botswana . Analysis of prevalence across sub-Saharan Africa between 2000 and 2017 found high variation in prevalence at a subnational level, with some countries demonstrating a more than five-fold difference in prevalence between different districts. [32]
Eastern Africa also experiences relatively high levels of prevalence with estimates above 10% in some countries, although there are signs that the pandemic is declining in this region. West Africa on the other hand has been much less affected by the pandemic.
Several countries reportedly have prevalence rates around 2 to 3%, and no country has rates above 10%. In Nigeria and Côte d'Ivoire, two of the region's most populous countries, between 5 and 7% of adults are reported to carry the virus. [ citation needed ]
Across Sub-Saharan Africa, more women are infected with HIV than men, with 13 women infected for every 10 infected men. This gender gap continues to grow. Throughout the region, women are being infected with HIV at earlier ages than men. The differences in infection levels between women and men are most pronounced among young people (aged 15–24 years). In this age group, there are 36 women infected with HIV for every 10 men. The widespread prevalence of sexually transmitted diseases , the promiscuous culture, [33] the practice of scarification , unsafe blood transfusions , and the poor state of hygiene and nutrition in some areas may all be facilitating factors in the transmission of HIV-1 (Bentwich et al., 1995).
Mother-to-child transmission is another contributing factor in the transmission of HIV-1 in developing nations. Due to a lack of testing, a shortage in antenatal therapies and through the feeding of contaminated breast milk, 590,000 infants born in developing countries are infected with HIV-1 per year. In 2000, the World Health Organization estimated that 25% of the units of blood transfused in Africa were not tested for HIV, and that 10% of HIV infections in Africa were transmitted via blood. [ citation needed ]
Poor economic conditions (leading to the use of dirty needles in healthcare clinics) and lack of sex education contribute to high rates of infection. In some African countries, 25% or more of the working adult population is HIV-positive. Poor economic conditions caused by slow onset-emergencies, such as drought, or rapid onset natural disasters and conflict can result in young women and girls being forced into using sex as a survival strategy. [34] Worse still, research indicates that as emergencies, such as drought, take their toll and the number of potential 'clients' decreases, women are forced by clients to accept greater risks, such as not using contraceptives. [34]
AIDS-denialist policies have impeded the creation of effective programs for distribution of antiretroviral drugs.  Denialist policies by former South African President Thabo Mbeki 's administration led to several hundred thousand unnecessary deaths. [35] [36] UNAIDS estimates that in 2005 there were 5.5 million people in South Africa infected with HIV — 12.4% of the population. This was an increase of 200,000 people since 2003. [ citation needed ]
Although HIV infection rates are much lower in Nigeria than in other African countries, the size of Nigeria's population meant that by the end of 2003, there were an estimated 3.6 million people infected. On the other hand, Uganda, Zambia , Senegal , and most recently Botswana have begun intervention and educational measures to slow the spread of HIV, and Uganda has succeeded in actually reducing its HIV infection rate. [ citation needed ]
HIV/AIDS prevalence among the adult population (15-49) in the Middle East and North Africa is estimated less than 0.1 between 1990 and 2018. This is the lowest prevalence rate compared to other regions in the world. [37]
In the MENA, roughly 240,000 people are living with HIV as of 2018 [38] and Iran accounted for approximately one-quarter (61,000) of the population with HIV followed by Sudan (59,000). [38] As well as, Sudan (5,200), Iran (4,400) and Egypt (3,600) took up more than 60% of the number of new infections in the MENA (20,000). Roughly two-thirds of AIDS-related deaths in this region happened in these countries for the year 2018. [38]
Although the prevalence is low, concerns remain in this region. First, unlike the global downward trend in new HIV infections and AIDS-related deaths, the numbers have continuously increased in the MENA. [39] Second, compared to the global rate of antiretroviral therapy (62%), [40] the MENA region's rate is far below (32%). [38] The low participation of ART increases not only the number of AIDS-related deaths but the risk of mother-to-baby HIV infections, in which the MENA (24.7%) shows relatively high rates compared to other regions, for example, southern Africa (10%), Asia and the Pacific (17%). [37]
Key population at high risk in this region is identified as injection drug users, female sex workers and men who have sex with men. [37]
The geographical size and human diversity of South and South-East Asia have resulted in HIV epidemics differing across the region.
In South and Southeast Asia, the HIV epidemic remains largely concentrated in injecting drug users , men who have sex with men (MSM), sex workers , and clients of sex workers and their immediate sexual partners. [41] In the Philippines, in particular, sexual contact between males comprise the majority of new infections. An HIV surveillance study conducted by Dr. Louie Mar Gangcuangco and colleagues from the University of the Philippines - Philippine General Hospital showed that out of 406 MSM tested for HIV in Metro Manila, HIV prevalence was 11.8% (95% confidence interval: 8.7- 15.0). [42] [43]
Migrants, in particular, are vulnerable and 67% of those infected in Bangladesh and 41% in Nepal are migrants returning from India. [41] This is in part due to human trafficking and exploitation, but also because even those migrants who willingly go to India in search of work are often afraid to access state health services due to concerns over their immigration status. [41]
The national HIV prevalence levels in East Asia is 0.1% in the adult (15–49) group. However, due to the large populations of many East Asian nations, this low national HIV prevalence still means that large numbers of people are infected with HIV.  The picture in this region is dominated by China. Much of the current spread of HIV in China is through injecting drug use and paid sex. In China, the number was estimated at between 430,000 and 1.5 million by independent researchers, with some estimates going much higher. [ citation needed ]
In the rural areas of China, where large numbers of farmers, especially in Henan province, participated in unclean blood transfusions ; estimates of those infected are in the tens of thousands. In Japan, just over half of HIV/AIDS cases are officially recorded as occurring amongst homosexual men, with the remainder occurring amongst heterosexuals and also via drug abuse , in the womb or unknown means. [ citation needed ]
In East Asia, men who have sex with men account for 18% of new HIV/AIDS cases and are therefore a key affected group along with sex workers and their clients who makeup 29% of new cases. This is also a noteworthy aspect because men who have sex with men had a prevalence of at least 5% or higher in countries in Asia and Pacific. [44]
The Caribbean is the second-most affected region in the world. [29] Among adults aged 15–44, AIDS has become the leading cause of death. The region's adult prevalence rate is 0.9%. [29] with national rates ranging up to 2.7%. [45] HIV transmission occurs largely through heterosexual intercourse. A greater number of people who get infected with HIV/AIDS are heterosexuals. [46] with two-thirds of AIDS cases in this region attributed to this route. Sex between men is also a significant route of transmission, even though it is heavily stigmatised and illegal in many areas. HIV transmission through injecting drug use remains rare, except in Bermuda and Puerto Rico.
Within the Caribbean, the country with the highest prevalence of HIV/AIDS is the Bahamas with a rate of 3.2% of adults with the disease. However, when comparing rates from 2004 to 2013, the number of newly diagnosed cases of HIV decreased by 4% over those years. Increased education and treatment drugs will help to decrease incidence levels even more. [47]
The populations of Central and South America have approximately 1.6 million people currently infected with HIV and this number has remained relatively unvarying with having a prevalence of approximately .4%. In Latin America, those infected with the disease have received help in the form of Antiretroviral treatment, with 75% of people with HIV receiving the treatment. [48]
In these regions of the American continent, only Guatemala and Honduras have national HIV prevalence of over 1%. In these countries, HIV-infected men outnumber HIV-infected women by roughly 3:1. [ citation needed ]
With HIV/AIDS incidence levels rising in Central America, education is the most important step in controlling the spread of this disease. In Central America, many people do not have access to treatment drugs. This results in 8–14% of people dying from AIDS in Honduras. To reduce the incidence levels of HIV/AIDS, education and drug access needs to improve. [49]
In a study of immigrants traveling to Europe, all asymptomatic persons were tested for a variety of infectious diseases. The prevalence of HIV among the 383 immigrants from Latin America was low, with only one person testing positive for a HIV infection. This data was collected from a group of immigrants with the majority from Bolivia, Ecuador and Colombia. [50]
Since the epidemic began in the early 1980s, 1,216,917 people have been diagnosed with AIDS in the US. In 2016, 14% of the 1.1 million people over age 13 living with HIV were unaware of their infection. [51] The most recent CDC HIV Surveillance Report estimates that 38,281 new cases of HIV were diagnosed in the United States in 2017, a rate of 11.8 per 100,000 population. [52] Men who have sex with men accounted for approximately 8 out of 10 HIV diagnoses among males. Regionally, the population rates (per 100,000 people) of persons diagnosed with HIV infection in 2015 were highest in the South (16.8), followed by the Northeast (11.6), the West (9.8), and the Midwest (7.6). [53]
The most frequent mode of transmission of HIV continues to be through male homosexual sexual relations. In general, recent studies have shown that 1 in 6 gay and bisexual men were infected with HIV. [54] As of 2014, in the United States, 83% of new HIV diagnoses among all males aged 13 and older and 67% of the total estimated new diagnoses were among homosexual and bisexual men. Those aged 13 to 24 also accounted for an estimated 92% of new HIV diagnoses among all men in their age group. [55]
A review of studies containing data regarding the prevalence of HIV in transgender women found that nearly 11.8% self-reported that they were infected with HIV. [56] Along with these findings, recent studies have also shown that transgender women are 34 times more likely to have HIV than other women. [54] A 2008 review of HIV studies among transgender women found that 28 percent tested positive for HIV. [57] In the National Transgender Discrimination Survey, 20.23% of black respondents reported being HIV-positive, with an additional 10% reporting that they were unaware of their status. [58]
AIDS is one of the top three causes of death for African American men aged 25–54 and for African American women aged 35–44 years in the United States of America. In the United States, African Americans make up about 48% of the total HIV-positive population and make up more than half of new HIV cases, despite making up only 12% of the population. The main route of transmission for women is through unprotected heterosexual sex. African American women are 19 times more likely to contract HIV than other women. [59]
By 2008, there  was increased awareness that  young African-American women in particular were at high risk for HIV infection. [60] In 2010, African Americans made up 10% of the population but about half of the HIV/AIDS cases nationwide. [61] This disparity is attributed in part to a lack of information about AIDS and a perception that they are not vulnerable, as well as to limited access to health-care resources and a higher likelihood of sexual contact with at-risk male sexual partners. [62]
Since 1985, the incidence of HIV infection among women had been steadily increasing. In 2005 it was estimated that at least 27% of new HIV infections were in women. [63] There has been increasing concern for the concurrency of violence surrounding women infected with HIV. In 2012, a meta-analysis showed that the rates of psychological trauma, including Intimate Partner Violence and PTSD in HIV positive women were more than five times and twice the national averages, respectively. [64] In 2013, the White House commissioned an Interagency Federal Working Group to address the intersection of violence and women infected with HIV. [65]
There are also geographic disparities in AIDS prevalence in the United States, where it is most common in the large cities of California, esp. Los Angeles and San Francisco and the East Coast, ex. New York City and in urban cities of the Deep South . [66] Rates are lower in Utah , Texas , and Northern Florida . [66] Washington, D.C., the nation's capital, has the nation's highest rate of infection, at 3%. This rate is comparable to what is seen in west Africa, and is considered a severe epidemic. [67]
In the United States in particular, a new wave of infection is being blamed on the use of methamphetamine , known as crystal meth. Research presented at the 12th Annual Retrovirus Conference in Boston in February 2005 concluded that using crystal meth or cocaine is the biggest single risk factor for becoming HIV+ among US gay men, contributing 29% of the overall risk of becoming positive and 28% of the overall risk of being the receptive partner in anal sex . [68]
In addition, several renowned clinical psychologists now cite methamphetamine as the biggest problem facing gay men today, including Michael Majeski, who believes meth is the catalyst for at least 80% of seroconversions currently occurring across the United States, and Tony Zimbardi, who calls methamphetamine the number one cause of HIV transmission, and says that high rates of new HIV infection are not being found among non-crystal users. In addition, various HIV and STD clinics across the United States report anecdotal evidence that 75% of new HIV seroconversions they deal with are methamphetamine-related; indeed, in Los Angeles , methamphetamine usage is regarded as the main cause of HIV seroconversion among gay men in their late thirties. [68] The chemical "methamphetamine", in and of itself, cannot infect someone with HIV.
In 2016, there were approximately 63,100 people living with HIV/AIDS in Canada. [69] It was estimated that 9090 persons were living with undiagnosed HIV at the end of 2016. [69] Mortality has decreased due to medical advances against HIV/AIDS, especially highly active antiretroviral therapy (HAART). HIV/AIDS prevalence is increasing most rapidly amongst aboriginal Canadians , with 11.3% of new infections in 2016. [69]
There is growing concern about a rapidly growing epidemic in Eastern Europe and Central Asia , where an estimated 1.23–3.7 million people were infected as of December 2011, though the adult (15–49) prevalence rate is low (1.1%). The rate of HIV infections began to grow rapidly from the mid-1990s, due to social and economic collapse, increased levels of intravenous drug use and increased numbers of sex workers. By 2010 the number of reported cases in Russia was over 450,000 according to the World Health Organization , up from 15,000 in 1995 and 190,000 in 2002; some estimates claim the real number is up to eight times higher, well over 2 million.
Ukraine and Estonia also have growing numbers of infected people, with estimates of 240,000 and 7,400 respectively in 2018. Also, transmission of HIV is increasing through sexual contact and drug use among the young (<30 years). Indeed, over 84% of current AIDS cases in this region occur in non-drug-using heterosexuals less than 26 years of age.
In most countries of Western Europe, AIDS cases have fallen to levels not seen since the original outbreak; many attribute this trend to aggressive educational campaigns, screening of blood transfusions and increased use of condoms. Also, the death rate from AIDS in Western Europe has fallen sharply, as new AIDS therapies have proven to be an effective (though expensive) means of suppressing HIV. [ citation needed ]
In this area, the routes of transmission of HIV is diverse, including paid sex, injecting drug use, mother to child, male with male sex and heterosexual sex. [ citation needed ] However, many new infections in this region occur through contact with HIV-infected individuals from other regions. The adult (15–49) prevalence in this region is 0.3% with between 570,000 and 890,000 people currently infected with HIV. Due to the availability of antiretroviral therapy, AIDS deaths have stayed low since the lows of the late 1990s. However, in some countries, a large share of HIV infections remain undiagnosed and there is worrying evidence of antiretroviral drug resistance among some newly HIV-infected individuals in this region. [ citation needed ]
There is a very large range of national situations regarding AIDS and HIV in this region. This is due in part to the large distances between the islands of Oceania. The wide range of development in the region also plays an important role. The prevalence is estimated at between 0.2% and 0.7%, with between 45,000 and 120,000 adults and children currently infected with HIV.
Papua New Guinea has one of the most serious AIDS epidemics in the region. According to UNAIDS, HIV cases in the country have been increasing at a rate of 30 percent annually since 1997, and the country's HIV prevalence rate in late 2006 was 1.3%. [70]
In June 2001, the United Nations held a Special General Assembly to intensify international action to fight the HIV/AIDS epidemic as a global health issue , and to mobilize the resources needed towards this aim, labelling the situation a "global crisis". [71]
Regarding the social effects of the HIV/AIDS pandemic, some sociologists suggest that AIDS has caused a "profound re- medicalisation of sexuality ". [72] [73]
There has been extensive research done with HIV since 2001 in the United States, The National Institutes of Health (NIH) which is an agency funded by the U.S department of Health and Human Services (HHS) has substantially improved the health, treatment, and lives of many individuals across the nation. The human immunodeficiency virus (HIV) is generally the precursor to AIDS . To this day there is no cure for this virus; However, treatment, education programs, proper medical care, and support have been made available.
NIH , is coordinated by the Office of AIDS Research (OAR) and this research carried out by nearly all the NIH Institutes and Centers, in both at NIH and at NIH-funded institutions worldwide. The NIH HIV/AIDS Research Program, represents the world's largest public investment in AIDS research. [74] Other agencies like the National Institute of Allergy and Infectious Diseases have also made substantial efforts to provide the latest and newest research and treatment available.
The NIH found that in certain areas of the world, the correlation in risky behaviors and the acquisition of HIV/AIDS is causational. Consistent drug usage and related risk behaviors, such as the exchange of sex for drugs or money, are linked to an increased risk of HIV acquisition in marginalized areas. NIAID and other NIH institutes work to develop and optimize harm reduction interventions that decrease the risk of drug use-associated and sexual transmission of HIV among injecting and non-injecting drug users. [75] Most organizations work collectively around the globe to understand, diagnose, treat, and battle the spread of this notorious disease, through the use of intervention and preventive programs the risk of acquiring HIV and the development of AIDS has dramatically dropped by 40% since its peak of cases back in 1998. [76]
Despite the advancements in scientific research and treatment, to this day there's no available cure for HIV/AIDS. Yet major efforts to contain the disease and improve the lives of many individuals through modernized anti-viral therapy have resulted in positive and promising results that may one day lead to a cure. The U.S. President's Emergency Plan for AIDS Relief (PEPFAR) is one of the largest U.S. Government's response to the global HIV/AIDS epidemic and represents the largest commitment by any nation to address a single disease in history. PEPFAR provided HIV testing services for 79.6 million people in Fiscal Year 2019 and, as of September 30, 2019, supported lifesaving anti-retroviral therapy for nearly 15.7 million men, women, and children. [23] As of the end of 2019, 25.4 million people with HIV (67%) were accessing antiretroviral therapy (ART) globally. That means 12.6 million people are still waiting. HIV treatment access is key to the global effort to end AIDS as a public health threat. [23] Individuals who not only are aware of their condition but also are prescribed ART , are encouraged to remain consistent with their daily-dosage treatment so they can reduce the spread, viral load, and live happy and healthy lives.
Because HIV is more prevalent in urban areas of the United States, individuals living in rural areas generally don't participate or receive HIV diagnosis. The CDC found huge disparities in HIV cases between Northern and Southern regions of the Nation. At a rate of 15.9 the Southern regions account for a large number of reports of HIV; subsequently, regions like the North and Midwest account for general rates between 9 and 7.2 making it significantly lower in case prevalence. [77]
According to the CDC , populations affected and with most reported cases of HIV are generally found in gay, bisexual, and other men who reported male-to-male sexual contact. In 2018, gay and bisexual men accounted for 69% of the 37,968 new HIV diagnoses and 86% of diagnoses among males. HIV doesn't only affect individuals in this category, heterosexuals tend to be affected by HIV as well. In 2018, heterosexuals accounted for 24% of the 37,968 new HIV diagnoses in the United States.
UNAIDS also suggested that the individuals who may also be at risk of acquiring this disease are generally:
Joint United Nations Programme on HIV/AIDS (UNAIDS) (2011). Global HIV/AIDS Response, Epidemic update and health sector progress towards universal access (PDF) . Joint United Nations Programme on HIV/AIDS.
Multiple sex partners is the measure and incidence of engaging in sexual activities with two or more people within a specific time period. Sexual activity with MSP can happen simultaneously or serially. MSP includes sexual activity between people of a different gender or the same gender. A person can be said to have multiple sex partners, when the person have sex with more than one person at the same time. [1] Another term, polyamorous , is a behavior and not a measure describing multiple romantically sexually or romantically committed relationships at the same time. [2]
Young people having MSP in the last year is an indicator used by the Centers for Disease Control and Prevention (CDC) to evaluate risky sexual behavior in adolescents and  monitoring changes in the worldwide HIV/AIDS infection rates and deaths. [3]
Epidemiologists and clinicians who quantify risks associated with MSP do so to identify those who have had sexual intercourse with more than one partner in the past 12 months. For the purposes of the World Health Organization (WHO)'s effort to eliminate HIV infection, quantifying measures progress in reducing the percentage of those with AIDS . The World Health Organization (WHO) has described their rationale by assuming that the spread of HIV in most locales depends upon the number of  MSP. Those who have MSP possess a higher risk of HIV transmission than individuals that do not have multiple sex partners. [4]
WHO uses indicators, such as MSP, age, mortality, morbidity, geographical location and signs and symptoms of disease. This is done so that change can be measured and so that the effect of indicators can be assessed. [4]
Following the initial quantification of the number of MSP, the respondent is again surveyed three and then five years later. In addition to the survey, respondents' sexual histories are obtained. Analysis assists those conducting the study to verify and help define the term MSP. [4]
For the indicator MSP, WHO has defined a summary of what it measures, rationale for the indicator, numerator, denominator and calculation, recommended measurement tools,  measurement, frequency, and the strengths and weaknesses of the indicator. [4]
WHO's definition of MSP has some strengths and weaknesses The quantification is an indicator and a picture of the levels of higher-risk sex in a locale. If those surveyed changed their activity to one sexual partner, the change will be quantified by changes in the indicator. This disadvantage is that though a respondent may reduce the number of MSP in a 12 month period, the indicator will not reflect this change in sexual activity. Even so, decreasing the number of MSP may not indicate a change. Potentially this definition and quantification may have a significant impact on the pandemic of HIV and used as a measure of program success. WHO recommends that additional indicators that quantify MSP more precisely to capture the reduction in multiple sexual partners in general. [4] [5] According to the CDC’s Youth Risk Behavior Surveillance System, having multiple sex partners has been quantified to mean that   those greater than or equal to age 25 had four or more sexual partners in one year. [6]
Epidemiologists in Tanzania used the indicator MSP in their study of AIDS incidence among 15-19-year-olds by documenting the respondent as being sexually active and having MSP in the last 12 months. [7]
A complete medical history includes an assessment of the number sexual partners with which a person has had within a certain time period. [8]
A social history (abbreviated "SocHx") that part of a medical exam addressing familial, occupational, and recreational aspects of the patient's personal life that have the potential to be clinically important. [9] MSP is only the description of the behavior described in clinical terms. Promiscuity can mean that a moral judgement is made because some parts of societies promote sexual activity to occur only within exclusive, single-partner, committed relationships. [10] is often the way researchers define a society's promiscuity levels at any given time.  MSP increases the risk of many diseases and other conditions.
The CDC in the past has quantified  MSP for adolescents with the following descriptions:
Some clinicians define MSP by also taking into account concurrent sexual relationships. [12]
The likelihood of developing substance abuse or dependence increases linearly with the number of sex partners, an effect more pronounced for women. People who have a higher number of sex partners do not have higher rates of anxiety or depression. [13] [14]
MSP increases the risk of developing bacterial vaginosis. [15] MSP can result in pregnant women with a greater risk of contracting HIV. [16] HIV is strongly associated with having MSP. [17] Having multiple sex partners is associated with higher incidences of STIs. [18]
Prevention of disease strategies include intensive counseling of those who have met the definition of multiple sex partners. [9]
In Jamaica one of the primary contributing associations to the AIDS/HIV epidemic is the risky behavior of having multiple sex partners. A 2004 Behavioral Surveillance Survey demonstrated that 89 percent of males and 78 percent of females aged 15 to 24 had sex with a nonmarital or noncohabitating partner in the preceding 12 months. Fifty-six percent of males and 16 percent of females had multiple sex partners in the preceding 12 months. [19]
In Sub-sahara Africa, travel and wealth is a risk factor in engaging in sexual activities with multiple sex partners. [20]
Risky sexual behavior is the description of the activity that will increase the probability that a person engaging in sexual activity with another person infected with a sexually transmitted infection will be infected [1] or become pregnant , or make a partner pregnant. It can mean two similar things: the behavior itself, the description of the partner's behavior. The behavior could be unprotected vaginal , oral , or anal intercourse. The partner could be a nonexclusive partner , HIV-positive , or an person who injects substances . [2] Substance use is associated with risky sexual behaviors. [3]
Risky sexual behavior can be: [4]
Risky sexual behavior includes unprotected intercourse, multiple sex partners , and illicit drug use. [8] The use of alcohol and illicit drugs greatly increases the risk of gonorrhea , chlamydia , trichomoniasis , hepatitis B , and HIV/AIDS . Trauma from penile-anal sex has been identified as a risky sexual behavior. [9]
North American adolescents can be sexually active yet do not take appropriate precautions to prevent infection or pregnancies. Misconceptions of invulnerability and the practice of ignoring long-term consequences of their behavior tend to promote risky sexual behavior. [ citation needed ]
Risky sexual behaviors can lead to serious consequences both for person and their partner(s). This sometimes includes cervical cancer , ectopic pregnancy and infertility . [2] An association exists between those with a higher incidence of body art ( body piercings and tattoos ) and risky sexual behavior. [9]
According to the National Youth Behavior Risk Survey, 19% of all sexually active adolescents in the US consumed alcohol or used other drugs before their last sexual intercourse. [10] In contrast, adolescents who reported no substance use were found to be the least likely to engage in sexual risk-taking. [11]
Most Canadian and American adolescents aged 15 to 19 years describe having had sexual intercourse at least one time. In the same population, 23.9% and 45.5% of young, adolescent females describe having sex with two or more sexual partners during the previous year. Of the males in the same population, 32.1% of Canadian males had two or more partners and 50.8% of American males also describe a similar experience. [2]
Alcohol is the most commonly used substance among youth aged 18–25 years. 10% of young adults had an alcohol use disorder in 2018, which is greater than the prevalence among all other age cohorts. [12] Research indicates that alcohol can lead to risky sexual behavior including lack of condom use, sexual intercourse with a non-primary partner, as well as lower likelihood of using contraception in general. [13]
Among older age cohorts, a similar positive trend can be observed in risky sexual behavior when combined with alcohol use. For instance, research on older men who have sex with men (MSM) showed that the likelihood of engaging in risky sexual activities increased with the use of alcohol and other drugs. [14]
Several factors linked to risky sexual behaviors. These include inconsistent condom use, alcohol use, polysubstance use, depression, lack of social support, recent incarceration, residing with a partner, and exposure to intimate partner violence and childhood sexual abuse . Further research is needed to establish the exact causal relationship between these factors and risky sexual behaviors. [15] [16] Sexual health risk reduction can include motivational exercises, assertiveness skills, educational and behavioral interventions. Counseling has been developed and implemented for people with severe mental illness, may improve participants' knowledge, attitudes, beliefs, behaviors or practices (including assertiveness skills) and could lead to a reduction in risky sexual behavior. [8]
There are several studies on the management of risky sexual behavior among youth, with most focusing on the prevention of sexually transmitted infections (STIs) such as HIV . [17] [18] [19] A meta-analysis evaluating prevention interventions among adolescents offers support for these programs in contributing to successful outcomes such as decreased incident STIs, increased condom use , and decreased or delayed penetrative sex . [18] The findings showed that most interventions were administered in a group format and involved psychoeducation on HIV/AIDS, active interpersonal skills-training with some additionally focusing on self-management skills-training and condom information/ demonstrations. Some evidence suggests that family interventions may be beneficial in preventing long-term risky sexual behavior in early adulthood. [20]
The HIV Cohorts Data Exchange Protocol ( HICDEP ) is a protocol that allows for simple sharing of data from HIV cohorts by providing a standard format for datasets. It is used by several major HIV Cohort Collaborations such as CASCADE , COHERE , [1] [2] EuroSIDA , [3] PENTA , EPPICC [4] ART-CC [5] and D:A:D. [6] Its first version was presented at the 7th International Workshop on HIV Observational Databases on March 29–30, 2003, Fiuggi , Italy , [7] and a summary was published in Antiviral Therapy . [8] The specification currently consists of 27 tables in a relational structure [9] which cover most medical aspects of HIV patient histories used in clinical research, including pediatric aspects.
Despite the tremendous success of Anti-retroviral treatment there remain many questions about HIV and its effects on the individual. Since many events occur infrequently or relate to a small group of patients, it is necessary to pull information from different cohorts . The standardized format given by HICDEP not only simplifies merging data from multiple HIV cohorts, but also provides a set of quality control checks. Prior to HICDEP, each collaboration used proprietary protocols for data exchange requiring substantial data-management efforts, which are potentially error-prone.
Where possible, HICDEP relies on already published coding systems such as the WHO 's Anatomical Therapeutic Chemical (ATC) codes [10] or the International Classification of Diseases (ICD-10) codes . [11]
Special emphasis was given to the different ways resistance data are collected and reported in research settings and routine clinical practice: for genotypic analyses nucleotide or amino acid sequences (level 1), amino acid changes (mutations) from a reference strain (level 2) and drug-specific resistance scores (level 3) were considered. [8]
Since January 2011, the format is being developed further in the scope of the FP7 EuroCoord project. [12] On September 4, 2019 the current version 1.110 was released. [13] For the development of the HICDEP standard, there was set up a discussion forum [14] on the project website. In addition, to learn more about HICDEP and how it works, there is an e-learning module available on the EuroCoord training page. [15]
HIV Drug Resistance Database , also known as Stanford HIV RT and Protease Sequence Database is a database at Stanford University that tracks 93 common mutations of HIV .  It has been recompiled in 2008 listing 93 common mutations, after its initial mutation compilation in 2007 of 80 mutations.  The latest list utilizes data from other laboratories in Europe, Canada and the United States including more than 15,000 sequences from untreated individuals. [1]
Human infectious diseases may be characterized by their case fatality rate (CFR), the proportion of people diagnosed with a disease who die from it ( cf. mortality rate ). It should not be confused with the infection fatality rate (IFR), the estimated proportion of people infected by a disease-causing agent, including asymptomatic and undiagnosed infections, who die from the disease. IFR cannot be higher than the CFR and is often much lower, but is also much harder to calculate. Data are based on optimally treated patients and exclude isolated cases or minor outbreaks, unless otherwise indicated.
Human mortality from H5N1 or the human fatality ratio from H5N1 or the case-fatality rate of H5N1 refer to the ratio of the number of confirmed human deaths resulting from confirmed cases of transmission and infection of H5N1 to the number of those confirmed cases. For example, if there are 100 confirmed cases of humans infected with H5N1 and 10 die, then there is a 10% human fatality ratio (or mortality rate). H5N1 flu is a concern due to the global spread of H5N1 that constitutes a pandemic threat. The majority of H5N1 flu cases have been reported in southeast and east Asia. The case-fatality rate is central to pandemic planning. Estimates of case-fatality (CF) rates for past influenza pandemics have ranged from  to 2-3% for the 1918 pandemic to about 0.6% for the 1957 pandemic [1] to  0.2% for the 1968 pandemic . As of 2008, the official World Health Organization estimate for the case-fatality rate for the outbreak of H5N1 avian influenza was approximately 60%. [2] Public health officials in Ontario, Canada argue that the true case-fatality rate could be lower, pointing to studies suggesting it could be 14–33%, but warned that it was unlikely to be as low as the 0.1–0.4% rate that was built into many pandemic plans. [2]
H5N1 infections in humans are generally caused by bird to human transmission of the virus.  Until May 2006, the WHO estimate of the number of human to human transmissions had been "two or three cases". On May 24, 2006, Dr. Julie L. Gerberding, director of the United States Centers for Disease Control and Prevention in Atlanta , estimated that there had been "at least three." On May 30, Maria Cheng, a WHO spokeswoman, said there were "probably about half a dozen," but that no one "has got a solid number." [3] The cases of suspected human to human transmission that continue to be found have been isolated and contained, [4] and include transmission among members of a family in Sumatra, Indonesia in June 2006 [5] as well as earlier and later instances arising in other countries. However, no pandemic strain of H5N1 has yet been found. The key point is that, at present, "the virus is not spreading efficiently or sustainably among humans." [6]
H5N1 vaccines for chickens exist and are sometimes used, although there are many difficulties that make it especially difficult to decide whether vaccination will do more harm than good. In the U.S. H5N1 pre-pandemic vaccines exist in quantities sufficient to inoculate a few million people [7] and might be useful for priming to "boost the immune response to a different H5N1 vaccine tailor-made years later to thwart an emerging pandemic". [8] Japan has inoculated 6,000 health care workers with a pre-pandemic vaccine, and is planning how to proceed with widespread vaccinations, particularly workers who would provide utilities during an outbreak. [9] [10] [11] Switzerland is also considering preemptive vaccination to protect the general public. [12] H5N1 pandemic vaccines and the technologies to rapidly create them are in the H5N1 clinical trials stage but cannot be verified as useful until after a pandemic strain emerges. Efforts to identify the changes that might result in a human-communicable strain have resulted in laboratory-generated H5N1 with substantially greater affinity for human cellular receptors after a change of just two of the H5 surface proteins. [13] Significantly, mouse antibodies were 10 times less potent against the mutants than against the pre-mutated viruses. [13]
A graphic exhibiting total cases and mortality incidence is kept current by the WHO at https://web.archive.org/web/20080827215244/http://www.wpro.who.int/NR/rdonlyres/7549914F-5C83-4418-8C20-007ADCC07C61/0/s3.jpg and complements the country-specific information shown below.
Country-specific totals of cases and deaths kept current by the WHO may be viewed by clicking through the links provided at http://www.who.int/csr/disease/avian_influenza/country/en/ Epidemic and Pandemic Alert and Response (EPR) Confirmed Human Cases of Avian Influenza A(H5N1)

A strain of H5N1 killed chickens in 1959 in Scotland and turkeys in 1991 in England . [14] This strain was "highly pathogenic" (deadly to birds) but caused neither illness nor death in humans. [15] "The precursor of the H5N1 influenza virus that spread to humans in 1997 was first detected in Guangdong , China , in 1996, when it caused a moderate number of deaths in geese and attracted very little attention." [16] In 1997, in Hong Kong , 18 humans were infected and 6 died in the first known case of H5N1 infecting humans. [17] H5N1 had evolved from a zero mortality rate to a 33% mortality rate.
The first report, in the current wave of HPAI A(H5N1) outbreaks, was of an outbreak that began December 10, 2003 in the Republic of Korea and continued for fourteen weeks. This strain caused asymptomatic infections in humans and may have died out, [18] [19] like the 1959 strain, so that its low mortality level would have little value for predicting the mortality rate of a pandemic evolving from existing HPAI A(H5N1) strains. [20] [21] The apparently extinct strain that caused human deaths from H5N1 in the Northern part of Vietnam in 2003, 2004 and 2005 also had a much lower case mortality rate than the currently existing strains. [21] Changes are occurring in H5N1 that are increasing its pathogenicity in mammals. [22] [23]
From inception through 2007, the total number of WHO -confirmed cases was 349, with 216 of those fatalities (as reported by the U.N. on January 15, 2008, confirming earlier deaths) reflecting a 62% fatality rate among WHO -confirmed cases through 2007. [24] These overall figures fail to bring forward fluctuations that have appeared from year to year and in particular geographic areas. In 2005, when a markedly less-lethal strain in Northern Vietnam was responsible for most of the cases reported worldwide, only 42 of 97 people confirmed by the WHO to be infected with H5N1 died — a 43% fatality rate. In 2006, the case fatality ratio was higher among the WHO -confirmed cases, with 79 deaths among 114 confirmed cases. [25] — or 69%.  In 2007, 59 of the 86 WHO-confirmed cases ended in death, again a 69% fatality rate. [26] And 24 of the first 31 cases of 2008 (to April 30, 2008) have been fatal, [27] or 77%.
The higher total case fatality ratio after the end of 2005 may reflect the widespread circulation in Vietnam of a less-lethal clade of H5N1 in 2005, which was subsequently brought under control. The change was nonetheless interpreted by some as indicating that the virus itself was becoming more deadly over time. [28] In fact, when less-virulent strains die off, the surviving strains are the more virulent. Such difficulties in interpretation underscore that the global case fatality ratio can serve as but a crude and imperfect summary of the current complex situation with its many contributing factors, and not a clear or reliable predictive tool. If and when an influenza pandemic arises from one of the currently circulating pre-pandemic strains of Asian lineage HPAI A(H5N1) , the mortality rates for the resulting human adapted pandemic strain cannot be predicted with any confidence.
The global case fatality ratio looks only to the official tally [29] of cases confirmed by the WHO . It takes no account of other cases, such as those appearing in press reports.  Nor does it reflect any estimate of the global extent of mild, asymptomatic, [30] or other cases which are undiagnosed, unreported by national governments to the WHO , or for any reason cannot be confirmed by the WHO .  While the WHO 's case count is clearly the most authoritative, these unavoidable limitations result in an unknown number of cases being omitted from it. The problem of overlooked but genuine cases is emphasized by occasional reports in which later serology reveals antibodies to the H5N1 infection in the blood of persons who were never known to have bird flu, and who then are confirmed by the WHO only retroactively as "cases."  Press reports of such cases, often poultry handlers, have appeared in various countries.  The largest number of asymptomatic cases was  confirmed in 2006 among Korean workers who had assisted in massive culls of H5N1-infected poultry. [31] This relatively benign Korean strain of H5N1 has died out, and the remaining strains of H5N1 have a higher case fatality rate in humans.
Unconfirmed cases have a potentially huge impact on the case fatality ratio. This mathematical impact is well understood by epidemiologists, and is easy to see in theory.  For example, if for each confirmed case reported by the WHO we assume that there has been another mild and unreported case, the actual global number of cases would be double the current number of WHO -confirmed cases.  The fatality ratio for H5N1 infections would then be calculated as the same number of deaths, but divided by a doubled number for total cases, resulting in a hypothetical death ratio of half the currently reported fatality ratio. Such a result would indicate to epidemiologists that the world was confronting an H5N1 virus that is less-lethal than currently assumed, although possibly one that was more contagious and difficult to track. [32]
A case-fatality ratio based on an accurate and all-inclusive count of cases would be invaluable, but unfortunately it is impossible to attain. The ability to diagnose every case of H5N1 as it arises does not exist.  A few small reported studies have attempted to gather preliminary data on this crucial statistic, by carrying out systematic blood testing of neighbors and contacts of fatal cases in villages where there had been confirmed H5N1 fatalities. In most cases, this testing failed to turn up any overlooked mild cases, though in at least one study mild overlooked cases were identified. [33] [34] [35] These methodical studies of contacts provide significant evidence that the high death rate among confirmed cases in the villages where these studies were carried out cannot be simply attributed to a wholesale failure to detect mild cases. Unfortunately, these studies are likely to remain too few and sketchy to define the complex situation worldwide regarding the lethality of the varying H5N1 clades.  The testing and reporting necessary for mass serology studies to determine the incidence of overlooked cases for each existing clade and strain of H5N1 worldwide would be prohibitively costly.
Hence the precise allocation of infections by the various H5N1 clades across the spectrum including lethal, serious, mild, and asymptomatic cases is likely to remain unknown in both humans and the hundreds of other species it can infect. Scientists are very concerned about what we do know about H5N1 ; but even more concerned about the vast amount of important data that we don't know about H5N1 and its future mutations.
Review of patient ages and outcomes reveals that H5N1 attacks are especially lethal in pre-adults and young adults, while older victims tend to have milder attacks and to survive. [36] [37] [38] This is consistent with the frequent development of a cytokine storm in the afflicted. [39] Few persons over 50 years of age seem to have become infected by H5N1, and very few have died after suffering an H5N1 attack. [40] Instead, the age-fatality curve of H5N1 influenza attacks in humans resembles that of the 1918 Spanish pandemic flu, and is the opposite of the mortality curve of seasonal flu strains, since seasonal influenza preferentially kills the elderly and does not kill by cytokine storm . An additional factor which may be active is that H1N1 was the predominant human flu circulating from 1918 until 1957 when the H2N2 strain emerged. [41] Hence those over 50 years old have had the opportunity to be exposed to H1N1, and to develop some immune response to the N1 group contained in that human form of flu. Likewise, annual flu vaccination includes inoculation against a type-A human H1N1 flu, leading to the possibility that the annual flu shot or Flumist inoculation might confer some immunity against H5N1 bird flu infection, and indeed testing the blood of volunteers to look for immune response to H5N1 found that some blood samples showed immunity, but more of the blood samples of persons who had received the flu shot showed an immune response. [41]
Another factor complicating any attempt to predict lethality of an eventual pandemic strain is the variability of the resistance of human victims to the pathogen.  Many human victims of the current H5N1 influenza have been blood relatives (but rarely [42] spouses) of other victims.  Though this observation seemed to suggest that a familial genetic susceptibility might have played a role in human infection, [43] a study by researchers at the Harvard School of public health noted no significant familial pattern of infection. [44] Clearly, those whose immune systems are best able to fight off the virus are the most likely to survive a pandemic.  Those with impairment of the needed immune function, whether from familial genetics or from AIDS, have poorer chances.  Moreover, the health care system is generally expected to be overwhelmed throughout a pandemic.  Persons needing access to medical care, whether for influenza or for unrelated serious maladies, are unlikely to receive the accustomed care, and without it their survival chances will be reduced.
Although the actual rate of mortality during a pandemic is unknowable in advance, it is pressing to predict the possible ranges for that lethality responsibly in advance. The pre-pandemic case fatality ratio of over 50% provides a grim backdrop for the fact that the currently circulating H5N1 strains have certain genetic similarities with the Spanish Influenza pandemic virus. In that pandemic, 50 million to 100 million people worldwide were killed during about a year in 1918 and 1919. [45] The highly lethal second and third waves of the 1918 Spanish flu evolved through time into a less virulent and more transmissible human form. Although the overall fatality rate for the Spanish flu is estimated to have been 10% to 20% of the population, [ citation needed ] the lethal waves of the Spanish flu are not reported to have emerged with anything like the over-50% case fatality ratio observed to date in human H5N1 infection. Studies indicating that an H5N1 pandemic may be more pathogenic than was the Spanish flu include a mouse study in which the H5N1 virus elicited significantly higher levels of pro-inflammatory cytokines in the lungs. [46]
A human H5N1 pandemic might emerge with initial lethality resembling that over-50% case fatality now observed in pre-pandemic H5N1 human cases, rather than with the still-high 1-2% seen with the Spanish flu or with the lower rates seen in the two more recent influenza pandemics. [47] As a WHO working group noted,
Determinants of virulence and transmissibility.
The U.S. CDC presents a similarly sobering conclusion authored by Robert G. Webster et al.:
Although some mammalian adaptations have been noted, H5N1 remains better adapted for infecting birds than mammalian hosts, [49] which is why the disease it causes is called a bird flu .  No pandemic strain of H5N1 has yet been found. The precise nature and extent of the genetic alterations that might change one of the currently circulating avian influenza strains into a human flu strain cannot be known in advance.
While many of the current H5N1 strains circulating in birds can generate a dangerous cytokine storm in healthy adult humans, the ultimate pandemic strain might arise from a less-lethal strain, or its current level of lethality might be lost in the adaptation to a human host. [50] [51] [52] [53] [54]
If H5N1 mutates so that it can jump from human to human, while maintaining a relatively high level of mortality, how many people could die? Risk communication analysts Peter M. Sandman and Jody Lanard give a round-up of the various estimates:
Worldwide mortality estimates range all the way from 2-7.4 million deaths (the “conservatively low” pandemic influenza calculation of a flu modeling expert at the U.S. Centers for Disease Control and Prevention) to 1000 million deaths (the bird flu pandemic prediction of one Russian virologist). The estimates of most H5N1 experts range less widely but still widely. In an H5N1 pandemic, the experts guess that somewhere between a quarter of us and half of us would get sick, and somewhere between one percent and five percent of those who got sick would die — the young and hale as well as the old and frail. If it's a quarter and one percent, that's 16 million dead; if it's a half and five percent, it's 160 million dead. Either way it's a big number. [55]
The renowned virus expert Robert G. Webster provided perhaps the most extreme estimate when he acknowledged in March 2006 that H5N1 has the theoretical capacity to mutate into a form that could kill one half of the human population, [56] stating, "Society just can't accept the idea that 50 percent of the population could die.  And I think we have to face that possibility". [57]
H5N1 may cause more than one influenza pandemic as it is expected to continue mutating in birds regardless of whether humans develop herd immunity to a future pandemic strain. [58] Influenza pandemics from its genetic offspring may include influenza A virus subtypes other than H5N1. [59] While genetic analysis of the H5N1 virus shows that influenza pandemics from its genetic offspring can easily be far more lethal than the Spanish flu pandemic, [60] planning for a future influenza pandemic is based on what can be done and there is no higher Pandemic Severity Index level than a Category 5 pandemic which, roughly speaking, is any pandemic as bad the Spanish flu or worse; and for which all intervention measures are to be used. [61]
There "is evidence of at least three independent virulence factors connected with three different genes . It is highly unlikely that all of the high-virulence alleles will simultaneously mutate and disappear if and when the haemagglutinin gene changes so as to make the haemagglutinin molecule better adapted for the human-type (alpha-2,6-linked) receptor (which is a necessary prerequisite in order that a pandemic with H5N1 virus may start). It is more probable that evolutionary adaptation of the haemagglutinin of H5N1 viruses to the human-type receptor will happen without any simultaneous change in those other genetic properties that now are important for explaining the exceptionally high virulence of certain strains of avian-adapted H5N1 influenza virus. The change of the haemagglutinin molecule from avian adaptation to human adaptation must be expected to act as an additional virulence factor because it will enhance the total number of cells that can be infected (per host organism), increase the total rate of virus replication and potentiate the effects of the other virulence factors already present." [60] The H5N1 genes work together in ways we don't yet understand. [62] Influenza research is continuing. The genetic factors that make H5N1 so deadly are only partly understood. Known factors involve the surface antigen encoding gene segments H ( hemagglutinin ) [63] and N ( neuraminidase ) genes (causing it to be H5N1 for example), as well as the matrix M2 gene, and the polymerase genes.
A change of just two genes identified in laboratory testing appears to substantially increase the affinity of H5N1 for binding with human cell surface receptors. [13]
Neuraminidase is an antigenic glycoprotein enzyme found on the surface of the influenza viruses . It helps the release of progeny viruses from infected cells. Flu drugs Tamiflu and Relenza  work by inhibiting some strains of neuraminidase . They were developed based on N2 and N9. "In the N1 form of the protein, a small segment called the 150-loop is inverted, creating a hollow pocket that does not exist in the N2 and N9 proteins. [...] When the researchers looked at how existing drugs interacted with the N1 protein, they found that, in the presence of neuraminidase inhibitors, the loop changed its conformation to one similar to that in the N2 and N9 proteins." [65]
The amino acid substitution (Ser31Asn) in the M2 gene in some H5N1 genotypes is associated with amantadine resistance which increases lethality. However the pathogenicity of H5N1/97 was related to the nonstructural (NS) gene. NS codes for two nonstructural proteins (NS1 and NEP). The NS1 gene of the highly pathogenic avian H5N1 viruses circulating in poultry and waterfowl in Southeast Asia is believed to be responsible for an enhanced proinflammatory cytokine response (especially TNFa) induced by these viruses in human macrophages. H5N1 NS1 is characterized by a single amino acid change at position 92. By changing the amino acid from glutamic acid to aspartic acid, researchers were able to abrogate the effect of the H5N1 NS1. This single amino acid change in the NS1 gene greatly increased the pathogenicity of the H5N1 influenza virus. This is one genetic factor in why H5N1 is so deadly.
Polymerase encoding gene segments are also implicated in why H5N1 is so deadly. PA genes code for the PA protein, which is a critical component of the viral polymerase. The PB1 gene codes for the PB1 protein and the PB1-F2 protein. The PB1-F2 protein probably contributes to viral pathogenicity and might have an important role in determining the severity of pandemic influenza. Until H5N1, all known avian influenza viruses had a Glu at position 627, while all human influenza viruses had a lysine.  Recently, some 75% of H5N1 human virus isolates identified in Vietnam had a mutation consisting of Lysine at residue 627 in the PB2 protein; a change believed associated with high levels of virulence.
Areas of research to identify the likelihood of rapid or slow evolution to human contagion, or for predicting the greater or lesser likelihood of a rather lethal human-adapted influenza include:
Computer simulations and direct gene manipulation have yielded inconclusive results.
Scientific advances may attenuate probable lethality. The genetic lethality potential of the initial flu pandemic strain is only one important factor in determining the ultimate outcome in number of human lives lost. Another factor that grows potentially more important with the passage of time is human preparation. For example, no influenza vaccine specific to H5N1 could be produced when it emerged in Hong Kong in 1997, because it was lethal to eggs. Reverse DNA techniques have since made a vaccine possible, and several H5N1 vaccines have been tested and are in production in at least limited quantities. Vaccine development and production facilities are being ramped up, and possible pre-pandemic vaccines are being produced and studied. If a human pandemic does not emerge in the next few years, its eventual emergence may become almost a non-event if a very-effective pre-pandemic vaccine has prepared the population with sufficient herd immunity to blunt its lethality. Indeed, if there is sufficient immunity to stop it at the source, it will not become pandemic.
As long as the likelihood of protecting the population continues to rise with the passage of time, that likelihood becomes an increasingly important factor in predicting the loss of lives and the amount of economic dislocation that will ultimately occur. In light of human potential to develop herd immunity via vaccination in advance of a pandemic strain, the time that it allows us to do so before it evolves may become as crucial or more crucial to the measure of damage it causes than its own lethality and contagiousness.
Among the more attractive alternatives available for reducing mortality is vaccine stockpiling and prepandemic vaccination. "Human H5N1 vaccines are currently available and can induce heterotypic immunity. WHO and governments should give urgent consideration to the use of these vaccines for the priming of individuals or communities who would be at greatest risk of infection if an H5N1 influenza pandemic were to emerge." [66] Death associated with influenza A viruses "is usually mediated by superinfection with bacteria, mainly Streptococcus pneumoniae.", [67] suggesting that lethality may be reduced by vaccination against pneumonia.
Among others, the Secretary of the United States Department of Health and Human Services (HHS) has repeatedly pointed out the key role of preparation in reducing pandemic mortality, including as examples research in cell- and DNA-based vaccines, as well as stockpiling available vaccines and antivirals and increasing vaccine manufacturing capacity. [68]
Governments and other organizations at many levels and in many places have produced "planning" reports that, among other things, have offered speculation on the mortality rate of an eventual H5N1 pandemic. That speculation has varied widely. [69] One such report stated that "over half a million Americans could die and over 2.3 million could be hospitalized if a moderately severe strain of a pandemic flu virus hits the U.S.". [70] No one knew if "moderately severe" was an accurate guess or not. A report entitled A Killer Flu? [71] projected that, with an assumed (guessed) contraction rate of just 25%, and with a severity rate as low as that of the two lowest severity flu pandemics of the 1900s, a modern influenza A pandemic would cause 180 thousand deaths in the US, while a pandemic equaling the 1918 Spanish flu in level of lethality would cause one million deaths in the US. Again, the report offered no evidence that an emerging H5N1 flu pandemic would be between these figures. [72]
The current avian flu, in humans, is fatal in over 50% of confirmed cases.  Yet early projections like those above have assumed that such a lethal avian strain would surely lose genes contributing to its lethality in humans as it made the adaptations necessary for ready transmission in the human population. This optimistic assumption cannot be relied on. As the WHO reported in November 2006, initial outbreaks of an H5N1 pandemic could rival the current lethality of over 50%. [47] Further information necessary to make an accurate projection of initial lethality of an H5N1 pandemic does not exist, as no data was collected that could show the pre-pandemic virulence in any potential flu strain until after the last pandemic of the 20th Century. There is no basis for assuming that an H5N1 pandemic will emerge with only the far lower 1-2% lethality rate of the Spanish flu, once assumed to be a worst-case scenario.  There exists no reliable prediction of the mortality rate of an H5N1 pandemic, and it would be irresponsible to confine planning to only optimistic assumptions out of step with the currently observed case fatality ratio.
Although marred by unrealistically low ranges of assumed mortality, the earlier planning reports nevertheless show convincingly that we are not prepared even for a pandemic as severe as the milder pandemics of the past century., [73] let alone the much higher case fatality ratios seen more recently.
"The viruses that are circulating in Africa and Europe are the ones closest to becoming a human virus," Kawaoka said. But he pointed out that one mutation is not sufficient to turn H5N1 into a major threat to humans.
"... 45 478 randomly selected ( cluster sampling ) inhabitants. Household representatives were asked screening questions about exposure to poultry and flulike illness ...
... A dose-response relationship between poultry exposure and flulike illness was noted: poultry in the household (odds ratio, 1.04; 95% confidence interval, 0.96-1.12), sick or dead poultry in the household but with no direct contact (odds ratio, 1.14; 95% confidence interval, 1.06-1.23), and direct contact with sick poultry (odds ratio, 1.73; 95% confidence interval, 1.58-1.89). The flulike illness attributed to direct contact with sick or dead poultry was estimated to be 650 to 750 cases.
CONCLUSIONS: Our epidemiological data are consistent with transmission of mild, highly pathogenic avian influenza to humans and suggest that transmission could be more common than anticipated, though close contact seems required. Further microbiological studies are needed to validate these findings."
But note the discussion and critique New Study of Bird Flu Raises Important Issues January 9, 2006 "Archived copy" . Archived from the original on 2009-05-15 . Retrieved 2008-02-23 . CS1 maint: archived copy as title ( link )
"Are the conclusions of this one study enough to warrant rethinking the current bird-flu paradigm and considering this threat similar to that posed by the similar "Asian Flu," as opposed to the deadly "Spanish Flu" pandemic? (The Asian Flu pandemic occurred in 1957-8, and caused millions of cases but much lower mortality than the global "Spanish flu" of 1918-9, which killed over 20 million.) Unfortunately, no. While, on its surface, the new study seems to point in that direction, a closer analysis of the study reveals several weaknesses, the most important of which is that no blood samples were taken. As a result, no data on antibody status could be collected, nor could there be any confirmation of a specific viral cause of the reported ailments.
Indeed, it is just as likely that the illnesses sustained by the rural Vietnamese were caused by some other virus, not a bird-type flu at all — or that if their ailments were due to bird contact, that the cause was any number of bird flu variants, rather than the lethal H5N1 strain being studied intensively now. ... "
Human-to-human transmission (HHT) is a particularly problematic epidemiologic vector , [1] [2] [3] [4] [5] [6] [7] [8] especially in case the disease is borne by individuals known as superspreaders . In these cases, the basic reproduction number of the virus, which is the average number of additional people that a single case will infect without any preventative measures, can be as high as 3.9. [9] [10] Interhuman transmission is a synonym for HHT. [11]
The World Health Organization designation of a pandemic hinges on the demonstrable fact that there is sustained HHT in two regions of the world. [12]
Relevant pathogens may be viruses , bacteria , or fungi , and they may be spread through breathing, talking, coughing, sneezing, spraying of liquids, toilet flushing or any activities which generate aerosol particles or droplets or generate fomites , such as raising of dust. [13] [14]
A 2007 study showed that influenza virus was still active on stainless steel 24 hours after contamination. Though on hands it survives only for five minutes, the constant contact with steel almost certainly transmits infection. [15] Transfer efficiency depends not only on surface, but also on pathogen type. For example, avian influenza survives on both porous and non-porous materials for 144 hours. [13]
The pathogens may also be transmitted by poor use of cutlery or improper sanitation of dishes or bedlinen. Particularly problematic are toilet practices, which lead to the fecal-oral route . STDs are by definition spread through this vector. [ citation needed ]
Examples of some HHT diseases are listed below.
Human-to-primate transmission (HPT) is a seldom-remarked epidemiologic vector . It is by definition a cross-species transmission vector.
In 1998, The Zoonotic Importance of Mycobacterium Tuberculosis: Transmission From Human to Monkey was noticed. [1]
In 2001, scientists noticed that antibodies peculiar to humans were found in macaque monkeys , both wild and domesticated. Of the panel of human viruses studied, measles , influenza A , and parainfluenza 1, 2 and 3 were found in some of the studied animals. [2]
In 2006, scientists noticed HPT of measles , rubella , and parainfluenza in the case of performing monkeys , who are "a common phenomena in Asia". [3]
In 2015 and  again in 2016, scientists found that HPT likely had occurred in the case of Staphylococcus aureus . [4] [5]
In 2018, scientists speculated that HPT was likely to have occurred in the case of human malaria parasites . [6]
Also in 2018, scientists speculated that an outbreak of tuberculosis among a closed breeding colony of Aotus monkeys was likely due to continuing intermittent HPT exposure. [7]

In medicine, the hygiene hypothesis states that early childhood exposure to particular microorganisms (such as the gut flora and helminth parasites) protects against allergic diseases by contributing to the development of the immune system . [1] [2] In particular, a lack of exposure is thought to lead to defects in the establishment of immune tolerance . [1] The time period for exposure begins in utero and ends at school age. [3]
While early versions of the hypothesis referred to exposure to microbes in general, updated versions apply to a specific set of microbial species that have co-evolved with humans. [1] [4] [2] The updates have been given various names, including the microbiome depletion hypothesis, the microflora hypothesis, and the "old friends" hypothesis. [4] [5] There is a significant amount of evidence supporting the idea that lack of exposure to these microbes is linked to allergies or other conditions, [2] [6] [7] although scientific disagreement still exists. [4] [8] [9]
The term "hygiene hypothesis" has been described as a misnomer because people incorrectly interpret it as referring to personal cleanliness. [1] [8] [10] [11] Reducing personal hygiene, such as not washing hands before eating, is expected to simply increase the risk of infection without having any impact on allergies or immune disorders. [1] [4] [9] Hygiene is essential for protecting vulnerable populations such as the elderly from infections, preventing the spread of antibiotic resistance , and for combating emerging infectious diseases such as Ebola or COVID-19 . [12] The hygiene hypothesis does not suggest that having more infections during childhood would be an overall benefit. [1] [8]
The idea of a link between parasite infection and immune disorders was first suggested in 1968. [13] The original formulation of the hygiene hypothesis dates from 1989, when David Strachan proposed that lower incidence of infection in early childhood could be an explanation for the rise in allergic diseases such as asthma and hay fever during the 20th century. [14]
The hygiene hypothesis has also been expanded beyond allergies, and is also studied in the context of a broader range of conditions affected by the immune system, particularly inflammatory diseases . [15] These include type 1 diabetes , [16] multiple sclerosis, [17] [10] and also some types of depression [17] [18] and cancer. [19] [ specify ] For example, the global distribution of multiple sclerosis is negatively correlated with that of the helminth Trichuris trichiura and its incidence is negatively correlated with Helicobacter pylori infection. [10] Strachan's original hypothesis could not explain how various allergic conditions spiked or increased in prevalence at different times, such as why respiratory allergies began to increase much earlier than food allergies, which did not become more common until near the end of the 20th century. [12]
In 2003 Graham Rook proposed the "old friends hypothesis" which has been described as a more rational explanation for the link between microbial exposure and inflammatory disorders. [20] The hypothesis states that the vital microbial exposures are not colds, influenza, measles and other common childhood infections which have evolved relatively recently over the last 10,000 years, but rather the microbes already present during mammalian and human evolution, that could persist in small hunter-gatherer groups as microbiota, tolerated latent infections, or carrier states. He proposed that coevolution with these species has resulted in their gaining a role in immune system development.
Strachan's original formulation of the hygiene hypothesis also centred around the idea that smaller families provided insufficient microbial exposure partly because of less person-to-person spread of infections, but also because of "improved household amenities and higher standards of personal cleanliness". [14] It seems likely that this was the reason he named it the "hygiene hypothesis". Although the "hygiene revolution" of the nineteenth and twentieth centuries may have been a major factor, it now seems more likely that, although public health measures such as sanitation , potable water and garbage collection were instrumental in reducing our exposure to cholera , typhoid and so on, they also deprived people of their exposure to the "old friends" that occupy the same environmental habitats. [21]
The rise of autoimmune diseases and acute lymphoblastic leukemia in young people in the developed world was linked to the hygiene hypothesis. [22] [23] [24] Autism may be associated with changes in the gut microbiome and early infections. [25] The risk of chronic inflammatory diseases also depends on factors such as diet, pollution, physical activity, obesity, socio-economic factors and stress. Genetic predisposition is also a factor. [26] [27] [28]
Since allergies and other chronic inflammatory diseases are largely diseases of the last 100 years or so, the "hygiene revolution" of the last 200 years came under scrutiny as a possible cause. During the 1800s, radical improvements to sanitation and water quality occurred in Europe and North America. The introduction of toilets and sewer systems and the cleanup of city streets, and cleaner food were part of this program. This in turn led to a rapid decline in infectious diseases, particularly during the period 1900–1950, through reduced exposure to infectious agents. [21]
Although the idea that exposure to certain infections may decrease the risk of allergy is not new, Strachan was one of the first to formally propose it, in an article published in the British Medical Journal in 1989. This article proposed to explain the observation that hay fever and eczema , both allergic diseases, were less common in children from larger families, which were presumably exposed to more infectious agents through their siblings, than in children from families with only one child. [29] The increased occurrence of allergies had previously been thought to be a result of increasing pollution. [8] The hypothesis was extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of chronic inflammatory disorders.
The "old friends hypothesis" proposed in 2003 [20] may offer a better explanation for the link between microbial exposure and inflammatory diseases. [18] [20] This hypothesis argues that the vital exposures are not common childhood and other recently evolved infections, which are no older than 10,000 years, but rather microbes already present in hunter-gatherer times when the human immune system was evolving. Conventional childhood infections are mostly " crowd infections " that kill or immunise and thus cannot persist in isolated hunter-gatherer groups. Crowd infections started to appear after the neolithic agricultural revolution, when human populations increased in size and proximity. The microbes that co-evolved with mammalian immune systems are much more ancient. According to this hypothesis, humans became so dependent on them that their immune systems can neither develop nor function properly without them.
Rook proposed that these microbes most likely include:
The modified hypothesis later expanded to include exposure to symbiotic bacteria and parasites. [30]
"Evolution turns the inevitable into a necessity." This means that the majority of mammalian evolution took place in mud and rotting vegetation and more than 90 percent of human evolution took place in isolated hunter-gatherer communities and farming communities. Therefore, the human immune systems have evolved to anticipate certain types of microbial input, making the inevitable exposure into a necessity. The organisms that are implicated in the hygiene hypothesis are not proven to cause the disease prevalence, however there are sufficient data on lactobacilli, saprophytic environment mycobacteria, and helminths and their association. These bacteria and parasites have commonly been found in vegetation, mud, and water throughout evolution. [18] [20]
Multiple possible mechanisms have been proposed for how the 'Old Friends' microorganisms prevent autoimmune diseases and asthma.  They include:
The "microbial diversity" hypothesis, proposed by Paolo Matricardi and developed by von Hertzen, [31] [32] holds that diversity of microbes in the gut and other sites is a key factor for priming the immune system, rather than stable colonization with a particular species. Exposure to diverse organisms in early development builds a "database" that allows the immune system to identify harmful agents and normalize once the danger is eliminated.
For allergic disease, the most important times for exposure are: early in development; later during pregnancy; and the first few days or months of infancy. Exposure needs to be maintained over a significant period. This fits with evidence that delivery by Caesarean section may be associated with increased allergies, whilst breastfeeding can be protective. [21]
Humans and the microbes they harbor have co-evolved for thousands of centuries; however, it is thought that the human species has gone through numerous phases in history characterized by different pathogen exposures.  For instance, in very early human societies, small interaction between its members has given particular selection to a relatively limited group of pathogens that had high transmission rates. It is considered that the human immune system is likely subjected to a selective pressure from pathogens that are responsible for down regulating certain alleles and therefore phenotypes in humans. The thalassemia genes that are shaped by the Plasmodium species expressing the selection pressure might be a model for this theory [33] but is not shown in-vivo.
Recent comparative genomic studies have shown that immune response genes (protein coding and non-coding regulatory genes) have less evolutionary constraint, and are rather more frequently targeted by positive selection from pathogens that coevolve with the human subject.  Of all the various types of pathogens known to cause disease in humans, helminths warrant special attention, because of their ability to modify the prevalence or severity of certain immune-related responses in human and mouse models.  In fact recent research has shown that parasitic worms have served as a stronger selective pressure on select human genes encoding interleukins and interleukin receptors when compared to viral and bacterial pathogens.  Helminths are thought to have been as old as the adaptive immune system , suggesting that they may have co-evolved, also implying that our immune system has been strongly focused on fighting off helminthic infections, insofar as to potentially interact with them early in infancy.  The host-pathogen interaction is a very important relationship that serves to shape the immune system development early on in life. [34] [35] [36] [37]
The primary proposed mechanism of the hygiene hypothesis is an imbalance between the T H 1 and T H 2 subtypes of T helper cells . [10] [38] Insufficient activation of the T H 1 arm would stimulate the cell defense of the immune system and lead to an overactive T H 2 arm, stimulating the antibody-mediated immunity of the immune systems, which in turn led to allergic disease. [39]
However, this explanation cannot explain the rise in incidence (similar to the rise of allergic diseases) of several T H 1-mediated autoimmune diseases , including inflammatory bowel disease , multiple sclerosis and type I diabetes . [Figure 1Bach] However, the North South Gradient seen in the prevalence of multiple sclerosis has been found to be inversely related to the global distribution of parasitic infection.[Figure 2Bach] Additionally, research has shown that MS patients infected with parasites displayed T H 2 type immune responses as opposed to the proinflammatory T H 1 immune phenotype seen in non-infected multiple sclerosis patients.[Fleming] Parasite infection has also been shown to improve inflammatory bowel disease and may act in a similar fashion as it does in multiple sclerosis.[Lee] [ citation needed ]
Allergic conditions are caused by inappropriate immunological responses to harmless antigens driven by a T H 2 -mediated immune response, T H 2 cells produce interleukin 4 , interleukin 5 , interleukin 6 , interleukin 13 and predominantly immunoglobulin E . [23] Many bacteria and viruses elicit a T H 1 -mediated immune response, which down-regulates T H 2 responses. T H 1 immune responses are characterized by the secretion of pro-inflammatory cytokines such as interleukin 2 , IFNγ , and TNFα . Factors that favor a predominantly T H 1 phenotype include: older siblings, large family size, early day care attendance, infection (TB, measles, or hepatitis), rural living, or contact with animals. A T H 2-dominated phenotype is associated with high antibiotic use, western lifestyle, urban environment, diet, and sensitivity to dust mites and cockroaches. T H 1 and T H 2 responses are reciprocally inhibitory, so when one is active, the other is suppressed. [40] [41] [42]
An alternative explanation is that the developing immune system must receive stimuli (from infectious agents, symbiotic bacteria, or parasites) to adequately develop regulatory T cells . Without that stimuli it becomes more susceptible to autoimmune diseases and allergic diseases, because of insufficiently repressed T H 1 and T H 2 responses, respectively. [43] For example, all chronic inflammatory disorders show evidence of failed immunoregulation. [26] Secondly, helminths, non-pathogenic ambient pseudocommensal bacteria or certain gut commensals and probiotics , drive immunoregulation. They block or treat models of all chronic inflammatory conditions. [44]
There is a significant amount of evidence supporting the idea that microbial exposure is linked to allergies or other conditions, [2] [6] [7] although scientific disagreement still exists. [4] [8] [9] Since hygiene is difficult to define or measure directly, surrogate markers are used such as socioeconomic status, income, and diet. [38]
Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world. [23] This is true for asthma and other chronic inflammatory disorders. [18] The increase in allergy rates is primarily attributed to diet and reduced microbiome diversity, although the mechanistic reasons are unclear. [45]
The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases, [46] and increased asthma rates are also associated with birth by Caesarean section . [47] However, at least one study suggests that personal hygienic practices may be unrelated to the incidence of asthma. [9] Antibiotic usage reduces the diversity of gut microbiota. Although several studies have shown associations between antibiotic use and later development of asthma or allergy, other studies suggest that the effect is due to more frequent antibiotic use in asthmatic children. Trends in vaccine use may also be relevant, but epidemiological studies provide no consistent support for a detrimental effect of vaccination/immunization on atopy rates. [21] In support of the old friends hypothesis, the intestinal microbiome was found to differ between allergic and non-allergic Estonian and Swedish children (although this finding was not replicated in a larger cohort), and the biodiversity of the intestinal flora in patients with Crohn's disease was diminished. [23]
The hygiene hypothesis does not apply to all populations. [9] [38] For example, in the case of inflammatory bowel disease , it is primarily relevant when a person's level of affluence increases, either due to changes in society or by moving to a more affluent country, but not when affluence remains constant at a high level. [38]
The hygiene hypothesis has difficulty explaining why allergic diseases also occur in less affluent regions. [9] Additionally, exposure to some microbial species actually increases future susceptibility to disease instead, as in the case of infection with rhinovirus (the main source of the common cold ) which increases the risk of asthma. [4] [48]
Current research suggests that manipulating the intestinal microbiota may be able to treat or prevent allergies and other immune-related conditions. [2] Various approaches are under investigation. Probiotics (drinks or foods) have never been shown to reintroduce microbes to the gut. As yet, therapeutically relevant microbes have not been specifically identified. [49] However, probiotic bacteria have been found to reduce allergic symptoms in some studies. [15] Other approaches being researched include prebiotics , which promote the growth of gut flora, and synbiotics , the use of prebiotics and probiotics at the same time. [2]
Should these therapies become accepted, public policy implications include providing green spaces in urban areas or even providing access to agricultural environments for children. [50]
Helminthic therapy is the treatment of autoimmune diseases and immune disorders by means of deliberate infestation with a helminth larva or ova . Helminthic therapy emerged from the search for reasons why the incidence of immunological disorders and autoimmune diseases correlates with the level of industrial development. [51] [52] The exact relationship between helminths and allergies is unclear, in part because studies tend to use different definitions and outcomes, and because of the wide variety among both helminth species and the populations they infect. [53] The infections induce a type 2 immune response, which likely evolved in mammals as a result of such infections; chronic helminth infection has been linked with a reduced sensitivity in peripheral T cells, and several studies have found deworming to lead to an increase in allergic sensitivity. [54] [13] However, in some cases helminths and other parasites are a cause of developing allergies instead. [4] In addition, such infections are not themselves a treatment as they are a major disease burden and in fact they are one of the most important neglected diseases . [54] [13] The development of drugs that mimic the effects without causing disease is in progress. [4]
The reduction of public confidence in hygiene has significant possible consequences for public health. [12] Hygiene is essential for protecting vulnerable populations such as the elderly from infections, preventing the spread of antibiotic resistance , and for combating emerging infectious diseases such as SARS and Ebola . [12]
The misunderstanding of the term "hygiene hypothesis" has resulted in unwarranted opposition to vaccination as well as other important public health measures.” [8] It has been suggested that public awareness of the initial form of the hygiene hypothesis has led to an increased disregard for hygiene in the home. [55] The effective communication of science to the public has been hindered by the presentation of the hygiene hypothesis and other health-related information in the media. [12]
No evidence supports the idea that reducing modern practices of cleanliness and hygiene would have any impact on rates of chronic inflammatory and allergic disorders, but a significant amount of evidence that it would increase the risks of infectious diseases. [21] The phrase "targeted hygiene" has been used in order to recognize the importance of hygiene in avoiding pathogens. [1]
If home and personal cleanliness contributes to reduced exposure to vital microbes, its role is likely to be small. The idea that homes can be made “sterile” through excessive cleanliness is implausible, and the evidence shows that after cleaning, microbes are quickly replaced by dust and air from outdoors, by shedding from the body and other living things, as well as from food. [21] [56] [57] [58] The key point may be that the microbial content of urban housing has altered, not because of home and personal hygiene habits, but because they are part of urban environments. Diet and lifestyle changes also affects the gut, skin and respiratory microbiota.
At the same time that concerns about allergies and other chronic inflammatory diseases have been increasing, so also have concerns about infectious disease. [21] [59] [60] Infectious diseases continue to exert a heavy health toll. Preventing pandemics and reducing antibiotic resistance are global priorities, and hygiene is a cornerstone of containing these threats.
The International Scientific Forum on Home Hygiene has developed a risk management approach to reducing home infection risks. This approach uses microbiological and epidemiological evidence to identify the key routes of infection transmission in the home. These data indicate that the critical routes involve the hands, hand and food contact surfaces and cleaning utensils. Clothing and household linens involve somewhat lower risks. Surfaces that contact the body, such as baths and hand basins, can act as infection vehicles, as can surfaces associated with toilets. Airborne transmission can be important for some pathogens. A key aspect of this approach is that it maximises protection against pathogens and infection, but is more relaxed about visible cleanliness in order to sustain normal exposure to other human, animal and environmental microbes. [56]
In epidemiology , incidence is a measure of the probability of occurrence of a given medical condition in a population within a specified period of time. Although sometimes loosely expressed simply as the number of new cases during some time period, it is better expressed as a proportion or a rate [1] with a denominator .
Incidence proportion ( IP ), also known as cumulative incidence , is defined as the probability that a particular event, such as occurrence of a particular disease, has occurred before a given time. [2]
It is calculated dividing the number of new cases during a given period by the number of subjects at risk in the population initially at risk at the beginning of the study.
Where the period of time considered is an entire lifetime, the incidence proportion is called lifetime risk . [3]
For example, if a population initially contains 1,000 persons and 28 develop a condition since the disease first occurred until a certain point in time, the cumulative incidence proportion is 28 cases per 1,000 persons, i.e. 2.8%.
IP is related to incidence rate (IR) and duration of exposure (D) as follows: [4]
The incidence rate is a measure of the frequency with which a disease or other incident occurs over a specified time period. [5] [6] It is also known as the incidence density rate or person-time incidence rate , [7] when the denominator is the combined person-time of the population at risk (the sum of the time duration of exposure across all persons exposed). [8]
In the same example as above, the incidence rate is 14 cases per 1000 person-years , because the incidence proportion (28 per 1,000) is divided by the number of years (two). Using person-time rather than just time handles situations where the amount of observation time differs between people, or when the population at risk varies with time. [9]
Use of this measure implies the assumption that the incidence rate is constant over different periods of time, such that for an incidence rate of 14 per 1000 persons-years, 14 cases would be expected for 1000 persons observed for 1 year or 50 persons observed for 20 years. [10] When this assumption is substantially violated, such as in describing survival after diagnosis of metastatic cancer, it may be more useful to present incidence data in a plot of cumulative incidence, over time, taking into account loss to follow-up, using a Kaplan-Meier Plot .
Incidence should not be confused with prevalence , which is the proportion of cases in the population at a given time rather than rate of occurrence of new cases. Thus, incidence conveys information about the risk of contracting the disease, whereas prevalence indicates how widespread the disease is. Prevalence is the proportion of the total number of cases to the total population and is more a measure of the burden of the disease on society with no regard to time at risk or when subjects may have been exposed to a possible risk factor. Prevalence can also be measured with respect to a specific subgroup of a population (see: denominator data ). Incidence is usually more useful than prevalence in understanding the disease etiology: for example, if the incidence rate of a disease in a population increases, then there is a risk factor that promotes the incidence.
For example, consider a disease that takes a long time to cure and was widespread in 2002 but dissipated in 2003. This disease will have both high incidence and high prevalence in 2002, but in 2003 it will have a low incidence yet will continue to have a high prevalence (because it takes a long time to cure, so the fraction of individuals that are affected remains high). In contrast, a disease that has a short duration may have a low prevalence and a high incidence. When the incidence is approximately constant for the duration of the disease, prevalence is approximately the product of disease incidence and average disease duration, so prevalence = incidence × duration . The importance of this equation is in the relation between prevalence and incidence; for example, when the incidence increases, then the prevalence must also increase. Note that this relation does not hold for age-specific prevalence and incidence, where the relation becomes more complicated. [11]
Consider the following example. Say you are looking at a sample population of 225 people, and want to determine the incidence rate of developing HIV over a 10-year period:
If you were to measure prevalence you would simply take the total number of cases (25 + 20 + 30 = 75) and divide by your sample population (225). So prevalence would be 75/225 = 0.33 or 33% (by the end of the study). This tells you how widespread HIV is in your sample population, but little about the actual risk of developing HIV for any person over a coming year.
To measure incidence you must take into account how many years each person contributed to the study, and when they developed HIV. When it is not known exactly when a person develops the disease in question, epidemiologists frequently use the actuarial method, and assume it was developed at a half-way point between follow-ups. [ citation needed ] In this calculation:
That is a total of (1500 + 275) = 1775 person-years of life. Now take the 50 new cases of HIV, and divide by 1775 to get 0.028, or 28 cases of HIV per 1000 population, per year. In other words, if you were to follow 1000 people for one year, you would see 28 new cases of HIV.
This is a much more accurate measure of risk than prevalence.
Incubation period is the time elapsed between exposure to a pathogenic organism, a chemical, or radiation , and when symptoms and signs are first apparent. [1] In a typical infectious disease, the incubation period signifies the period taken by the multiplying organism to reach a threshold necessary to produce symptoms in the host.
While latent or latency period may be synonymous, a distinction is sometimes made between incubation period , the period between infection and onset of the disease, and latent period , the time from infection to infectiousness. Which period is shorter depends on the disease. A person may carry disease, such as Streptococcus in the throat, without exhibiting any symptoms. Depending on the disease, the person may or may not be contagious during the incubation period.
During latency, an infection is subclinical . With respect to viral infections , in incubation the virus is replicating. [2] This is in contrast to viral latency , a form of dormancy in which the virus does not replicate. An example of latency is HIV infection. HIV may at first have no symptoms and show no signs of AIDS , despite HIV replicating in the lymphatic system and rapidly accumulating a large viral load . These persons may be infectious .
The terms "intrinsic incubation period" and "extrinsic incubation period" are used in vector-borne diseases . The intrinsic incubation period is the time taken by an organism to complete its development in the definitive host . The extrinsic incubation period is the time taken by an organism to develop in the intermediate host .
For example, once ingested by a mosquito, malaria parasites must undergo development within the mosquito before they are infectious to humans. The time required for development in the mosquito ranges from 10 to 28 days, depending on the parasite species and the temperature. This is the extrinsic incubation period of that parasite. If a female mosquito does not survive longer than the extrinsic incubation period, then she will not be able to transmit any malaria parasites.
But if a mosquito successfully transfers the parasite to a human body via a bite, the parasite starts developing. The time between the injection of the parasite into the human and the development of the first symptoms of malaria is its intrinsic incubation period. [3]
The specific incubation period for a disease process is the result of multiple factors, including:
Due to inter-individual variation, the incubation period is always expressed as a range. When possible, it is best to express the mean and the 10th and 90th percentiles, though this information is not always available.
For many conditions, incubation periods are longer in adults than they are in children or infants.
The index case or patient zero is the first documented patient in a disease epidemic within a population, [1] or the first documented patient included in an epidemiological study. [2] It can also refer to the first case of a condition or syndrome (not necessarily contagious) to be described in the medical literature, whether or not the patient is thought to be the first person affected. [ citation needed ] An index case can achieve the status of a "classic" case study in the literature, as did Phineas Gage , the first known person to exhibit a definitive personality change as a result of a brain injury. [3]
The index case may or may not indicate the source of the disease , the possible spread, or which reservoir holds the disease in between outbreaks , but may bring awareness of an emerging outbreak. [4] [5] Earlier cases may or may not be found and are labeled primary or coprimary, secondary, tertiary, etc. [4] The term primary case can only apply to infectious diseases that spread from human to human, and refers to the person who first brings a disease into a group of people. [5] In epidemiology, the term is often used by both scientists and journalists alike to refer to the individual known or believed to have been the first infected or source of the resulting outbreak in a population as the index case, but such would technically refer to the primary case. [5] [6]
Patient zero was used to refer to the supposed source of HIV outbreak in the United States , flight attendant Gaëtan Dugas , but later research found he was not the index case of this outbreak. [7] The expression was based on a misunderstanding: in the 1984 study of Centers for Disease Control and Prevention (CDC), one of the earliest recorded HIV-patients was code-named "patient O", which stands for "patient o ut of California". The letter O , however, was interpreted by some readers of the report as the numeral 0 . The designation patient zero (for Gaëtan Dugas) was subsequently propagated by the San Francisco Chronicle journalist Randy Shilts in his book And the Band Played On in 1987. William Darrow,  behavioral scientist of CDC responsible to figure out why gay men in Los Angeles were dying of a strange illness, said: "That's correct. I never labeled him Patient Zero". [8]
The term has been expanded into general usage to refer to an individual identified as the first carrier of a communicable disease in a population (the primary case), or the first incident in the onset of a catastrophic trend. [9] [10] In some cases, a known or suspected patient zero may be informally referred to as an index case for the purpose of a scientific study, such as the two-year-old boy in a remote village in Guinea who was thought to be the source of the largest Ebola virus outbreak in history. [2] [11]
In genetics, the index case is the case of the original patient (i.e. propositus or proband ) that stimulates investigation of other members of the family to discover a possible genetic factor. [12]
The term can also be used in non-medical fields to describe the first individual affected by something negative that since propagated to others, such as the first user on a network infected by malware . [13]
In the early years of the AIDS epidemic , a patient zero transmission scenario was compiled by Dr. William Darrow and colleagues at the United States Centers for Disease Control and Prevention (CDC). [15] This epidemiological study showed how patient zero had infected multiple partners with HIV , and they, in turn, transmitted it to others causing rapid spread of the virus to locations all over the world (Auerbach et al., 1984). The CDC identified Gaëtan Dugas as a carrier of the virus from Europe to the United States, who spread it to other men he had intimate relations with at gay bathhouses . [16]
Journalist Randy Shilts subsequently wrote about patient zero, based on Darrow's findings, [15] in his 1987 book And the Band Played On , which identified patient zero as being Gaëtan Dugas. [17] Dugas was a flight attendant who was sexually promiscuous in several North American cities, according to Shilts' book. He was vilified for several years as a "mass spreader" of HIV, and was seen as the original source of the HIV epidemic among homosexual men . Four years later, Darrow repudiated the study's methodology and how Shilts had represented its conclusions. [15]
A 2007 study by Michael Worobey and Dr. Arthur Pitchenik published in the Proceedings of the National Academy of Sciences of the United States of America claimed that, based on the results of genetic analysis , current North American strains of HIV probably moved from Africa to Haiti before entering the United States around 1969, [18] probably through a single immigrant . However, a teenager named Robert Rayford died in St. Louis , Missouri , possibly of complications from AIDS in 1969, having most likely become infected with the virus before 1966. This would imply that there were prior carriers of HIV-strains in North America. [19] [20]
The phrase patient zero is now used in the media to refer to the primary case for infectious disease outbreaks, as well as for computer virus outbreaks, and more broadly, as the source of ideas or actions that have far-reaching consequences. [21] [22] [23] [24] [25]
David Heymann , professor of Infectious Disease Epidemiology at the London School of Hygiene & Tropical Medicine, and formerly with the World Health Organization ( WHO ), [26] has questioned the importance of finding patient zero, stating, "Finding patient zero may be important in some instances, but only if they are still alive and spreading the disease; and more often than not, especially in large disease outbreaks, they're not." [27]
The term is used to identify the first computer or user to be infected with malware on a network, which then infected other systems. [13] [39]
Monica Lewinsky has described herself as the "patient zero" of online harassment, meaning that she was the first person to receive widespread public harassment via the internet. [40]
The thirteenth season of the WNYC radio series Radiolab included an hour-long segment on patient zero. [41]
Indicator bacteria are types of bacteria used to detect and estimate the level of fecal contamination of water. They are not dangerous to human health but are used to indicate the presence of a health risk.
Each gram of human feces contains approximately ~100 billion ( 1 × 10 11 ) bacteria. [1] These bacteria may include species of pathogenic bacteria, such as Salmonella or Campylobacter , associated with gastroenteritis . In addition, feces may contain pathogenic viruses , protozoa and parasites . Fecal material can enter the environment from many sources including waste water treatment plants , livestock or poultry manure, sanitary landfills, septic systems , sewage sludge , pets and wildlife. If sufficient quantities are ingested, fecal pathogens can cause disease. The variety and often low concentrations of pathogens in environmental waters makes them difficult to test for individually. Public agencies therefore use the presence of other more abundant and more easily detected fecal bacteria as indicators of the presence of fecal contamination.
The US Environmental Protection Agency (EPA) lists the following criteria for an organism to be an ideal indicator of fecal contamination: [ citation needed ]
None of the types of indicator organisms that are currently in use fit all of these criteria perfectly, however, when cost is considered, use of indicators becomes necessary.
Commonly used indicator bacteria include total coliforms, or a subset of this group, fecal coliforms , which are found in the intestinal tracts of warm blooded animals. Total coliforms were used as fecal indicators by public agencies in the US as early as the 1920s.  These organisms can be identified based on the fact that they all metabolize the sugar lactose, producing both acid and gas as byproducts.  Fecal coliforms are more useful as indicators in recreational waters than total coliforms which include species that are naturally found in plants and soil; however, there are even some species of fecal coliforms that do not have a fecal origin, such as Klebsiella pneumoniae .  Perhaps the biggest drawback to using coliforms as indicators is that they can grow in water under certain conditions.
Escherichia coli ( E. coli ) and enterococci are also used as indicators.
Indicator bacteria can be cultured on media which are specifically formulated to allow the growth of the species of interest and inhibit growth of other organisms.  Typically, environmental water samples are filtered through membranes with small pore sizes and then the membrane is placed onto a selective agar.  It is often necessary to vary the volume of water sample filtered in order to prevent too few or too many colonies from forming on a plate. Bacterial colonies can be counted after 24 to 48 hours depending on the type of bacteria. Counts are reported as colony forming units per 100 mL (cfu/100 mL).
One technique for detecting indicator organisms is the use of chromogenic compounds, which are added to conventional or newly devised media used for isolation of the indicator bacteria. These chromogenic compounds are modified to change color or fluorescence by the addition of either enzymes or specific bacterial metabolites. This enables for easy detection and avoids the need for isolation of pure cultures and confirmatory tests. [2]
Immunological methods using monoclonal antibodies can be used to detect indicator bacteria in water samples. Precultivation in select medium must preface detection to avoid detection of dead cells. ELISA antibody technology has been developed to allow for readable detection by the naked eye for rapid identification of coliform microcolonies . Other uses of antibodies in detection use magnetic beads coated with antibodies for the concentration and separation of the oocysts and cysts as described below for immunomagnetic separation (IMS) methods. [2]
Immunomagnetic separation involves purified antigens biotinylated and bound to streptoavidin-coated paramagnetic particles. The raw sample is mixed with the beads, then a specific magnet is used to hold the target organisms against the vial wall and the non-bound material is poured off. This method can be used to recover specific indicator bacteria. [2]
Gene sequence-based methods depend on the recognition of exclusive gene sequences particular to specific strains of organisms. Polymerase chain reaction (PCR) and fluorescence in situ hybridization (FISH) are gene sequence-based methods currently being used to detect specific strains of indicator bacteria. [2]
World Health Organization Guidelines for Drinking Water Quality state that as an indicator organism Escherichia coli provides conclusive evidence of recent fecal pollution and should not be present in water meant for human consumption. In the U.S., the EPA Total Coliform Rule states that a water system is out of compliance if more than 5 percent of its monthly water samples contain coliforms. [3]
Early studies showed that individuals who swam in waters with geometric mean coliform densities above 2300/100 mL for three days had higher illness rates. [4] In the 1960s, these numbers were converted to fecal coliform concentrations assuming 18 percent of total coliforms were fecal. Consequently, the National Technical Advisory Committee in the US recommended the following standard for recreational waters in 1968: 10 percent of total samples during any 30-day period should not exceed 400 fecal coliforms/100 mL or a log mean of 200/100 mL (based on a minimum of 5 samples taken over not more than a 30-day period). [5]
Despite criticism, EPA recommended this criterion again in 1976, however, the Agency initiated numerous studies in the 1970s and 1980s to overcome the weaknesses of the earlier studies. In 1986, EPA revised its bacteriological ambient water quality criteria recommendations to include E. coli and enterococci.
[5]
Canada's National Agri-Environmental Standards Initiative's approach to characterizing risks associated with fecal water pollution bacterial water quality at agricultural sites is to compare these sites with those at reference sites away from human or livestock sources.  This approach generally results in lower levels if E. coli being used as a standard or “benchmark” based on a study that indicated pathogens were detected in 80% of water samples with less than 100 cfu E. coli per 100 mL. [6]
Most cases of bacterial gastroenteritis are caused by food-borne enteric microorganisms, such as Salmonella and Campylobacter ; however, it is also important to understand the risk of exposure to pathogens via recreational waters.  This is especially the case in watersheds where human or animal wastes are discharged to streams and downstream waters are used for swimming or other recreational activities. Other important pathogens other than bacteria include viruses such as rotavirus , hepatitis A and hepatitis E and protozoa like giardia , cryptosporidium and Naegleria fowleri . [7] Due to the difficulties associated with monitoring pathogens in the environment, risk assessments often rely on the use of indicator bacteria.
In the 1950s, a series of epidemiological studies were done in the US to determine the relationship between water quality of natural waters and the health of bathers. The results indicated that swimmers were more likely to have gastrointestinal symptoms, eye infections, skin complaints, ear, nose, and throat infections and respiratory illness than non-swimmers and in some cases, higher coliform levels correlated to higher incidence of gastrointestinal illness, although the sample sizes in these studies were small.  Since then, studies have been done to confirm causative relations between swimming and certain health outcomes.  A review of 22 studies in 1998 [8] confirmed that the health risks for swimmers increased as the number of indicator bacteria increased in recreational waters and that E. coli and enterococci concentrations correlated best with health outcomes among all the indicators studied.  The relative risk (RR) of illness for swimmers in polluted freshwater versus swimmers in unpolluted water was between 1-2 for the majority of the data sets reviewed.  The same study concluded that bacterial indicators were not well correlated to virus concentrations. [8]
Survival of pathogens in waste materials, soil, or water, depends on many environmental factors including temperature, pH, organic matter content, moisture, exposure to light, and the presence of other organisms. [9] Fecal material can be directly deposited, washed into waters by overland runoff, transported through the ground, or discharged to surface waters via sewer lines, pipes, or drainage tiles. Risk of exposure to humans requires: (1) pathogens to survive and be present; (2) pathogens to recreate in surface waters; and (3) individuals to come in contact with water for sufficient time, or ingest sufficient volumes of water to receive an infectious dose.  Die-off rates of bacteria in the environment are often exponential, therefore, direct deposition of fecal material into waters generally contribute higher concentrations of pathogens than material that must be transported overland or through the subsurface.
In general, children, the elderly, and immunocompromised individuals require a lower dose of a pathogenic organism in order to contract an infection. Presently there are very few studies which are able to quantify the amount of time people are likely to spend in recreational waters and how much water they are likely to ingest.  In general, children swim more often, stay in the water longer, submerge their heads more often, and swallow more water. This makes people more fearful of water in the sea as more bacteria will be growing on and around them.
Quantitative microbiological risk assessments (QMRAs) combine pathogen concentrations in water with dose-response relationships and data reflecting potential exposure to estimate the risk of infection.
Data on water exposure are generally collected using questionnaires, but may also be determined from actual measurements of water ingested, or estimated from previously published data.  Respondents are asked to report the frequency and timing and location of exposures, detailed information about the amount of water swallowed and head submersion, and basic demographic characteristics such as age, gender, socioeconomic status and family composition.  Once sufficient data are collected and determined to be representative of the general population, they are usually fit with distributions, and these distribution parameters are then used in the risk assessment equations.  Monitoring data representing occurrence of pathogens, direct measurement of pathogen concentrations, or estimations deriving pathogen concentrations from indicator bacteria concentrations, are also fit with distributions.  Dose is calculated by multiplying the concentration of pathogens per volume by volume.  Dose-responses can also be fit with a distribution. [10]
The more assumptions that are made, the more uncertain estimates of risk related to pathogens will be.  However, even with considerable uncertainty, QMRAs are a good way to compare different risk scenarios.  In a study comparing estimated health risks from exposures to recreational waters impacted by human and non-human sources of fecal contamination, QMRA determined that the risk of gastrointestinal illness from exposure to waters impacted by cattle were similar to those impacted by human waste, and these were higher than for waters impacted by gull, chicken, or pig faeces. [11] Such studies could be useful to risk managers for determining how best to focus their limited resources, however, risk managers must be aware of the limitations of data used in these calculations.  For example, this study used data describing concentrations of Salmonella in chicken feces published in 1969. [12] Methods for quantifying bacteria, changes in animal housing practices and sanitation, and many other factors may have changed the prevalence of Salmonella since that time.  Also, such an approach often ignores the complicated fate and transport processes that determine bacteria concentrations from the source to the point of exposure.
In the US, individual states are allowed to develop their own water quality standards based on EPA's recommendations under the Clean Water Act of 1977.  Once water quality standards are approved, states are tasked with monitoring their surface waters to determine where impairments occur, and watershed plans called Total Maximum Daily Loads (TMDLs) are developed to direct water quality improvement efforts including changes to allowable bacteria loading by point sources and recommendations for changes to practices that reduce nonpoint-source contributions to bacteria loads.  Also, many states have beach monitoring programs to warn swimmers when high levels of indicator bacteria are detected. [13]
An infection is the invasion of an organism's body tissues by disease-causing agents , their multiplication, and the reaction of host tissues to the infectious agents and the toxins they produce. [1] An infectious disease , also known as a transmissible disease or communicable disease , is an illness resulting from an infection.
Infections can be caused by a wide range of pathogens, most prominently bacteria and viruses . [2] Hosts can fight infections using their immune system . Mammalian hosts react to infections with an innate response, often involving inflammation , followed by an adaptive response.
Specific medications used to treat infections include antibiotics , antivirals , antifungals , antiprotozoals , and antihelminthics . Infectious diseases resulted in 9.2 million deaths in 2013 (about 17% of all deaths). [3] The branch of medicine that focuses on infections is referred to as infectious disease . [4]
Infections are caused by infectious agents ( pathogens ) including:
Symptomatic infections are apparent and clinical , whereas an infection that is active but does not produce noticeable symptoms may be called inapparent, silent, subclinical , or occult . An infection that is inactive or dormant is called a latent infection . [9] An example of a latent bacterial infection is latent tuberculosis . Some viral infections can also be latent, examples of latent viral infections are any of those from the Herpesviridae family. [10]
The word infection can denote any presence of a particular pathogen at all (no matter how little) but also is often used in a sense implying a clinically apparent infection (in other words, a case of infectious disease). [11] This fact occasionally creates some ambiguity or prompts some usage discussion; to get around this it is common for health professionals to speak of colonization (rather than infection ) when they mean that some of the pathogens are present but that no clinically apparent infection (no disease) is present.
Different terms are used to describe infections. The first is an acute infection. An acute infection is one in which symptoms develop rapidly; its course can either be rapid or protracted. [12] The next is a chronic infection. A chronic infection is when symptoms develop gradually, over weeks or months, and are slow to resolve. [13] A subacute infection is one in which symptoms take longer to develop than in an acute infection but arise more quickly than a chronic infection. A latent infection is a type of infection that may occur after an acute episode; the organism is present but symptoms are not; after time, the disease can reappear. A focal infection is defined as the initial site of infection from which organisms travel via the bloodstream to another area of the body. [14]
Among the many varieties of microorganisms , relatively few cause disease in otherwise healthy individuals. [15] Infectious disease results from the interplay between those few pathogens and the defenses of the hosts they infect. The appearance and severity of disease resulting from any pathogen depend upon the ability of that pathogen to damage the host as well as the ability of the host to resist the pathogen. However, a host's immune system can also cause damage to the host itself in an attempt to control the infection. Clinicians, therefore, classify infectious microorganisms or microbes according to the status of host defenses - either as primary pathogens or as opportunistic pathogens :
Primary pathogens cause disease as a result of their presence or activity within the normal, healthy host, and their intrinsic virulence (the severity of the disease they cause) is, in part, a necessary consequence of their need to reproduce and spread. Many of the most common primary pathogens of humans only infect humans, however, many serious diseases are caused by organisms acquired from the environment or that infect non-human hosts.
Opportunistic pathogens can cause an infectious disease in a host with depressed resistance ( immunodeficiency ) or if they have unusual access to the inside of the body (for example, via trauma ). Opportunistic infection may be caused by microbes ordinarily in contact with the host, such as pathogenic bacteria or fungi in the gastrointestinal or the upper respiratory tract , and they may also result from (otherwise innocuous) microbes acquired from other hosts (as in Clostridium difficile colitis ) or from the environment as a result of traumatic introduction (as in surgical wound infections or compound fractures ). An opportunistic disease requires impairment of host defenses, which may occur as a result of genetic defects (such as Chronic granulomatous disease ), exposure to antimicrobial drugs or immunosuppressive chemicals (as might occur following poisoning or cancer chemotherapy ), exposure to ionizing radiation , or as a result of an infectious disease with immunosuppressive activity (such as with measles , malaria or HIV disease ). Primary pathogens may also cause more severe disease in a host with depressed resistance than would normally occur in an immunosufficient host. [16]
While a primary infection can practically be viewed as the root cause of an individual's current health problem, a secondary infection is a sequela or complication of that root cause. For example, an infection due to a burn or penetrating trauma (the root cause) is a secondary infection. Primary pathogens often cause primary infection and often cause secondary infection. Usually, opportunistic infections are viewed as secondary infections (because immunodeficiency or injury was the predisposing factor).
Other types of infection consist of mixed, iatrogenic, nosocomial, and community-acquired infection. A mixed infection is an infection that is caused by two or more pathogens. An example of this is Appendicitis , which is caused by Bacteroides fragilis and Escherichia coli. The second is an iatrogenic infection. This type of infection is one that is transmitted from a health care worker to a patient. A nosocomial infection is also one that occurs in a health care setting. Nosocomial infections are those that are acquired during a hospital stay. Lastly, a community-acquired infection is one in which the infection is acquired from a whole community. [14]
One manner of proving that a given disease is infectious, is to satisfy Koch's postulates (first proposed by Robert Koch ), which require that first, the infectious agent be identifiable only in patients who have the disease, and not in healthy controls, and second, that patients who contract the infectious agent also develop the disease. These postulates were first used in the discovery that Mycobacteria species cause tuberculosis .
However, Koch's postulates cannot usually be tested in modern practice for ethical reasons. Proving them would require experimental infection of a healthy individual with a pathogen produced as a pure culture. Conversely, even clearly infectious diseases do not always meet the infectious criteria; for example, Treponema pallidum , the causative spirochete of syphilis , cannot be cultured in vitro – however the organism can be cultured in rabbit testes . It is less clear that a pure culture comes from an animal source serving as host than it is when derived from microbes derived from plate culture.
Epidemiology , or the study and analysis of who, why and where disease occurs, and what determines whether various populations have a disease, is another important tool used to understand infectious disease. Epidemiologists may determine differences among groups within a population, such as whether certain age groups have a greater or lesser rate of infection; whether groups living in different neighborhoods are more likely to be infected; and by other factors, such as gender and race. Researchers also may assess whether a disease outbreak is sporadic, or just an occasional occurrence; endemic , with a steady level of regular cases occurring in a region; epidemic , with a fast arising, and unusually high number of cases in a region; or pandemic , which is a global epidemic. If the cause of the infectious disease is unknown, epidemiology can be used to assist with tracking down the sources of infection.
Infectious diseases are sometimes called contagious diseases when they are easily transmitted by contact with an ill person or their secretions (e.g., influenza ). Thus, a contagious disease is a subset of infectious disease that is especially infective or easily transmitted. Other types of infectious, transmissible, or communicable diseases with more specialized routes of infection, such as vector transmission or sexual transmission, are usually not regarded as "contagious", and often do not require medical isolation (sometimes loosely called quarantine ) of victims. However, this specialized connotation of the word "contagious" and "contagious disease" (easy transmissibility) is not always respected in popular use.
Infectious diseases are commonly transmitted from person to person through direct contact. The types of contact are through person to person and droplet spread. Indirect contact such as airborne transmission, contaminated objects, food and drinking water, animal person contact, animal reservoirs, insect bites, and environmental reservoirs  are another way infectious diseases are transmitted. [17]
Infections can be classified by the anatomic location or organ system infected, including:
In addition, locations of inflammation where infection is the most common cause include pneumonia , meningitis and salpingitis .
The symptoms of an infection depend on the type of disease. Some signs of infection affect the whole body generally, such as fatigue , loss of appetite, weight loss, fevers , night sweats, chills, aches and pains. Others are specific to individual body parts, such as skin rashes , coughing , or a runny nose .
In certain cases, infectious diseases may be asymptomatic for much or even all of their course in a given host. In the latter case, the disease may only be defined as a "disease" (which by definition means an illness) in hosts who secondarily become ill after contact with an asymptomatic carrier. An infection is not synonymous with an infectious disease, as some infections do not cause illness in a host. [16]
As bacterial and viral infections can both cause the same kinds of symptoms, it can be difficult to distinguish which is the cause of a specific infection. [18] Distinguishing the two is important, since viral infections cannot be cured by antibiotics whereas bacterial infections can. [19]
There is a general chain of events that applies to infections. [21] The chain of events involves several steps – which include the infectious agent, reservoir, entering a susceptible host, exit and transmission to new hosts. Each of the links must be present in a chronological order for an infection to develop. Understanding these steps helps health care workers target the infection and prevent it from occurring in the first place. [22]
Infection begins when an organism successfully enters the body, grows and multiplies. This is referred to as colonization. Most humans are not easily infected. Those with compromised or weakened immune systems have an increased susceptibility to chronic or persistent infections. Individuals who have a suppressed immune system are particularly susceptible to opportunistic infections . Entrance to the host at host-pathogen interface , generally occurs through the mucosa in orifices like the oral cavity , nose, eyes, genitalia, anus, or the microbe can enter through open wounds. While a few organisms can grow at the initial site of entry, many migrate and cause systemic infection in different organs. Some pathogens grow within the host cells (intracellular) whereas others grow freely in bodily fluids.
Wound colonization refers to non-replicating microorganisms within the wound, while in infected wounds, replicating organisms exist and tissue is injured. [23] All multicellular organisms are colonized to some degree by extrinsic organisms, and the vast majority of these exist in either a mutualistic or commensal relationship with the host. An example of the former is the anaerobic bacteria species, which colonizes the mammalian colon , and an example of the latter are the various species of staphylococcus that exist on human skin . Neither of these colonizations are considered infections. The difference between an infection and a colonization is often only a matter of circumstance. Non-pathogenic organisms can become pathogenic given specific conditions, and even the most virulent organism requires certain circumstances to cause a compromising infection. Some colonizing bacteria, such as Corynebacteria sp. and viridans streptococci , prevent the adhesion and colonization of pathogenic bacteria and thus have a symbiotic relationship with the host, preventing infection and speeding wound healing .
The variables involved in the outcome of a host becoming inoculated by a pathogen and the ultimate outcome include:
As an example, several staphylococcal species remain harmless on the skin, but, when present in a normally sterile space, such as in the capsule of a joint or the peritoneum , multiply without resistance and cause harm.
An interesting fact that gas chromatography–mass spectrometry , 16S ribosomal RNA analysis, omics , and other advanced technologies have made more apparent to humans in recent decades is that microbial colonization is very common even in environments that humans think of as being nearly sterile . Because it is normal to have bacterial colonization, it is difficult to know which chronic wounds can be classified as infected and how much risk of progression exists. Despite the huge number of wounds seen in clinical practice, there are limited quality data for evaluated symptoms and signs. A review of chronic wounds in the Journal of the American Medical Association's "Rational Clinical Examination Series" quantified the importance of increased pain as an indicator of infection. [27] The review showed that the most useful finding is an increase in the level of pain [likelihood ratio (LR) range, 11–20] makes infection much more likely, but the absence of pain (negative likelihood ratio range, 0.64–0.88) does not rule out infection (summary LR 0.64–0.88).

Disease can arise if the host's protective immune mechanisms are compromised and the organism inflicts damage on the host. Microorganisms can cause tissue damage by releasing a variety of toxins or destructive enzymes. For example, Clostridium tetani releases a toxin that paralyzes muscles, and staphylococcus releases toxins that produce shock and sepsis. Not all infectious agents cause disease in all hosts. For example, less than 5% of individuals infected with polio develop disease. [28] On the other hand, some infectious agents are highly virulent. The prion causing mad cow disease and Creutzfeldt–Jakob disease invariably kills all animals and people that are infected.
Persistent infections occur because the body is unable to clear the organism after the initial infection. Persistent infections are characterized by the continual presence of the infectious organism, often as latent infection with occasional recurrent relapses of active infection. There are some viruses that can maintain a persistent infection by infecting different cells of the body. Some viruses once acquired never leave the body. A typical example is the herpes virus, which tends to hide in nerves and become reactivated when specific circumstances arise.
Persistent infections cause millions of deaths globally each year. [29] Chronic infections by parasites account for a high morbidity and mortality in many underdeveloped countries.
For infecting organisms to survive and repeat the infection cycle in other hosts, they (or their progeny) must leave an existing reservoir and cause infection elsewhere. Infection transmission can take place via many potential routes:
The relationship between virulence versus transmissibility is complex; if a disease is rapidly fatal, the host may die before the microbe can be passed along to another host.
Diagnosis of infectious disease sometimes involves identifying an infectious agent either directly or indirectly. [33] In practice most minor infectious diseases such as warts , cutaneous abscesses , respiratory system infections and diarrheal diseases are diagnosed by their clinical presentation and treated without knowledge of the specific causative agent. Conclusions about the cause of the disease are based upon the likelihood that a patient came in contact with a particular agent, the presence of a microbe in a community, and other epidemiological considerations. Given sufficient effort, all known infectious agents can be specifically identified. The benefits of identification, however, are often greatly outweighed by the cost, as often there is no specific treatment, the cause is obvious, or the outcome of an infection is benign .
Diagnosis of infectious disease is nearly always initiated by medical history and physical examination. More detailed identification techniques involve the culture of infectious agents isolated from a patient. Culture allows identification of infectious organisms by examining their microscopic features, by detecting the presence of substances produced by pathogens, and by directly identifying an organism by its genotype. Other techniques (such as X-rays , CAT scans , PET scans or NMR ) are used to produce images of internal abnormalities resulting from the growth of an infectious agent. The images are useful in detection of, for example, a bone abscess or a spongiform encephalopathy produced by a prion .
The diagnosis is aided by the presenting symptoms in any individual with an infectious disease, yet it usually needs additional diagnostic techniques to confirm the suspicion. Some signs are specifically characteristic and indicative of a disease and are called pathognomonic signs; but these are rare. Not all infections are symptomatic. [34]
In children the presence of cyanosis , rapid breathing, poor peripheral perfusion, or a petechial rash increases the risk of a serious infection by greater than 5 fold. [35] Other important indicators include parental concern, clinical instinct, and temperature greater than 40 °C. [35]
Microbiological culture is a principal tool used to diagnose infectious disease. In a microbial culture, a growth medium is provided for a specific agent. A sample taken from potentially diseased tissue or fluid is then tested for the presence of an infectious agent able to grow within that medium. Most pathogenic bacteria are easily grown on nutrient agar , a form of solid medium that supplies carbohydrates and proteins necessary for growth of a bacterium , along with copious amounts of water. A single bacterium will grow into a visible mound on the surface of the plate called a colony , which may be separated from other colonies or melded together into a "lawn". The size, color, shape and form of a colony is characteristic of the bacterial species, its specific genetic makeup (its strain ), and the environment that supports its growth. Other ingredients are often added to the plate to aid in identification. Plates may contain substances that permit the growth of some bacteria and not others, or that change color in response to certain bacteria and not others. Bacteriological plates such as these are commonly used in the clinical identification of infectious bacterium. Microbial culture may also be used in the identification of viruses : the medium, in this case, being cells grown in culture that the virus can infect, and then alter or kill. In the case of viral identification, a region of dead cells results from viral growth, and is called a "plaque". Eukaryotic parasites may also be grown in culture as a means of identifying a particular agent.
In the absence of suitable plate culture techniques, some microbes require culture within live animals. Bacteria such as Mycobacterium leprae and Treponema pallidum can be grown in animals, although serological and microscopic techniques make the use of live animals unnecessary. Viruses are also usually identified using alternatives to growth in culture or animals. Some viruses may be grown in embryonated eggs. Another useful identification method is Xenodiagnosis, or the use of a vector to support the growth of an infectious agent. Chagas disease is the most significant example, because it is difficult to directly demonstrate the presence of the causative agent, Trypanosoma cruzi in a patient, which therefore makes it difficult to definitively make a diagnosis. In this case, xenodiagnosis involves the use of the vector of the Chagas agent T. cruzi , an uninfected triatomine bug, which takes a blood meal from a person suspected of having been infected. The bug is later inspected for growth of T. cruzi within its gut.
Another principal tool in the diagnosis of infectious disease is microscopy . Virtually all of the culture techniques discussed above rely, at some point, on microscopic examination for definitive identification of the infectious agent. Microscopy may be carried out with simple instruments, such as the compound light microscope , or with instruments as complex as an electron microscope . Samples obtained from patients may be viewed directly under the light microscope, and can often rapidly lead to identification. Microscopy is often also used in conjunction with biochemical staining techniques, and can be made exquisitely specific when used in combination with antibody based techniques. For example, the use of antibodies made artificially fluorescent (fluorescently labeled antibodies) can be directed to bind to and identify a specific antigens present on a pathogen. A fluorescence microscope is then used to detect fluorescently labeled antibodies bound to internalized antigens within clinical samples or cultured cells. This technique is especially useful in the diagnosis of viral diseases, where the light microscope is incapable of identifying a virus directly.
Other microscopic procedures may also aid in identifying infectious agents. Almost all cells readily stain with a number of basic dyes due to the electrostatic attraction between negatively charged cellular molecules and the positive charge on the dye. A cell is normally transparent under a microscope, and using a stain increases the contrast of a cell with its background. Staining a cell with a dye such as Giemsa stain or crystal violet allows a microscopist to describe its size, shape, internal and external components and its associations with other cells. The response of bacteria to different staining procedures is used in the taxonomic classification of microbes as well. Two methods, the Gram stain and the acid-fast stain, are the standard approaches used to classify bacteria and to diagnosis of disease. The Gram stain identifies the bacterial groups Firmicutes and Actinobacteria , both of which contain many significant human pathogens. The acid-fast staining procedure identifies the Actinobacterial genera Mycobacterium and Nocardia .
Biochemical tests used in the identification of infectious agents include the detection of metabolic or enzymatic products characteristic of a particular infectious agent. Since bacteria ferment carbohydrates in patterns characteristic of their genus and species , the detection of fermentation products is commonly used in bacterial identification. Acids , alcohols and gases are usually detected in these tests when bacteria are grown in selective liquid or solid media.
The isolation of enzymes from infected tissue can also provide the basis of a biochemical diagnosis of an infectious disease. For example, humans can make neither RNA replicases nor reverse transcriptase , and the presence of these enzymes are characteristic., of specific types of viral infections. The ability of the viral protein hemagglutinin to bind red blood cells together into a detectable matrix may also be characterized as a biochemical test for viral infection, although strictly speaking hemagglutinin is not an enzyme and has no metabolic function.
Serological methods are highly sensitive, specific and often extremely rapid tests used to identify microorganisms. These tests are based upon the ability of an antibody to bind specifically to an antigen. The antigen, usually a protein or carbohydrate made by an infectious agent, is bound by the antibody. This binding then sets off a chain of events that can be visibly obvious in various ways, dependent upon the test. For example, " Strep throat " is often diagnosed within minutes, and is based on the appearance of antigens made by the causative agent, S. pyogenes , that is retrieved from a patient's throat with a cotton swab. Serological tests, if available, are usually the preferred route of identification, however the tests are costly to develop and the reagents used in the test often require refrigeration . Some serological methods are extremely costly, although when commonly used, such as with the "strep test", they can be inexpensive. [16]
Complex serological techniques have been developed into what are known as Immunoassays . Immunoassays can use the basic antibody – antigen binding as the basis to produce an electro-magnetic or particle radiation signal, which can be detected by some form of instrumentation. Signal of unknowns can be compared to that of standards allowing quantitation of the target antigen. To aid in the diagnosis of infectious diseases, immunoassays can detect or measure antigens from either infectious agents or proteins generated by an infected organism in response to a foreign agent. For example, immunoassay A may detect the presence of a surface protein from a virus particle. Immunoassay B on the other hand may detect or measure antibodies produced by an organism's immune system that are made to neutralize and allow the destruction of the virus.
Instrumentation can be used to read extremely small signals created by secondary reactions linked to the antibody – antigen binding. Instrumentation can control sampling, reagent use, reaction times, signal detection, calculation of results, and data management to yield a cost-effective automated process for diagnosis of infectious disease.
Technologies based upon the polymerase chain reaction (PCR) method will become nearly ubiquitous gold standards of diagnostics of the near future, for several reasons. First, the catalog of infectious agents has grown to the point that virtually all of the significant infectious agents of the human population have been identified. Second, an infectious agent must grow within the human body to cause disease; essentially it must amplify its own nucleic acids in order to cause a disease. This amplification of nucleic acid in infected tissue offers an opportunity to detect the infectious agent by using PCR. Third, the essential tools for directing PCR, primers , are derived from the genomes of infectious agents, and with time those genomes will be known, if they are not already.
Thus, the technological ability to detect any infectious agent rapidly and specifically are currently available. The only remaining blockades to the use of PCR as a standard tool of diagnosis are in its cost and application, neither of which is insurmountable. The diagnosis of a few diseases will not benefit from the development of PCR methods, such as some of the clostridial diseases ( tetanus and botulism ). These diseases are fundamentally biological poisonings by relatively small numbers of infectious bacteria that produce extremely potent neurotoxins . A significant proliferation of the infectious agent does not occur, this limits the ability of PCR to detect the presence of any bacteria.
Given the wide range of bacterial, viral, fungal, protozoal, and helminthic pathogens that cause debilitating and life-threatening illnesses, the ability to quickly identify the cause of infection is important yet often challenging. For example, more than half of cases of encephalitis , a severe illness affecting the brain, remain undiagnosed, despite extensive testing using the standard of care ( microbiological culture ) and state-of-the-art clinical laboratory methods. Metagenomic sequencing-based diagnostic tests are currently being developed for clinical use and show promise as a sensitive, specific, and rapid way to diagnose infection using a single all-encompassing test. This test is similar to current PCR tests; however, an untargeted whole genome amplification is used rather than primers for a specific infectious agent. This amplification step is followed by next-generation sequencing or third-generation sequencing , alignment comparisons , and taxonomic classification using large databases of thousands of pathogen and commensal reference genomes . Simultaneously, antimicrobial resistance genes within pathogen and plasmid genomes are sequenced and aligned to the taxonomically-classified pathogen genomes to generate an antimicrobial resistance profile – analogous to antibiotic sensitivity testing – to facilitate antimicrobial stewardship and allow for the optimization of treatment using the most effective drugs for a patient's infection.
Metagenomic sequencing could prove especially useful for diagnosis when the patient is immunocompromised . An ever-wider array of infectious agents can cause serious harm to individuals with immunosuppression, so clinical screening must often be broader. Additionally, the expression of symptoms is often atypical, making a clinical diagnosis based on presentation more difficult. Thirdly, diagnostic methods that rely on the detection of antibodies are more likely to fail. A rapid, sensitive, specific, and untargeted test for all known human pathogens that detects the presence of the organism's DNA rather than antibodies is therefore highly desirable.
There is usually an indication for a specific identification of an infectious agent only when such identification can aid in the treatment or prevention of the disease, or to advance knowledge of the course of an illness prior to the development of effective therapeutic or preventative measures. For example, in the early 1980s, prior to the appearance of AZT for the treatment of AIDS , the course of the disease was closely followed by monitoring the composition of patient blood samples, even though the outcome would not offer the patient any further treatment options. In part, these studies on the appearance of HIV in specific communities permitted the advancement of hypotheses as to the route of transmission of the virus. By understanding how the disease was transmitted, resources could be targeted to the communities at greatest risk in campaigns aimed at reducing the number of new infections. The specific serological diagnostic identification, and later genotypic or molecular identification, of HIV also enabled the development of hypotheses as to the temporal and geographical origins of the virus, as well as a myriad of other hypothesis. [16] The development of molecular diagnostic tools have enabled physicians and researchers to monitor the efficacy of treatment with anti-retroviral drugs . Molecular diagnostics are now commonly used to identify HIV in healthy people long before the onset of illness and have been used to demonstrate the existence of people who are genetically resistant to HIV infection. Thus, while there still is no cure for AIDS, there is great therapeutic and predictive benefit to identifying the virus and monitoring the virus levels within the blood of infected individuals, both for the patient and for the community at large.
Techniques like hand washing, wearing gowns, and wearing face masks can help prevent infections from being passed from one person to another. Aseptic technique was introduced in medicine and surgery in the late 19th century and greatly reduced the incidence of infections caused by surgery. Frequent hand washing remains the most important defense against the spread of unwanted organisms. [37] There are other forms of prevention such as avoiding the use of illicit drugs, using a condom , wearing gloves, and having a healthy lifestyle with a balanced diet and regular exercise. Cooking foods well and avoiding foods that have been left outside for a long time is also important.
Antimicrobial substances used to prevent transmission of infections include:
One of the ways to prevent or slow down the transmission of infectious diseases is to recognize the different characteristics of various diseases. [38] Some critical disease characteristics that should be evaluated include virulence , distance traveled by victims, and level of contagiousness. The human strains of Ebola virus, for example, incapacitate their victims extremely quickly and kill them soon after. As a result, the victims of this disease do not have the opportunity to travel very far from the initial infection zone. [39] Also, this virus must spread through skin lesions or permeable membranes such as the eye. Thus, the initial stage of Ebola is not very contagious since its victims experience only internal hemorrhaging. As a result of the above features, the spread of Ebola is very rapid and usually stays within a relatively confined geographical area. In contrast, the Human Immunodeficiency Virus ( HIV ) kills its victims very slowly by attacking their immune system. [16] As a result, many of its victims transmit the virus to other individuals before even realizing that they are carrying the disease. Also, the relatively low virulence allows its victims to travel long distances, increasing the likelihood of an epidemic .
Another effective way to decrease the transmission rate of infectious diseases is to recognize the effects of small-world networks . [38] In epidemics, there are often extensive interactions within hubs or groups of infected individuals and other interactions within discrete hubs of susceptible individuals. Despite the low interaction between discrete hubs, the disease can jump and spread in a susceptible hub via a single or few interactions with an infected hub. Thus, infection rates in small-world networks can be reduced somewhat if interactions between individuals within infected hubs are eliminated (Figure 1). However, infection rates can be drastically reduced if the main focus is on the prevention of transmission jumps between hubs. The use of needle exchange programs in areas with a high density of drug users with HIV is an example of the successful implementation of this treatment method. [6] [ full citation needed ] Another example is the use of ring culling or vaccination of potentially susceptible livestock in adjacent farms to prevent the spread of the foot-and-mouth virus in 2001. [40]
A general method to prevent transmission of vector -borne pathogens is pest control .
In cases where infection is merely suspected, individuals may be quarantined until the incubation period has passed and the disease manifests itself or the person remains healthy. Groups may undergo quarantine, or in the case of communities, a cordon sanitaire may be imposed to prevent infection from spreading beyond the community, or in the case of protective sequestration , into a community. Public health authorities may implement other forms of social distancing , such as school closings, to control an epidemic.
Infection with most pathogens does not result in death of the host and the offending organism is ultimately cleared after the symptoms of the disease have waned. [15] This process requires immune mechanisms to kill or inactivate the inoculum of the pathogen. Specific acquired immunity against infectious diseases may be mediated by antibodies and/or T lymphocytes . Immunity mediated by these two factors may be manifested by:
The immune system response to a microorganism often causes symptoms such as a high fever and inflammation , and has the potential to be more devastating than direct damage caused by a microbe. [16]
Resistance to infection ( immunity ) may be acquired following a disease, by asymptomatic carriage of the pathogen, by harboring an organism with a similar structure (crossreacting), or by vaccination . Knowledge of the protective antigens and specific acquired host immune factors is more complete for primary pathogens than for opportunistic pathogens .
There is also the phenomenon of herd immunity which offers a measure of protection to those otherwise vulnerable people when a large enough proportion of the population has acquired immunity from certain infections.
Immune resistance to an infectious disease requires a critical level of either antigen-specific antibodies and/or T cells when the host encounters the pathogen. Some individuals develop natural serum antibodies to the surface polysaccharides of some agents although they have had little or no contact with the agent, these natural antibodies confer specific protection to adults and are passively transmitted to newborns.
The organism that is the target of an infecting action of a specific infectious agent is called the host. The host harbouring an agent that is in a mature or sexually active stage phase is called the definitive host. The intermediate host comes in contact during the larvae stage. A host can be anything living and can attain to asexual and sexual reproduction. [41] The clearance of the pathogens, either treatment-induced or spontaneous, it can be influenced by the genetic variants carried by the individual patients. For instance, for genotype 1 hepatitis C treated with Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b (brand names Pegasys or PEG-Intron) combined with ribavirin , it has been shown that genetic polymorphisms near the human IL28B gene, encoding interferon lambda 3, are associated with significant differences in the treatment-induced clearance of the virus. This finding, originally reported in Nature, [42] showed that genotype 1 hepatitis C patients carrying certain genetic variant alleles near the IL28B gene are more possibly to achieve sustained virological response after the treatment than others. Later report from Nature [43] demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus.
When infection attacks the body, anti-infective drugs can suppress the infection. Several broad types of anti-infective drugs exist, depending on the type of organism targeted; they include antibacterial ( antibiotic ; including antitubercular ), antiviral , antifungal and antiparasitic (including antiprotozoal and antihelminthic ) agents. Depending on the severity and the type of infection, the antibiotic may be given by mouth or by injection, or may be applied topically . Severe infections of the brain are usually treated with intravenous antibiotics. Sometimes, multiple antibiotics are used in case there is resistance to one antibiotic. Antibiotics only work for bacteria and do not affect viruses. Antibiotics work by slowing down the multiplication of bacteria or killing the bacteria. The most common classes of antibiotics used in medicine include penicillin , cephalosporins , aminoglycosides , macrolides , quinolones and tetracyclines . [44] [45]
Not all infections require treatment, and for many self-limiting infections the treatment may cause more side-effects than benefits. Antimicrobial stewardship is the concept that healthcare providers should treat an infection with an antimicrobial that specifically works well for the target pathogen for the shortest amount of time and to only treat when there is a known or highly suspected pathogen that will respond to the medication. [46]
In 2010, about 10 million people died of infectious diseases. [48]
The World Health Organization collects information on global deaths by International Classification of Disease (ICD) code categories . The following table lists the top infectious disease by number of deaths in 2002. 1993 data is included for comparison.
The top three single agent/disease killers are HIV / AIDS , TB and malaria . While the number of deaths due to nearly every disease have decreased, deaths due to HIV/AIDS have increased fourfold. Childhood diseases include pertussis , poliomyelitis , diphtheria , measles and tetanus . Children also make up a large percentage of lower respiratory and diarrheal deaths. In 2012, approximately 3.1 million people have died due to lower respiratory infections, making it the number 4 leading cause of death in the world. [54]
With their potential for unpredictable and explosive impacts, infectious diseases have been major actors in human history . [55] A pandemic (or global epidemic ) is a disease that affects people over an extensive geographical area. For example:
In most cases, microorganisms live in harmony with their hosts via mutual or commensal interactions. Diseases can emerge when existing parasites become pathogenic or when new pathogenic parasites enter a new host.
Several human activities have led to the emergence of zoonotic human pathogens, including viruses, bacteria, protozoa, and rickettsia, [64] and spread of vector-borne diseases, [63] see also globalization and disease and wildlife disease :
In Antiquity , the Greek historian Thucydides (c. 460 – c. 400 BCE) was the first person to write, in his account of the plague of Athens , that diseases could spread from an infected person to others. [66] [67] In his On the Different Types of Fever (c. AD 175), the Greco-Roman physician Galen speculated that plagues were spread by "certain seeds of plague", which were present in the air. [68] In the Sushruta Samhita , the ancient Indian physician Sushruta theorized: "Leprosy, fever, consumption, diseases of the eye, and other infectious diseases spread from one person to another by sexual union, physical contact, eating together, sleeping together, sitting together, and the use of same clothes, garlands and pastes." [69] [70] This book has been dated to about the sixth century BC. [71]
A basic form of contagion theory was proposed by Persian physician Ibn Sina (known as Avicenna in Europe) in The Canon of Medicine (1025), which later became the most authoritative medical textbook in Europe up until the 16th century. In Book IV of the Canon , Ibn Sina discussed epidemics , outlining the classical miasma theory and attempting to blend it with his own early contagion theory. He mentioned that people can transmit disease to others by breath, noted contagion with tuberculosis , and discussed the transmission of disease through water and dirt. [72] The concept of invisible contagion was later discussed by several Islamic scholars in the Ayyubid Sultanate who referred to them as najasat ("impure substances"). The fiqh scholar Ibn al-Haj al-Abdari (c. 1250–1336), while discussing Islamic diet and hygiene , gave warnings about how contagion can contaminate water, food, and garments, and could spread through the water supply, and may have implied contagion to be unseen particles. [73]
When the Black Death bubonic plague reached Al-Andalus in the 14th century, the Arab physicians Ibn Khatima (c. 1369) and Ibn al-Khatib (1313–1374) hypothesised that infectious diseases were caused by "minute bodies" and described how they can be transmitted through garments, vessels and earrings. [74] Ideas of contagion became more popular in Europe during the Renaissance , particularly through the writing of the Italian physician Girolamo Fracastoro . [75] Anton van Leeuwenhoek (1632–1723) advanced the science of microscopy by being the first to observe microorganisms, allowing for easy visualization of bacteria.
In the mid-19th century John Snow and William Budd did important work demonstrating the contagiousness of typhoid and cholera through contaminated water. Both are credited with decreasing epidemics of cholera in their towns by implementing measures to prevent contamination of water. [76] Louis Pasteur proved beyond doubt that certain diseases are caused by infectious agents, and developed a vaccine for rabies . Robert Koch , provided the study of infectious diseases with a scientific basis known as Koch's postulates . Edward Jenner , Jonas Salk and Albert Sabin developed effective vaccines for smallpox and polio , which would later result in the eradication and near-eradication of these diseases, respectively. Alexander Fleming discovered the world's first antibiotic , Penicillin , which Florey and Chain then developed. Gerhard Domagk developed sulphonamides , the first broad spectrum synthetic antibacterial drugs.
The medical treatment of infectious diseases falls into the medical field of Infectious Disease and in some cases the study of propagation pertains to the field of Epidemiology . Generally, infections are initially diagnosed by primary care physicians or internal medicine specialists. For example, an "uncomplicated" pneumonia will generally be treated by the internist or the pulmonologist (lung physician). The work of the infectious diseases specialist therefore entails working with both patients and general practitioners, as well as laboratory scientists , immunologists , bacteriologists and other specialists.
An infectious disease team may be alerted when:
Several studies have reported associations between pathogen load in an area and human behavior. Higher pathogen load is associated with decreased size of ethnic and religious groups in an area. This may be due high pathogen load favoring avoidance of other groups, which may reduce pathogen transmission, or a high pathogen load preventing the creation of large settlements and armies that enforce a common culture. Higher pathogen load is also associated with more restricted sexual behavior, which may reduce pathogen transmission. It also associated with higher preferences for health and attractiveness in mates. Higher fertility rates and shorter or less parental care per child is another association that may be a compensation for the higher mortality rate. There is also an association with polygyny which may be due to higher pathogen load, making selecting males with a high genetic resistance increasingly important. Higher pathogen load is also associated with more collectivism and less individualism, which may limit contacts with outside groups and infections. There are alternative explanations for at least some of the associations although some of these explanations may in turn ultimately be due to pathogen load. Thus, polygyny may also be due to a lower male: female ratio in these areas but this may ultimately be due to male infants having increased mortality from infectious diseases. Another example is that poor socioeconomic factors may ultimately in part be due to high pathogen load preventing economic development. [77]
Evidence of infection in fossil remains is a subject of interest for paleopathologists , scientists who study occurrences of injuries and illness in extinct life forms. Signs of infection have been discovered in the bones of carnivorous dinosaurs. When present, however, these infections seem to tend to be confined to only small regions of the body. A skull attributed to the early carnivorous dinosaur Herrerasaurus ischigualastensis exhibits pit-like wounds surrounded by swollen and porous bone. The unusual texture of the bone around the wounds suggests they were afflicted by a short-lived, non-lethal infection. Scientists who studied the skull speculated that the bite marks were received in a fight with another Herrerasaurus . Other carnivorous dinosaurs with documented evidence of infection include Acrocanthosaurus , Allosaurus , Tyrannosaurus and a tyrannosaur from the Kirtland Formation . The infections from both tyrannosaurs were received by being bitten during a fight, like the Herrerasaurus specimen. [78]
A 2006 Space Shuttle experiment found that Salmonella typhimurium , a bacterium that can cause food poisoning , became more virulent when cultivated in space. [79] On April 29, 2013, scientists in Rensselaer Polytechnic Institute, funded by NASA , reported that, during spaceflight on the International Space Station , microbes seem to adapt to the space environment in ways "not observed on Earth" and in ways that "can lead to increases in growth and virulence ". [80] More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space. [81] Microorganisms have been observed to survive the vacuum of outer space. [82] [83]

Infection prevention and control is the discipline concerned with preventing healthcare-associated infections ; a practical rather than academic sub-discipline of epidemiology . In Northern Europe , infection prevention and control is expanded from healthcare into a component in public health , known as "infection protection" ( smittevern, smittskydd, Infektionsschutz in the local languages). It is an essential part of the infrastructure of health care . Infection control and hospital epidemiology are akin to public health practice, practiced within the confines of a particular health-care delivery system rather than directed at society as a whole.
Infection control addresses factors related to the spread of infections within the healthcare setting, whether among patients, from patients to staff, from staff to patients, or among staff. This includes preventive measures such as hand washing , cleaning, disinfecting, sterilizing , and vaccinating . Other aspects include surveillance, monitoring, and investigating and managing suspected outbreaks of infection within a healthcare setting.
A subsidiary aspect of infection control involves preventing the spread of antimicrobial-resistant organisms such as MRSA .  This in turn connects to the discipline of antimicrobial stewardship --limiting the use of antimicrobials to necessary cases, as increased usage inevitably results in the selection and dissemination of resistant organisms. Antimicrobial medications (aka antimicrobials or anti-infective agents) include antibiotics , antibacterials , antifungals , antivirals and antiprotozoals . [1]
The World Health Organization (WHO) has set up an Infection Prevention and Control (IPC) unit in its Service Delivery and Safety department that publishes related guidelines. [2]
Aseptic technique is a key component of all invasive medical procedures.  Similar control measures are also recommended in any healthcare setting to prevent the spread of infection generally.
Hand hygiene is one of the basic, yet most important steps in IPC (Infection Prevention and Control). Hand hygiene reduces the chances of HAI (Healthcare Associated Infections) drastically at a floor-low cost. Hand hygiene consists of either hand wash(water based) or hand rubs(alcohol based). Hand wash is a solid 7-steps according to the WHO standards , wherein hand rubs are 5-steps.
Independent studies by Ignaz Semmelweis in 1846 in Vienna and Oliver Wendell Holmes, Sr. in 1843 in Boston established a link between the hands of health care workers and the spread of hospital-acquired disease . [3] The U.S. Centers for Disease Control and Prevention (CDC) state that "It is well documented that the most important measure for preventing the spread of pathogens is effective handwashing". [4] In the developed world, hand washing is mandatory in most health care settings and required by many different regulators. [ citation needed ]
In the United States, OSHA standards [5] require that employers must provide readily accessible hand washing facilities, and must ensure that employees wash hands and any other skin with soap and water or flush mucous membranes with water as soon as feasible after contact with blood or other potentially infectious materials (OPIM).
In the UK healthcare professionals have adopted the 'Ayliffe Technique', based on the 6 step method developed by Graham Ayliffe , JR Babb and AH Quoraishi. [6]
Drying is an essential part of the hand hygiene process.  In November 2008, a non-peer-reviewed [7] study was presented to the European Tissue Symposium by the University of Westminster , London, comparing the bacteria levels present after the use of paper towels , warm air hand dryers, and modern jet-air hand dryers. [8] Of those three methods, only paper towels reduced the total number of bacteria on hands, with "through-air dried" towels the most effective.
The presenters also carried out tests to establish whether there was the potential for cross-contamination of other washroom users and the washroom environment as a result of each type of drying method. They found that:
In 2005, in a study conducted by TUV Produkt und Umwelt, different hand drying methods were evaluated. [9] The following changes in the bacterial count after drying the hands were observed:
The field of infection prevention describes a hierarchy of removal of microorganisms from surfaces including medical equipment and instruments .  Cleaning is the lowest level, accomplishing substantial removal.   Disinfection involves the removal of all pathogens other than bacterial spores. Sterilization is defined as the removal or destruction of ALL microorganisms including bacterial spores.
Cleaning is the first and simplest step in preventing the spread of infection via surfaces and fomites.  Cleaning reduces microbial burden by chemical deadsorption of organisms (loosening bioburden/organisms from surfaces via cleaning chemicals), simple mechanical removal (rinsing, wiping), as well as disinfection (killing of organisms by cleaning chemicals).
In order to reduce their chances to contract an infection, individuals are recommended to maintain a good hygiene by washing their hands after every contact with questionable areas or bodily fluids and by disposing of garbage at regular intervals to prevent germs from growing. [10]
Disinfection uses liquid chemicals on surfaces and at room temperature to kill disease causing microorganisms. Ultraviolet light has also been used to disinfect the rooms of patients infected with Clostridium difficile after discharge. [11] Disinfection is less effective than sterilization because it does not kill bacterial endospores. [12]
Sterilization is a process intended to kill all microorganisms and is the highest level of microbial kill that is possible.
Sterilization, if performed properly, is an effective way of preventing Infections from spreading. It should be used for the cleaning of  medical instruments and any type of medical item that comes into contact with the blood stream and sterile tissues.
There are four main ways in which such items are usually sterilized: autoclave (by using high-pressure steam ), dry heat (in an oven), by using chemical sterilants such as glutaraldehydes or formaldehyde solutions or by exposure to ionizing radiation . The first two are the most widely used methods of sterilization mainly because of their accessibility and availability. Steam sterilization is one of the most effective types of sterilizations, if done correctly which is often hard to achieve. Instruments that are used in health care facilities are usually sterilized with this method. The general rule in this case is that in order to perform an effective sterilization, the steam must get into contact with all the surfaces that are meant to be disinfected. On the other hand, dry heat sterilization, which is performed with the help of an oven, is also an accessible type of sterilization, although it can only be used to disinfect instruments that are made of metal or glass . The very high temperatures needed to perform sterilization in this way are able to melt the instruments that are not made of glass or metal.
Effectiveness of the sterilizer, for example a steam autoclave is determined in three ways. [12] First, mechanical indicators and gauges on the machine itself indicate proper operation of the machine. Second heat sensitive indicators or tape on the sterilizing bags change color which indicate proper levels of heat or steam. And, third (most importantly) is biological testing in which a microorganism that is highly heat and chemical resistant (often the bacterial endospore) is selected as the standard challenge. If the process kills this microorganism, the sterilizer is considered to be effective. [12]
Steam sterilization is done at a temperature of 121 C (250 F) with a pressure of 209 kPa (~2atm). In these conditions, rubber items must be sterilized for 20 minutes, and wrapped items 134 C with pressure of 310 kPa for 7 minutes. The time is counted once the temperature that is needed has been reached. Steam sterilization requires four conditions in order to be efficient: adequate contact, sufficiently high temperature, correct time and sufficient moisture . [13] Sterilization using steam can also be done at a temperature of 132 C (270 F), at a double pressure.
Dry heat sterilization is performed at 170 C (340 F) for one hour or two hours at a temperature of 160 C (320 F). Dry heat sterilization can also be performed at 121 C, for at least 16 hours. [14]
Chemical sterilization, also referred to as cold sterilization, can be used to sterilize instruments that cannot normally be disinfected through the other two processes described above. The items sterilized with cold sterilization are usually those that can be damaged by regular sterilization. A variety of chemicals can be used including aldehydes, hydrogen peroxide, and peroxyacetic acid.   Commonly, glutaraldehydes and formaldehyde are used in this process, but in different ways. When using the first type of disinfectant, the instruments are soaked in a 2–4% solution for at least 10 hours while a solution of 8% formaldehyde will sterilize the items in 24 hours or more. Chemical sterilization is generally more expensive than steam sterilization and therefore it is used for instruments that cannot be disinfected otherwise. After the instruments have been soaked in the chemical solutions, they must be rinsed with sterile water which will remove the residues from the disinfectants . This is the reason why needles and syringes are not sterilized in this way, as the residues left by the chemical solution that has been used to disinfect them cannot be washed off with water and they may interfere with the administered treatment. Although formaldehyde is less expensive than glutaraldehydes, it is also more irritating to the eyes , skin and respiratory tract and is classified as a potential carcinogen , [13] so it is used much less commonly.
Ionizing radiation is typically used only for sterilizing items for which none of the above methods are practical, because of the risks involved in the process

Personal protective equipment (PPE) is specialized clothing or equipment worn by a worker for protection against a hazard. The hazard in a health care setting is exposure to blood, saliva, or other bodily fluids or aerosols that may carry infectious materials such as Hepatitis C , HIV , or other blood borne or bodily fluid pathogen . PPE prevents contact with a potentially infectious material by creating a physical barrier between the potential infectious material and the healthcare worker. [15]
The United States Occupational Safety and Health Administration (OSHA) requires the use of personal protective equipment (PPE) by workers to guard against blood borne pathogens if there is a reasonably anticipated exposure to blood or other potentially infectious materials. [16]
Components of PPE include gloves , gowns , bonnets, shoe covers, face shields , CPR masks , goggles , surgical masks , and respirators. How many components are used and how the components are used is often determined by regulations or the infection control protocol of the facility in question, which in turn are derived from knowledge of the mechanism of transmission of the pathogen(s) of concern. Many or most of these items are disposable to avoid carrying infectious materials from one patient to another patient and to avoid difficult or costly disinfection . In the US, OSHA requires the immediate removal and disinfection or disposal of a worker's PPE prior to leaving the work area where exposure to infectious material took place. [17] For health care professionals who may come into contact with highly infectious bodily fluids, using personal protective coverings on exposed body parts improves protection. [18] Breathable personal protective equipment improves user-satisfaction and may offer a similar level of protection. [18] In addition, adding tabs and other modifications to the protective equipment may reduce the risk of contamination during donning and doffing (putting on and taking off the equipment). [18] Implementing an evidence-based donning and doffing protocol such as a one-step glove and gown removal technique, giving oral instructions while donning and doffing, double gloving, and the use of glove disinfection may also improve protection for health care professionals. [18]
The inappropriate use of PPE equipment such as gloves, has been linked to an increase in rates of the transmission of infection, [19] and the use of such must be compatible with the other particular hand hygiene agents used. [20] Research studies in the form of randomized controlled trials and simulation studies are needed to determine the most effective types of PPE for preventing the transmission of infectious diseases to healthcare workers.  There is low quality evidence that supports making improvements or modifications to personal protective equipment in order to help decrease contamination. [18] Examples of modifications include adding tabs to masks or gloves to ease removal and designing protective gowns so that gloves are removed at the same time. In addition, there is weak evidence that the following PPE approaches or techniques may lead to reduced contamination and improved compliance with PPE protocols: Wearing double gloves, following specific doffing (removal) procedures such as those from the CDC, and providing people with spoken instructions while removing PPE. [18]
Microorganisms are known to survive on non-antimicrobial inanimate 'touch' surfaces (e.g., bedrails, over-the-bed trays, call buttons, bathroom hardware, etc.) for extended periods of time. [21] [22] This can be especially troublesome in hospital environments where patients with immunodeficiencies are at enhanced risk for contracting nosocomial infections.
Products made with antimicrobial copper alloy ( brasses , bronzes , cupronickel , copper-nickel-zinc, and others) surfaces destroy a wide range of microorganisms in a short period of time. [23] The United States Environmental Protection Agency has approved the registration of 355 different antimicrobial copper alloys and one synthetic copper-infused hard surface that kill E. coli O157:H7, methicillin -resistant Staphylococcus aureus ( MRSA ), Staphylococcus , Enterobacter aerogenes, and Pseudomonas aeruginosa in less than 2 hours of contact. Other investigations have demonstrated the efficacy of antimicrobial copper alloys to destroy Clostridium difficile , influenza A virus , adenovirus , and fungi . [23] As a public hygienic measure in addition to regular cleaning, antimicrobial copper alloys are being installed in healthcare facilities in the UK, Ireland, Japan, Korea, France, Denmark, and Brazil. The synthetic hard surface is being installed in the United States as well as in Israel. [24]
Health care workers may be exposed to certain infections in the course of their work. Vaccines are available to provide some protection to workers in a healthcare setting. Depending on regulation, recommendation, the specific work function, or personal preference, healthcare workers or first responders may receive vaccinations for hepatitis B ; influenza ; measles, mumps and rubella ; Tetanus, diphtheria, pertussis ; N. meningitidis ; and varicella . [25]
Surveillance is the act of infection investigation using the CDC definitions.  Determining the presence of a hospital acquired infection requires an infection control practitioner (ICP) to review a patient's chart and see if the patient had the signs and symptom of an infection. Surveillance definitions exist for infections of the bloodstream, urinary tract, pneumonia, surgical sites and gastroenteritis.
Surveillance traditionally involved significant manual data assessment and entry in order to assess preventative actions such as isolation of patients with an infectious disease. Increasingly, computerized software solutions are becoming available that assess incoming risk messages from microbiology and other online sources. By reducing the need for data entry, software can reduce the data workload of ICPs, freeing them to concentrate on clinical surveillance.
As of 1998, approximately one third of healthcare acquired infections were preventable. [26] Surveillance and preventative activities are increasingly a priority for hospital staff. The Study on the Efficacy of Nosocomial Infection Control (SENIC) project by the U.S. CDC found in the 1970s that hospitals reduced their nosocomial infection rates by approximately 32 per cent by focusing on surveillance activities and prevention efforts. [27]
In healthcare facilities , medical isolation refers to various physical measures taken to interrupt nosocomial spread of contagious diseases. Various forms of isolation exist, and are applied depending on the type of infection and agent involved, and its route of transmission , to address the likelihood of spread via airborne particles or droplets, by direct skin contact, or via contact with body fluids.
In cases where infection is merely suspected, individuals may be quarantined until the incubation period has passed and the disease manifests itself or the person remains healthy. Groups may undergo quarantine, or in the case of communities, a cordon sanitaire may be imposed to prevent infection from spreading beyond the community, or in the case of protective sequestration , into a community. Public health authorities may implement other forms of social distancing , such as school closings, when needing to control an epidemic . [28]
Barriers to the ability of healthcare workers to follow PPE and infection control guidelines include communication of the guidelines, workplace support (manager support), the culture of use at the workplace, adequate training, the amount of physical space in the facility, access to PPE, and healthcare worker motivation to provide good patient care. [29]
Facilitators include the importance of including all the staff in a facility (healthcare workers and support staff) should be done when guidelines are implemented. [29]
When an unusual cluster of illness is noted, infection control teams undertake an investigation to determine whether there is a true disease outbreak , a pseudo-outbreak (a result of contamination within the diagnostic testing process), or just random fluctuation in the frequency of illness.  If a true outbreak is discovered, infection control practitioners try to determine what permitted the outbreak to occur, and to rearrange the conditions to prevent ongoing propagation of the infection.  Often, breaches in good practice are responsible, although sometimes other factors (such as construction) may be the source of the problem. [ citation needed ]
Outbreaks investigations have more than a single purpose. These investigations are carried out in order to prevent additional cases in the current outbreak, prevent future outbreaks, learn about a new disease or learn something new about an old disease. Reassuring the public, minimizing the economic and social disruption as well as teaching epidemiology are some other obvious objectives of outbreak investigations. [30]
According to the WHO , outbreak investigations are meant to detect what is causing the outbreak, how the pathogenic agent is transmitted, where it all started from, what is the carrier, what is the population at risk of getting infected and what are the risk factors. [ citation needed ]
Practitioners can come from several different educational streams. Many begin as nurses, some as medical technologists (particularly in clinical microbiology), and some as physicians (typically infectious disease specialists). Specialized training in infection control and health care epidemiology are offered by the professional organizations described below. Physicians who desire to become infection control practitioners often are trained in the context of an infectious disease fellowship. Training that is conducted "face to face", via a computer, or via video conferencing may help improve compliance and reduce errors when compared with "folder based" training (providing health care professionals with written information or instructions). [18]
In the United States, Certification Board of Infection Control and Epidemiology is a private company that certifies infection control practitioners based on their educational background and professional experience, in conjunction with testing their knowledge base with standardized exams. The credential awarded is CIC, Certification in Infection Control and Epidemiology.  It is recommended that one has 2 years of Infection Control experience before applying for the exam.  Certification must be renewed every five years. [31]
A course in hospital epidemiology (infection control in the hospital setting) is offered jointly each year by the Centers for Disease Control and Prevention (CDC) and the Society for Healthcare Epidemiology of America. [32]
In 2002, the Royal Australian College of General Practitioners published a revised standard for office-based infection control which covers the sections of managing immunisation, sterilisation and disease surveillance. [33] [34] However, the document on the personal hygiene of health workers is only limited to hand hygiene, waste and linen management, which may not be sufficient since some of the pathogens are air-born and could be spread through air flow. [35] [36]
Since 1 November 2019, the Australian Commission on Safety and Quality in Health Care has managed the Hand Hygiene initiative in Australia, an initiative focused on improving hand hygiene practices to reduce the incidence of healthcare associated infections. [37]
Currently, the federal regulation that describes infection control standards, as related to occupational exposure to potentially infectious blood and other materials, is found at 29 CFR Part 1910.1030 Bloodborne pathogens. [38]
Infection in childcare is the spread of infection during childcare , typically because of contact among children in daycare or school . This happens when groups of children meet in a childcare environment, and there any individual with an infectious disease may spread it to the entire group. Commonly spread diseases include influenza-like illness and enteric illnesses, such as diarrhea among babies using diapers. It is uncertain how these diseases spread, but hand washing reduces some risk of transmission and increasing hygiene in other ways also reduces risk of infection.
Due to social pressure, parents of sick children in childcare may be willing to give unnecessary medical care to their children when advised to do so by childcare workers and even if it is against the advice of health care providers. In particular, children in childcare are more likely to be given antibiotics than children outside of childcare.
The presumption behind the idea of a "childcare infection" is that a place in which many children come into contact with each other can be a focus of infection , which is a place where infections are able to spread from person to person.
Flu and respiratory tract infection are lessened in groups which use frequent hand washing , but the actual pathway through which the diseases spread is unclear except for the fact that hand washing disrupts disease transmission. [1]
Diseases related to the human gastrointestinal tract , like diarrhea or other enteric illness, often spread through the fecal-oral route and are especially common in places where children have not completed toilet training . [2] Diapers, confined spaces for changing diapers, and the unhygienic habits of children contribute to the spread of these infections. [2] Bacterial infections most often spread through person to person contact, while eating food, or through the presence of animals. [2] It is difficult to determine how viral agents causing enteric illness spread. [2] Reviews of Helicobacter pylori have been unable to determine how it spreads during childcare, but have confirmed that it does easily spread in childcare environments, and that it is difficult to make recommendations for preventing it. [3]
Childcare infection is a public health concern because it harms the health of individual children and the infections which children get during childcare also may be spread within their homes and communities away from the childcare. [4] Generally, children who attend childcare are 2-3 times more likely to acquire an infection than children who do not receive such services. [4]
Infection happens because of individuals bringing infections into a childcare environment and spreading infectious agents within that environment, which children then contact and become at risk for infection. Increased risk of infection is related to practices of those in the childcare environment, and infection risk can be reduced by taking precautions. [4] Practices which reduce the likelihood of spreading infection include encouraging hand washing in all present, providing facial tissue to cover sneezes, doing food preparation in a place separate from other activity, cleaning and using a disinfectant on surfaces people touch, and among groups using diapers, having good practices to change and dispose of diapers while cleaning children and the changing area. [4]
There are some alternetives to prevent diseases through physical contact with objects. Most of the objects in child center like toys, chairs, tables, and everything that can be touch by anyone can be what causes infections or a disease. There is some disinfectant that has studies report of their effectiveness against " Salmonella Typhimurium " and " Staphylococcus aureus " on a chair and a toy. They can be used to prevent these infections. First, we have Clorox Anywhere (CA) that has achieved the greatest results of reducing "Staphylococcus aureus" and "Salmonella Typhimurium". Clorox Green Works (GCW) is the second to show reduction but not more than Clorox Anywhere. The one that has shown less reduction but still shows effectiveness is CITRUS Farm Edition (CFE). [5]
Childcare infections can be treated just as infections acquired outside of childcare, however, there are pressures on sick children to begin taking unnecessary health care even against the advice of health care providers. Antibiotics are commonly given to children for whom the drugs would serve no benefit, due to the child not having a medical condition which antibiotics can treat. [6] This is especially common in children with respiratory infections which antibiotics cannot treat, and in younger children, and in children who have privately purchased health insurance covering their medical expenses. [6]
Children who attend childcare are twice as likely to take an antibiotic when sick as children who do not attend childcare. [7] This is because child care providers wish to host children who are not sick, and consequently pressure parents to seek antibiotics or other treatment even when it is against the advice of health care providers. [7] In turn, parents feel compelled to seek this treatment for their children to please the care providers even if it is against the advice of their health care provider. [7] Overuse of antibiotics in child care has led to an increase of antibiotic resistance in bacteria in childcare settings. [8] While the increase of antibiotic resistance is worrisome, the current implications of this are uncertain, although it is expected that this will become more of a public health problem in the future. [8]
Families in which parents take time off work to care for their sick children instead of sending them to childcare services may be harmed by missing the loss of work hours and pay. [9] Some research has suggested that when parents have paid leave from work to tend to sick children then they are less likely to give their children antibiotics unless they are sure that it is recommended by a health care provider. [10]
Childcare providers often refuse to care for sick children, and ask that parents make alternate arrangements. [11] For various reasons including an inability of childcare providers to know which illnesses are infectious, childcare providers often refuse to care even for children who have acute illnesses which are unlikely to spread to others. [11]
An infection rate (or incident rate) is the probability or risk of an infection in a population . It is used to measure the frequency of occurrence of new instances of infection within a population during a specific time period.
Rate of infection = K × the number of infections the number of those at risk of infection {\displaystyle {\text{Rate of infection}}=K\times {\frac {\text{the number of infections}}{\text{the number of those at risk of infection}}}}
The number of infections equals the cases identified in the study or observed. An example might by HIV infection during a specific time period in the defined population. The population at risk are the cases appearing in the population during the same time period. An example would be all the people in a city during a specific time period. The constant, or K is assigned a value of 100 to represent a percentage. An example would be to find the percentage of people in a city who are infected with HIV: 6,000 cases in March divided by the population of a city (one million) multiplied by the constant (K) would give an infection rate of 0.6%.
.
Calculating the infection rate is used to analyze trends for the purpose of infection and disease control. [1]
An online infection rate calculator has been developed by the Centers for Disease Control and Prevention that allows the determination of the Streptococcal A infection rate in a population. [2]
Health care facilities routinely track their infection rates according to the guidelines issued by the Joint Commission . [3] The healthcare-associated infection (HAI) rates measure infection of patients in a particular hospital. This allows rates to compared with other hospitals. These infections can often be prevented when healthcare facilities follow guidelines for safe care. To get payment from Medicare , hospitals are required to report data about some infections to the Centers for Disease Control and Prevention's (CDC's) National Healthcare Safety Network (NHSN). Hospitals currently submit information on central line-associated bloodstream infections (CLABSIs), catheter-associated urinary tract infections (CAUTIs), surgical site infections (SSIs), MRSA Bacteremia , and C. difficile laboratory-identified events. The public reporting of these data is an effort by the Department of Health and Human Services . [4]
For meaningful comparisons of infection rates, populations must be very similar between the two or more assessments. However, a problem with mean rates is that they cannot reflect differences in risk between populations, [5]
This medical article is a stub . You can help Wikipedia by expanding it .
In epidemiology , particularly in the discussion of infectious disease dynamics (mathematical modeling of disease spread), the infectious period is the time interval during which a host (individual or patient) is infectious, i.e. capable of directly or indirectly transmitting pathogenic infectious agents or pathogens to another susceptible host. The infectious period can start before, during or after the onset of symptoms, and it may stop before or after the symptoms stop showing. It is also known in the literature by a variety of synonymous terms such as the infective period , the period of infectiousness , communicability period , the period of communicability , contagious period , the period of contagiousness , transmission period or transmissibility period . [1] [2] The degree of infectiousness is not constant but varies through the infectious period.
When pathogens encounter a susceptible individual and enter his or her body, it is called the exposure moment , and the individual turns into a host for those pathogens. After entering a host's body (which marks the beginning of the infection process), pathogens usually require time to multiply or replicate at their favorite site in the body (for example, the Hepatitis virus multiplies in the liver). After a certain time period, the pathogens become numerous enough so that the host is now able to transmit them into the environment. This marks the end of the latent period ( pre-infectious period ) and simultaneously the beginning of the infectious period. As the disease becomes more severe, infectiousness increases. Meanwhile the host's body mounts immune responses to contain or eradicate the pathogens, and after a certain period of time, it may achieve that goal. The quantity of pathogens in the host's body become sufficiently low so that the host is no longer capable of transmitting the disease. This usually marks the end of the infectious period, even though for some diseases such as Ebola, the virus continues to be present in the body fluids of the survivor. By contrast, if the host's body cannot recover from a potentially deadly infection, the host will die. Even after death, the infectious period might not be over. For example, the dead body of an individual who died of Ebola remains very infectious.
A related concept is the shedding period , which is the time interval during which a host or patient excretes the pathogenic organism through saliva, urine, feces or other bodily fluids. Shedding period usually coincides with the infectious period and used as its synonym. [2]
For viral infections, viral load and viral shedding are important related concepts. Viral load refers to the quantity of virions (individual virus particles) in a given bodily fluid like blood, saliva, urine, etc. at different moments after infection. Viral shedding refers to the event when a host releases pathogens into his surroundings. Together these two factors influence how much and how long pathogens will be released among a population from an infected individual, two important metrics to measure the infectiousness of a disease. If the infectious period starts before the onset of the symptoms of the disease (i.e. the end of the incubation period ), then asymptomatic carriers can unwittingly spread the disease in the community.
In epidemiology , infectivity is the ability of a pathogen to establish an infection.  More specifically, infectivity is a pathogen's capacity for horizontal transmission that is, how frequently it spreads among hosts that are not in a parent–child relationship. The measure of infectivity in a population is called incidence . [ citation needed ]
Infectivity has been shown to positively correlate with virulence . This means that as a pathogen's ability to infect a greater number of hosts increases, so does the level of harm it brings to the host. [1]
A pathogen's infectivity is subtly but importantly different from its transmissibility , which refers to a pathogen's capacity to pass from one organism to another. [ citation needed ]

This infectious disease article is a stub . You can help Wikipedia by expanding it .
First European contact in 1492 started an influx of disease into the Caribbean. Diseases originating in Europe and Africa came to North America for the first time, resulting in demographic and sociopolitical changes. The indigenous Caribbeans had little immunity to the predominantly European diseases, resulting in significant loss of life and contributing to their enslavement and exploitation. Enslaved Africans were brought to replace the dwindling indigenous population, solidifying the position of disease in triangular trade .

Before European contact , the indigenous peoples of the Caribbean are thought to have lived with infrequent epidemic diseases, brought about by limited contact between tribes. [1] This left them socially and biologically unprepared when Christopher Columbus and his crew introduced several infectious diseases, including smallpox, whooping cough, and measles. [1] The European diseases spread from the carriers to the indigenous populations, who had no immunity, leading to more serious cases and higher mortality. [2] Because the indigenous societies were not used to the diseases as European nations were at the time, there was no system in place to care for the sick. [3]
Smallpox is among the most notable of diseases in the Columbian exchange due to the high number of deaths and impact on life for indigenous societies. [2] Smallpox first broke out in the Americas on the island of Hispaniola shortly after Columbus's first voyage. [2] The disease was carried by Christopher Columbus and his crew from Europe, where it had been endemic for over seven hundred years. [2] Like the other diseases introduced in the time period, the Europeans were familiar with the treatment of the disease and had some natural immunity, which reduced mortality and facilitated quicker recovery. [2] The Taino people , who inhabited Hispaniola, had no natural smallpox immunity and were unfamiliar with treating epidemic disease. [2] The Taino population before European contact is estimated to have been between 60,000 to 8 million people, and the entire nation was virtually extinct 50 years after contact, which has primarily been attributed to the diseases. [1]
After contact, social disruption and epidemic diseases [4] led to a decline in the Amerindian population. [5] Because the indigenous societies, including the Tainos, were unfamiliar with the diseases, they were not prepared to deal with the social consequences. [3] The high number of people incapacitated by the disease disrupted the normal cycles of agriculture and hunting that sustained the native populations. [3] This led to increased dependence on the Europeans, and reduced capacity to resist the European invasion. [3] The eventual enslavement of the Taino people by the Europeans compounded the effects of the epidemics in the downfall of indigenous societies. [1]
As the population of enslaved indigenous peoples fell due to disease and abuse, the Spanish began to import enslaved workers from Africa in 1505. [6] Until 1800 the population rose as slaves arrived from West Africa. [7] Because there was already an established European colonial presence in Africa at the time, the enslaved people were less vulnerable to disease than the Taino people on Hispanola. [8] However, they came carrying their own diseases, including malaria . [9] At the time, malaria was endemic in both Europe and Africa, though more prevalent in Africa. [9] The climate of the Caribbean was hospitable to mosquitoes of the genus Anopheles , which acts as a vector for the disease and allowed it to spread. [10] Many of the African-born enslaved people had genetic protections against malaria that indigenous enslaved people did not. [8] As malaria, smallpox and other diseases spread the indigenous population continued to fall, which increased the motivation for the Spanish to continue to import more enslaved workers from Africa. [8] This enslaved people worked in mining and agriculture, driving the development of triangular trade. [8]
Infodemiology , as defined by Gunther Eysenbach in the early 2000s, is an area of science research focused on scanning the internet for user-contributed health-related content, with the ultimate goal of improving public health . [1] [2] [3] It is also defined as the science of mitigating public health problems resulting from an infodemic . [4]
Eysenbach first used the term in the context of measuring and predicting the quality of health information on the Web (i.e., measuring the "supply" side of information). [1] He later included in his definition methods and techniques which are designed to automatically measure and track health information "demand" (e.g., by analyzing search queries) as well as "supply" (e.g., by analyzing postings on webpages, in blogs, and news articles, for example through GPHIN ) on the Internet with the overarching goal of informing public health policy and practice. In 2013, the Infovigil Project was launched in an effort to bring the research community together to help realize this goal. It is funded by the Canadian Institutes of Health Research . [5]
Eysenbach demonstrated his point by showing a correlation between flu-related searches on Google (demand data) and flu- incidence data. [2] The method is shown to be better and more timely (i.e., can predict public health events earlier) than traditional syndromic surveillance methods such as reports by sentinel physicians.
Researchers have applied an infodemiological approach to studying the spread of HIV/AIDS , [6] SARS [7] and influenza , [8] [9] [10] vaccination uptake, [11] [12] antibiotics consumption, [13] the incidence of multiple sclerosis , [14] [15] patterns of alcohol consumption, [16] the efficacy of using the social web for personalization of health treatment, [17] [18] the contexts of status epilepticus patients, [19] [20] factors of Abdominal pain and its impact on quality of life [21] and the effectiveness of the Great American Smokeout anti-smoking awareness event. [22] Applications outside the field of health care include urban planning [23] and the study of economic trends and voter preferences. [24]

Infoveillance is a type of syndromic surveillance that specifically utilizes information found online. [1] The term, along with the term infodemiology , was coined by Gunther Eysenbach to describe research that uses online information to gather information about human behavior. [2] [3] [4]
Eysenbach's work using Google Search queries led to the birth of Google Flu Trends , and other search engines have also been used. [5] [6] Other researchers have utilized social media sites such as Twitter to observe disease outbreak patterns. [7] [8] Infoveillance can detect disease outbreaks faster than traditional public health surveillance systems with minimal costs involved. [9]
Infoveillance methods may be either passive or active. [4] Traditional infoveillance data like search engine queries and website navigation behavior are considered passive, as they attempt to recognize trends automatically, without action (or often even awareness) on the part of the internet users who are generating the data for analysis. Active infoveillance occurs when users choose to respond to a survey, enter symptoms into a website or app, or otherwise participate directly in surveillance efforts by contributing additional information. [4]
Beginning in 2008, Google used aggregated search query data to detect influenza trends and compared the results to countries' official surveillance data with the goal of predicting the spread of the flu. [10] In light of evidence that emerged in 2013 showing that Google Flu Trends sometimes substantially overestimated actual flu rates, researchers proposed a series of more advanced and better-performing approaches to flu modeling from Google search queries. [11] Google Flu Trends stopped publishing reports in 2015. [12]
Google also used aggregated search query data to detect dengue fever trends. [13] Research has also cast doubt on the accuracy of some of these predictions. [14] Google has continued this work to track and predict the COVID-19 pandemic , creating an open dataset on COVID-related search queries for use by researchers. [15]
Other flu prediction projects, including Flu Detector, have come and gone since the advent and removal of Google Flu Trends. Flu Detector was developed by Vasileios Lampos and other researchers at the University of Bristol . [7] It was an application of machine learning that first used feature selection to automatically extract flu-related terms from Twitter content and then used those terms to compute a flu-score for several UK regions based on geolocated tweets. It also formed the basis for a proposed generalized scheme able to track other events. [16]
Mood of the Nation was also developed by Lampos' team. It performed mood analysis on tweets geo-located in various regions of the United Kingdom by computing on a daily basis scores for four types of emotion: anger, fear, joy and sadness.
The rise of infoveillance brings up questions about privacy. Privacy concerns are partially dependent on the level of analysis and how data are collected and managed. [4] For instance, individuals may be re-identifiable from search query datasets that have not been properly de-identified . [17] Privacy concerns are increased if data analysis is not done automatically and if search trajectories of individual users are examined. [4]
Institute for Disease Modeling (IDM) is an institute within the Global Health Division of the Bill and Melinda Gates Foundation . Established in 2008 as part of the Global Good Fund, a non-profit subsidiary of Intellectual Ventures (IV) funded by Bill and Melinda Gates, IDM has transitioned in mid-2020 to the Gates Foundation. [1]
IDM specializes in mathematical modelling of infectious disease and other quantitative global health research. Its models include malaria, polio, measles, COVID-19 [2] and HIV (with EMOD). IDM releases source code of their stable models to the public. [3] [4] While at IV, the institute was located in Bellevue, Washington . After the outbreak of COVID-19 in Washington State , IDM has transitioned to all-remote work with no physical offices. It will eventually relocate to the Gates Foundation's main office in Seattle .
EMOD is the group's individual-based disease modeling software (not a compartmental model) initially coded c. 2005. It has been released to the public as open-source software . The software can model malaria, HIV, tuberculosis, measles, dengue, polio and typhoid. [5]
In 2020, IDM developed a designated COVID-19 agent-based model named "Covasim." It was used initially to advise on decision-making during in the COVID-19 pandemic in Oregon and in Washington State , [2] [6] gaining national attention. [7] [8] Covasim, coded in Python , is open-source and has been used by independent researchers around the world. [9]

This article about the COVID-19 pandemic is a stub . You can help Wikipedia by expanding it .
In medicine an intention-to-treat ( ITT ) analysis of the results of an experiment is based on the initial treatment assignment and not on the treatment eventually received. ITT analysis is intended to avoid various misleading artifacts that can arise in intervention research such as non-random attrition of participants from the study or crossover . ITT is also simpler than other forms of study design and analysis, because it does not require observation of compliance status for units assigned to different treatments or incorporation of compliance into the analysis. Although ITT analysis is widely employed in published clinical trials, it can be incorrectly described and there are some issues with its application. [1] Furthermore, there is no consensus on how to carry out an ITT analysis in the presence of missing outcome data. [2]
Randomized clinical trials analyzed by the intention-to-treat (ITT) approach provide unbiased comparisons among the treatment groups. Intention to treat analyses are done to avoid the effects of crossover and dropout , which may break the random assignment to the treatment groups in a study. ITT analysis provides information about the potential effects of treatment policy rather than on the potential effects of specific treatment.
Since it started in the 1960s, the principle of ITT has become widely accepted for the analysis of controlled clinical trials.
In an ITT population, none of the patients are excluded and the patients are analyzed according to the randomization scheme. In other words, for the purposes of ITT analysis, everyone who is randomized in the trial is considered to be part of the trial regardless of whether he or she is dosed or completes the trial.
For example, if people who have a more refractory or serious problem tend to drop out of a study at a higher rate, even a completely ineffective treatment may appear to be providing benefits if one merely compares the condition before and after the treatment for only those who finish the study (ignoring those who were enrolled originally, but have since been excluded or dropped out).
Medical investigators often have difficulties in completing ITT analysis because of clinical trial issues like missing data or poor treatment protocol adherence. [3]
To address some of these issues, many clinical trials have excluded participants after the random assignment in their analysis, which is often referred to as modified intention-to-treat analysis or mITT. Trials employing mITT have been linked to industry sponsorship and conflicts of interest by the authors. [4]
An important problem is the occurrence of missing data for participants in a clinical trial. This can happen when patients are lost to follow-up (for instance, by withdrawal due to adverse effects of the intervention) and no response is obtainable for these patients. However, full application of ITT analysis can only be performed where there is complete outcome data for all randomized subjects.
In order to include such participants in an analysis, outcome data could be imputed which involves making assumptions about the outcomes in the lost participants. Another approach would be efficacy subset analysis which selects the subset of the patients who received the treatment of interest—regardless of initial randomization—and who have not dropped out for any reason. This approach can introduce biases to the statistical analysis. It can also inflate the chance of a false positive ; this effect is greater the larger the trial. [5]
ITT analysis requires participants to be included even if they did not fully adhere to the protocol. Participants who strayed from the protocol (for instance, by not adhering to the prescribed intervention, or by being withdrawn from active treatment) should still be kept in the analysis. An extreme variation of this is the participants who receive the treatment from the group they were not allocated to, who should be kept in their original group for the analysis. This issue causes no problems provided that, as a systematic reviewer, you can extract the appropriate data from the trial reports. The rationale for this approach is that, in the first instance, we want to estimate the effects of allocating an intervention in practice, not the effects in the subgroup of the participants who adhere to it.
In comparison, in a per-protocol analysis , [6] only patients who complete the entire clinical trial according to the protocol are counted towards the final results. [7]
Intermittent preventive therapy or intermittent preventive treatment ( IPT ) is a public health intervention aimed at treating and preventing malaria episodes in infants (IPTi), children (IPTc), schoolchildren (IPTsc) and pregnant women (IPTp). The intervention builds on two tested malaria control strategies to clear existing parasites (treatment effect seen in mass drug administrations ) and to prevent new infections ( prophylaxis ).
IPTi using the antimalarial drug sulfadoxine / pyrimethamine (S/P) was pioneered in Ifakara, Tanzania in 1999. [1] Infants received S/P at ages 3, 6, and 9 months in combination with their routine childhood (EPI) vaccinations. IPTi reduced clinical attacks of malaria by 59% (95% CI, 41%–72%) in Ifakara. Remarkably, protection persisted throughout the second year of life, long after SP had disappeared from circulation. [2] A trial conducted in northern Tanzania using the antimalarial drug amodiaquine instead of S/P was similarly successful. [3] Six subsequent trials showed less encouraging results. [4] [5] [6] [7] [8]
The latest and so far largest IPTi study was an effectiveness study conducted in the South East of Tanzania. [9] A study area of approximately 250x180km2 with a population of about 900,000 people was subdivided into 24 similar clusters. Half of the 23,400 infants, those residing in 12 of 24 randomly selected clusters were invited in 2005 to receive IPTi. Between 47 and 76% of the eligible infants in each of the 12 selected clusters received IPT-SP. In the following year, 2006, the effect of IPTi on malaria and anaemia was assessed in a representative sample of 600 infants. An intention to treat analysis, which includes all eligible infants did not show a statistically significant benefit of IPTi-SP. Parasitaemia prevalence was 31% in the intervention and 38% in the comparison areas (p=0.06). In a ‘per protocol’ analysis, which only included infants who actually received IPTi there was a significant benefit: parasite prevalence was 22%, 19 percentage points lower than comparison children in the control group (p=0.01). This trial showed that IPTi has a protective effect at the individual level but is not effective at the community level. The study had followed up children for two years until 2007 but the findings from the surveillance in 2007 have not been reported. [ citation needed ]
Treating children with S/P and artesunate in Senegal where malaria is highly seasonal repeatedly during the malaria season reduced malaria attacks by 86% (95% CI 80-90)9. [10] A subsequent trial in Mali showed a protective efficacy of 43% [95% CI 29–54%]. [11]
Treating schoolchildren in Kenya with S/P and amodiaquine significantly improved anaemia (RR 0.52, 95% CI 0.29-0.93). [12]
IPTp consists in the administration of a single curative dose of an efficacious anti-malarial drug at least twice during pregnancy – regardless whether or not the woman is infected. The drug is administered under supervision during antenatal care (ANC) visits. Sulfadoxine-pyrimethamine is the drug currently recommended by the WHO because of its safety and efficacy in pregnancy. [13] Several studies have shown the high efficacy of IPTp with SP, compared to placebo or CQ prophylaxis, on placental infection, LBW and/or severe maternal anaemia. [14] [15] [16] [17] [18] [19] [20] [21] [ excessive citations ] More recent findings from Tanzania also suggest that IPTp using S/P has reached the end of its lifecycle. [22] The authors found that the "use of partially effective anti-malarial agents for IPTp may exacerbate malaria infections in the setting of widespread drug resistance". As for infants, there is no simple readily available replacement of S/P for malaria in pregnancy. Indeed, the fear of teratogenic effects add a layer of complexity how this intervention will evolve.
While some controversial aspects e.g. the drug of choice are shared by all forms of intermittent preventive therapy, the controversy has been reported in greatest detail for IPTi (see also politics below). The reasons which make the large scale introduction of IPTi highly controversial include: [23]
An added theoretical concern is that the widespread use of antimalarial drugs for prophylaxis will add to the already considerable drug pressure and will facilitate the emergence and spread of drug resistance. McGready summarised IPTi as an intervention which uses the wrong drug, probably in the wrong dose in the wrong age group. [25]
The politics of IPTi are well documented and illustrate the working of contemporary international health politics. The promising results of the first two IPTi studies led to the creation of the IPTi Consortium, whose brief is to determine the efficacy, safety, relation of efficacy to drug sensitivity, cost-effectiveness, and acceptability of this intervention. [26] The IPTi Consortium received approximately US$28 million from the Bill & Melinda Gates Foundation (BMGF). A WHO technical advisory group reviewed the evidence relevant for the widespread  introduction of IPTi  available in 2008, and came to the conclusion that the available evidence was not sufficient to recommend the widespread introduction of IPTi -SP. Program officers of the BMGF as well as scientists funded by the BMGF criticised the WHO conclusions. The criticism from the BMGF in turn triggered a memorandum of the WHO malaria chief Dr. Akira Kochi to the director general of the WHO which was leaked to The New York Times . [27]
Dr. Kochi wrote, although it was less and less straightforward  that the health agency should recommend IPTi, the agency's objections were met with intense and aggressive opposition from Gates-backed scientists and the foundation. The W.H.O., he wrote, needs to stand up to such pressures and ensure that the review of evidence is rigorously independent of vested interests.
At the request of Dr. Brandling-Bennett of the BMGF and with funding from the BMGF, the Institute of Medicine (IOM) convened an expert committee to evaluate the evidence concerning IPTi - SP and provide guidance on the value of continued investment in IPTi-SP. The committee was chaired by Myron M. Levine who has been funded and is currently funded by the BMGF. The committee concluded "… that an intervention with results of this magnitude is worthy of further investment as part of a public health strategy to decrease morbidity from malaria infections in infants." [28] The WHO technical expert group responded to the IOM report "WHO is committed to review the available information each year." Dr. Kochi was ultimately replaced by one of the members of the IPTi consortium, Dr. Robert Newman. In March 2010, i.e. after Dr. Kochi had been replaced, the WHO recommended the co-administration of the antimalarial drug sulfadoxine pyrimethamine with routine childhood vaccinations ( DTP2, DTP3 and measles immunization) in sub-Saharan Africa. [29] The recommendation applies only for areas with high malaria transmission and low resistance against SP, both measures are not free of controversy and only available for few spots in Africa. With the recent drop of malaria transmission in wide stretches of Africa [30] [31] and a steady increase in SP resistance [32] [33] few malaria control programs will hurry to implement this intervention.
In statistics and research , internal consistency is typically a measure based on the correlations between different items on the same test (or the same subscale on a larger test). It measures whether several items that propose to measure the same general construct produce similar scores. For example, if a respondent expressed agreement with the statements "I like to ride bicycles" and "I've enjoyed riding bicycles in the past", and disagreement with the statement "I hate bicycles", this would be indicative of good internal consistency of the test.
Internal consistency is usually measured with Cronbach's alpha, a statistic calculated from the pairwise correlations between items. Internal consistency ranges between negative infinity and one. Coefficient alpha will be negative whenever there is greater within-subject variability than between-subject variability. [1]
A commonly accepted rule of thumb for describing internal consistency is as follows: [2]
Very high reliabilities (0.95 or higher) are not necessarily desirable, as this indicates that the items may be redundant. [3] The goal in designing a reliable instrument is for scores on similar items to be related (internally consistent), but for each to contribute some unique information as well. Note further that Cronbach's alpha is necessarily higher for tests measuring more narrow constructs, and lower when more generic, broad constructs are measured. This phenomenon, along with a number of other reasons, argue against using objective cut-off values for internal consistency measures. [4] Alpha is also a function of the number of items, so shorter scales will often have lower reliability estimates yet still be preferable in many situations because they are lower burden.
An alternative way of thinking about internal consistency is that it is the extent to which all of the items of a test measure the same latent variable .  The advantage of this perspective over the notion of a high average correlation among the items of a test – the perspective underlying Cronbach's alpha – is that the average item correlation is affected by skewness (in the distribution of item correlations) just as any other average is.  Thus, whereas the modal item correlation is zero when the items of a test measure several unrelated latent variables, the average item correlation in such cases will be greater than zero.  Thus, whereas the ideal of measurement is for all items of a test to measure the same latent variable, alpha has been demonstrated many times to attain quite high values even when the set of items measures several unrelated latent variables. [5] [6] [7] [8] [9] [10] [11] The hierarchical "coefficient omega" may be a more appropriate index of the extent to which all of the items in a test measure the same latent variable. [12] [13] Several different measures of internal consistency are reviewed by Revelle & Zinbarg (2009). [14] [15]
The ICEID or International Conference on Emerging Infectious Diseases is a conference for public health professionals on the subject of emerging infectious diseases.
From CDC page for ICEID: [1]
The International Conference on Emerging Infectious Diseases was first convened in 1998; ICEID 2006 marks its fifth occurrence. The conference brings together public health professional to encourage the exchange of scientific and public health information on global emerging infectious disease issues. The program will include plenary and panel sessions with invited speakers as well as oral and poster presentations on emerging infections. Major topics include current work on surveillance, epidemiology, research, communication and training, bioterrorism, and preventions and control of emerging infectious diseases, both in the United States and abroad.
Major subjects covered include: [2]
From Yahoo news report: [3] [4]
Speaking to the International Conference on Emerging Infectious Diseases in Atlanta, Garten said the pool of H5N1 candidates with the potential to cause a human influenza pandemic is getting more genetically diverse, which makes studying the virus more complex and heightens the need for increased surveillance. "As the virus continues its geographic expansion, it is also undergoing genetic diversity expansion," Garten said in a statement.

This article about a medical organization or association is a stub . You can help Wikipedia by expanding it .
This epidemic - or pandemic - related article is a stub . You can help Wikipedia by expanding it .
Internet-mediated research ( IMR ) is the research conducted through the medium of the Internet. [1] In the medical field, it pertains to the practice of gathering medical, biomedical or health related research data via the internet directly from research subjects. The subject, uses a web browser to view and respond to questionnaires that are included in an approved medical research protocol. Other fields such as geography also use IMR as a research tool. [1]
The primary Internet-mediated research is classified into three main types: online questionnaires, virtual interviews, and virtual ethnographies. [1] There is also the case of secondary Internet research, which involves the use of the Internet in the location of secondary information sources such as journal databases, newspapers, and digital archives, among others. [2] Some sources, however, exclude this type in their conceptualization of IMR. [3]
In a traditional medical research study, the principal investigator , Research Coordinator, or other study staff conducts an interview with the research subject and records the information on a paper or electronic case report form . Using IMR, the research subject instead responds to a questionnaire without the guidance of a research staff member, often performing the action at a time and place disassociated with the research clinic, using only a computer connected to the internet and a standard browser.
Recently, the medical community has begun to study whether there are differences between IMR data and traditionally collected data. [4]
Inverse probability weighting is a statistical technique for calculating statistics standardized to a pseudo-population different from that in which the data was collected. Study designs with a disparate sampling population and population of target inference (target population) are common in application. [1] There may be prohibitive factors barring researchers from directly sampling from the target population such as cost, time, or ethical concerns. [2] A solution to this problem is to use an alternate design strategy, e.g. stratified sampling . Weighting, when correctly applied, can potentially improve the efficiency and reduce the bias of unweighted estimators.
One very early weighted estimator is the Horvitz–Thompson estimator of the mean. [3] When the sampling probability is known, from which the sampling population is drawn from the target population, then the inverse of this probability is used to weight the observations. This approach has been generalized to many aspects of statistics under various frameworks. In particular, there are weighted likelihoods , weighted estimating equations , and weighted probability densities from which a majority of statistics are derived. These applications codified the theory of other statistics and estimators such as marginal structural models , the standardized mortality ratio , and the EM algorithm for coarsened or aggregate data.
Inverse probability weighting is also used to account for missing data when subjects with missing data cannot be included in the primary analysis. [4] With an estimate of the sampling probability, or the probability that the factor would be measured in another measurement, inverse probability weighting can be used to inflate the weight for subjects who are under-represented due to a large degree of missing data .
The inverse probability weighting estimator can be used to demonstrate causality when the researcher cannot conduct a controlled experiment but has observed data to model. Because it is assumed that the treatment is not randomly assigned, the goal is to estimate the counterfactual or potential outcome if all subjects in population were assigned either treatment.
Suppose observed data are { ( X i , A i , Y i ) } i = 1 n {\displaystyle \{{\bigl (}X_{i},A_{i},Y_{i}{\bigr )}\}_{i=1}^{n}} drawn i.i.d [ clarification needed ] (independent and identically distributed) from unknown distribution P, where
The goal is to estimate the potential outcome, Y ∗ ( a ) {\displaystyle Y^{*}{\bigl (}a{\bigr )}} , that would be observed if the subject were assigned treatment a {\displaystyle a} . Then compare the mean outcome if all patients in the population were assigned either treatment: μ a = E Y ∗ ( a ) {\displaystyle \mu _{a}=\mathbb {E} Y^{*}(a)} . We want to estimate μ a {\displaystyle \mu _{a}} using observed data { ( X i , A i , Y i ) } i = 1 n {\displaystyle \{{\bigl (}X_{i},A_{i},Y_{i}{\bigr )}\}_{i=1}^{n}} .
μ ^ a , n I P W E = 1 n ∑ i = 1 n Y i 1 A i = a p ^ n ( A i | X i ) {\displaystyle {\hat {\mu }}_{a,n}^{IPWE}={\frac {1}{n}}\sum _{i=1}^{n}Y_{i}{\frac {\mathbf {1} _{A_{i}=a}}{{\hat {p}}_{n}(A_{i}|X_{i})}}}
With the mean of each treatment group computed, a statistical t-test or ANOVA test can be used to judge difference between group means and determine statistical significance of treatment effect.
The Inverse Probability Weighted Estimator (IPWE) can be unstable if estimated propensities are small. If the probability of either treatment assignment is small, then the logistic regression model can become unstable around the tails causing the IPWE to also be less stable.
An alternative estimator is the augmented inverse probability weighted estimator (AIPWE) combines both the properties of the regression based estimator and the inverse probability weighted estimator. It is therefore a 'doubly robust' method in that it only requires either the propensity or outcome model to be correctly specified but not both. This method augments the IPWE to reduce variability and improve estimate efficiency. This model holds the same assumptions as the Inverse Probability Weighted Estimator (IPWE). [5]
μ ^ a , n A I P W E = 1 n ∑ i = 1 n ( Y i 1 A i = a p ^ n ( A i | X i ) − 1 A i = a − p ^ n ( A i | X i ) p ^ n ( A i | X i ) Q ^ n ( X i , a ) ) = 1 n ∑ i = 1 n ( Q ^ n ( X i , a ) ) + 1 n ∑ i = 1 n 1 A i = a p ^ n ( A i | X i ) ( Y i − Q ^ n ( X i , a ) ) {\displaystyle {\hat {\mu }}_{a,n}^{AIPWE}={\frac {1}{n}}\sum _{i=1}^{n}{\Biggl (}{\frac {Y_{i}1_{A_{i}=a}}{{\hat {p}}_{n}(A_{i}|X_{i})}}-{\frac {1_{A_{i}=a}-{\hat {p}}_{n}(A_{i}|X_{i})}{{\hat {p}}_{n}(A_{i}|X_{i})}}{\hat {Q}}_{n}(X_{i},a){\Biggr )}={\frac {1}{n}}\sum _{i=1}^{n}{\Biggl (}{\hat {Q}}_{n}(X_{i},a){\Biggr )}+{\frac {1}{n}}\sum _{i=1}^{n}{\frac {1_{A_{i}=a}}{{\hat {p}}_{n}(A_{i}|X_{i})}}{\Biggl (}Y_{i}-{\hat {Q}}_{n}(X_{i},a){\Biggr )}}
With the following notations:
The later rearrangement of the formula helps reveal the underlying idea: our estimator is based on the average predicted outcome using the model (i.e.: 1 n ∑ i = 1 n ( Q ^ n ( X i , a ) ) {\displaystyle {\frac {1}{n}}\sum _{i=1}^{n}{\Biggl (}{\hat {Q}}_{n}(X_{i},a){\Biggr )}} ). However, if the model is biased, then the residuals of the model will not be (in the full treatment group a) around 0. We can correct this potential bias by adding the extra term of the average residuals of the model (Q) from the true value of the outcome (Y) (i.e.: 1 n ∑ i = 1 n 1 A i = a p ^ n ( A i | X i ) ( Y i − Q ^ n ( X i , a ) ) {\displaystyle {\frac {1}{n}}\sum _{i=1}^{n}{\frac {1_{A_{i}=a}}{{\hat {p}}_{n}(A_{i}|X_{i})}}{\Biggl (}Y_{i}-{\hat {Q}}_{n}(X_{i},a){\Biggr )}} ). Because we have missing values of Y, we give weights to inflate the relative importance of each residual (these weights are based on the inverse propensity, a.k.a. probability, of seeing each subject observations) (see page 10 in [6] ).
The "doubly robust" benefit of such an estimator comes from the fact that it's sufficient for one of the two models to be correctly specified, for the estimator to be unbiased (either Q ^ n ( X i , a ) {\displaystyle {\hat {Q}}_{n}(X_{i},a)} or p ^ n ( A i | X i ) {\displaystyle {\hat {p}}_{n}(A_{i}|X_{i})} , or both). This is because if the outcome model is well specified then it's residuals will be around 0 (regardless of the weights each residual will get). While if the model is biased, but the weighting model is well specified, then the bias will be well estimated (And corrected for) by the weighted average residuals. [6] [7] [8]
The bias of the doubly robust estimators is called a second-order bias , and it depends on the product of the difference 1 p ^ n ( A i | X i ) − 1 p n ( A i | X i ) {\displaystyle {\frac {1}{{\hat {p}}_{n}(A_{i}|X_{i})}}-{\frac {1}{{p}_{n}(A_{i}|X_{i})}}} and the difference Q ^ n ( X i , a ) − Q n ( X i , a ) {\displaystyle {\hat {Q}}_{n}(X_{i},a)-Q_{n}(X_{i},a)} . This property allows us, when having a "large enough" sample size, to lower the overall bias of doubly robust estimators by using machine learning estimators (instead of parametric models). [9]
The Israeli paradox is a catchphrase , first used in 1996, to summarize the apparently paradoxical epidemiological observation that Israeli Jews have a relatively high incidence of coronary heart disease (CHD), despite having a diet relatively low in saturated fats , in apparent contradiction to the widely held belief that the high consumption of such fats is a risk factor for CHD .  The paradox is that if the thesis linking saturated fats to CHD is valid, the Israelis ought to have a lower rate of CHD than comparable countries where the per capita consumption of such fats is higher.
The observation of Israel's paradoxically high rate of CHD is one of a number of paradoxical outcomes for which a literature now exists, regarding the thesis that a high consumption of saturated fats ought to lead to an increase in CHD incidence, and that a lower consumption ought to lead to the reverse outcome. The most famous of these paradoxes is known as the " French paradox ": France enjoys a relatively low incidence of CHD despite a high per-capita consumption of saturated fat.
The Israeli paradox implies two important possibilities. The first is that the hypothesis linking saturated fats to CHD is not completely valid (or, at the extreme, is entirely invalid). The second possibility is that the link between saturated fats and CHD is valid, but that some additional factor in the typical Israeli diet, lifestyle or genes creates another CHD risk—presumably with the implication that if this factor can be identified, it can be isolated in the diet or lifestyle of other countries, thereby allowing both the Israelis, and others, to avoid that particular risk.
Israeli Jews eat a diet which is richer in linoleic acid (the most readily available plant-based form of omega-6 fatty acid , found in many vegetable oils) than any other population on the planet. Average per capita consumption is approximately 30 grams a day (11 kilograms annually), [1] compared to 25 grams daily for the average American in 1985. [2]
Susan Allport summarizes the Israeli paradox in the following words:
Israelis eat less animal fat and cholesterol and fewer calories than Americans, but they have comparable rates of heart disease, obesity, diabetes and many cancers. They have an ideal diet, as far as the American food pyramid is concerned, but far from ideal health.” [3]
Allport notes that “Little butter is consumed in Israel, but large quantities of soybean, corn and safflower oil are…. This translates, researchers estimate, to a linoleic acid intake of about 11 percent of calories and a ratio of linoleic to alpha linolenic acid [the most readily-available plant-based omega-3 fatty acid] in the Israeli diet of about 26:1.” [4] She observes that mean serum cholesterol in Israel is quite low by the standards of developed countries: 210 milligrams/dl. Therefore, one distinction that can, without controversy, be attributed to the high levels of linoleic acid in the Israeli diet is the high percentage of linoleic acid in the adipose tissue of Israelis: 24% as compared to 16% in Americans and less than 10% in many northern Europeans. [5]
In 1993, a 23-year follow-up study to the Israeli Ischemic Heart Disease Study of ten thousand public service employees (widely referred to as the “Israeli Civil Service Study” [6] ) found that there were only “weak associations of long-term coronary mortality with the dietary intake patterns of fatty acids.” [7]
The term "Israeli Paradox" was first used by researchers Daniel Yam, Abraham Eliraz and Elliot Berry in a 1996 article in the Israel Journal of Medical Sciences . [8] The authors observed that Israelis consumed polyunsaturated fatty acids (primarily omega-6s rather than omega-3s)  at a rate about 8% higher than in the United States and about 10-12% higher than most of Europe. They wrote, “Israeli Jews may be regarded as a population-based dietary experiment of the effect of a high omega-6 PUFA [polyunsaturated fatty acid] diet.”
The most readily-observable result of the relatively high ratio of omega-6 fats to saturated fats in the Israeli diet was the deposition of omega-6 fats in preference to saturated fats in the adipose tissue of Israelis. This had been observed as early as 1976, when an article in the Israel Journal of Medical Sciences noted that, counting polyunsaturated fats (which includes both omega-6 and omega-3 fatty acids ) as a whole, the ratio of polyunsaturated fats to saturated fats in the adipose tissue of Ashkenazi Jews in Israel was 0.88:1, while for non-Ashkenazi Jews it was 1.13:1. This was, according to the authors, “the highest reported for any population on a free-choice diet.” [9]

The Johns Hopkins Bloomberg School of Public Health ( JHSPH ) is part of Johns Hopkins University in Baltimore, Maryland , United States . As the first independent, degree-granting institution for research in epidemiology and training in public health, [4] and the largest public health training facility in the United States, [5] [6] [7] [8] the Bloomberg School is a leading international authority on the improvement of health and prevention of disease and disability. The school's mission is to protect populations from illness and injury by pioneering new research, deploying its knowledge and expertise in the field, and training scientists and practitioners in the global defense of human life. [2] The school is ranked first in public health in the U.S. News and World Report rankings and has held that ranking since 1994. [9]
Originally named the Johns Hopkins School of Hygiene and Public Health, the school was founded in 1916 by William H. Welch with a grant from the Rockefeller Foundation. The school was renamed the Johns Hopkins Bloomberg School of Public Health on April 20, 2001, in honor of Michael Bloomberg (founder of the eponymous media company ) for his financial support and commitment to the school and Johns Hopkins University. Bloomberg has donated a total of $2.9 billion to Johns Hopkins University over a period of several decades.
The school is also the founder of Delta Omega (est. 1924), the national honorary society for graduate training in public health. [10] [11] The Bloomberg School is fully accredited by the Council on Education for Public Health (CEPH) . [12]
In 1913, the Rockefeller Foundation sponsored a conference on the need for public health education in the United States. Foundation officials were convinced that a new profession of public health was needed. It would be allied to medicine but also distinct, with its own identity and educational institutions. [13] The result of deliberations between public health leaders and foundation officials was the Welch–Rose Report of 1915, which laid out the need for adequately trained public health workers , and envisioned an "institute of hygiene" for the United States. [14] The Report, reflected the different preferences of the plan's two architects— William Henry Welch favored scientific research , whereas Wickliffe Rose wanted an emphasis on public health practice . [13]
In June 1916, the executive committee of the Rockefeller Foundation approved the plan to organize an institute or school of public health at the Johns Hopkins University in Baltimore, Maryland, United States. The institute was named the School of Hygiene and Public Health, indicating a compromise between those who wanted the practical public health training on the British model and those who favoured basic scientific research on the German model. [14] Welch, the first Dean of the Johns Hopkins School of Medicine also became the founding Dean of the first school of public health in the United States.
The facility is located on the former Maryland Hospital site founded in 1797. The Maryland Hospital was originally built as a hospital to care for Yellow Fever for the indigent away from the city. In 1840, the hospital expanded to exclusively care for the mentally ill. In 1873, the buildings were torn down as the facility relocated to a new site as the Spring Grove Hospital Center . [15]
The Johns Hopkins School of Public Health represents the archetype for formalized public health training and epidemiology education in the United States. By 1922, other schools of public health at Harvard , Columbia and Yale had all been established in accordance with the Hopkins model. [16] The Rockefeller Foundation continued to sponsor the creation of public health schools in the United States and around the world in the 1920s and 1930s, extending the American model of the Johns Hopkins School of Public Health to countries such as Brazil, Bulgaria, Canada, Czechoslovakia, England, Hungary, India, Italy, Japan, Norway, the Philippines, Poland, Rumania, Sweden, Turkey, and Yugoslavia. [14]
The school celebrated its 100th anniversary during the 2015–2016 academic year with programs, festivities, and innovative projects to spotlight 100 years of pioneering public health—connecting a century of achievements to the promise of new advances for the next century. [17]
The Bloomberg School is the largest school of public health in the world, with 530 full-time and 620 part-time faculty, and 2,030 students from 84 countries. [18] It is home to over fifty research centers and institutes with research ongoing in the U.S. and more than 90 countries worldwide. [19] The School ranks first in federal research support from the National Institutes of Health (NIH), receiving nearly 25 percent of all funds distributed among the 40 U.S. schools of public health, [18] and has consistently been ranked first among schools of public health by U.S. News & World Report . [9]
The School offers:
The Bloomberg School is composed of 10 academic departments: [26]
In addition to these ten academic departments, there is a school-wide MPH program and the Graduate Training Program in Clinical Investigation which is a collaborative program between the School of Public Health and School of Medicine.
The Bloomberg School of Public Health is located in the East Baltimore campus of the Johns Hopkins University. The campus, collectively known as the Johns Hopkins Medical Institutions [33] (JHMI), is also home to the School of Medicine and the School of Nursing and comprises several city blocks, radiating outwards from the Billings Building of the Johns Hopkins Hospital with its historic dome. The main building on which the school is located is on North Wolfe Street ; it has nine floors and features an observation area and a fitness center on the top floor. The Bloomberg School also occupies Hampton House on North Broadway. The school is also serviced by the Welch Medical Library, a central resource shared by all the schools of the Medical Campus. The campus includes the Lowell Reed Residence Hall [34] and the Denton Cooley Recreational Center. [35] Public transportation to and from the campus is served by the Baltimore Metro Subway , local buses, and the JHMI shuttle. [36]
Some of the graduates of the Bloomberg School of Public Health include
The official title of the head of the School has changed periodically between Director and Dean throughout the years. [42] Originally the title was Director. In 1931, it was changed to Dean and in 1946 back to Director. In 1958, the title again became Dean. The Deans (Directors) of the Bloomberg School include:
Coordinates : 39°17′52″N 76°35′27″W ﻿ / ﻿ 39.29785°N 76.590757°W ﻿ / 39.29785; -76.590757
A job-exposure matrix (JEM) is a tool used to assess exposure to potential health hazards in occupational epidemiological studies .
Essentially, a JEM comprises a list of levels of exposure to a variety of harmful (or potentially harmful) agents for selected occupational titles. In large population-based epidemiological studies, JEMs may be used as a quick and systematic means of converting coded occupational data (job titles) into a matrix of possible exposures, [1] eliminating the need to assess each individual's exposure in detail.
Assessing exposure by title is less costly than looking at individual cases. JEMs may also reduce differential information bias that might occur when evaluating exposure for individuals from medical records in which their jobs are apparent. [1]
Variability of exposure within occupational classes in different workplaces, countries, or throughout time is commonly not taken into account, which can result in nondifferential exposure misclassification. [1]
The Kelly M. West Award for Outstanding Achievement in Epidemiology is an honor bestowed by the American Diabetes Association . It has been awarded annually to an individual since 1986. [1] [2] The award is named in honor of Kelly M. West . [1]
Source: [11]
Kermack–McKendrick theory is a hypothesis that predicts the number and distribution of cases of an infectious disease as it is transmitted through a population over time. Building on the research of Ronald Ross and Hilda Hudson , A. G. McKendrick and W. O. Kermack published their theory in a set of three articles from 1927, 1932, and 1933.  While Kermack–McKendrick theory was indeed the source of SIR models and their relatives, Kermack and McKendrick were thinking of a more subtle and empirically useful problem than the simple compartmental models discussed here.  The text is somewhat difficult to read, compared to modern papers, but the important feature is it was a model where the age-of-infection affected the transmission and removal rates.
Because of their seminal importance to the field of theoretical epidemiology, these articles were republished in the Bulletin of Mathematical Biology in 1991. [1] [2] [3]
In its initial form, Kermack–McKendrick theory is a partial differential-equation model that structures the infected population in terms of age-of-infection, while
using simple compartments for people who are susceptible (S), infected (I), and recovered/removed (R).
Specified initial conditions would change over time according to
where δ ( a ) {\displaystyle \delta (a)} is a Dirac delta-function and the infection pressure
This formulation is equivalent to defining the incidence of infection i ( t , 0 ) = λ S {\displaystyle i(t,0)=\lambda S} .
Only in the special case when the removal rate γ ( a ) {\displaystyle \gamma (a)} and the transmission rate β ( a ) {\displaystyle \beta (a)} are constant for all ages can the epidemic dynamics be expressed in terms of the prevalence I ( t ) {\displaystyle I(t)} , leading to the standard compartmental SIR model .  This model only accounts for infection and removal events, which are sufficient to describe a simple epidemic, including the threshold condition necessary for an epidemic to start, but can not explain endemic disease transmission or recurring epidemics.
In their subsequent articles, Kermack and McKendrick extended their theory to allow for birth, migration, and death, as well as imperfect immunity.  In modern notation, their model can be represented as
where b 0 {\displaystyle b_{0}} is the immigration rate of susceptibles, b j is the per-capita birth rate for state j , m j is the per-capita mortality rate of individuals in state j , σ {\displaystyle \sigma } is the relative-risk of infection to recovered individuals who are partially immune, and the infection pressure
Kermack and McKendrick were able to show that it admits a stationary solution where disease is endemic, as long as the supply of susceptible individuals is sufficiently large.  This model is difficult to analyze in its full generality, and a number of open questions remain regarding its dynamics.
Koch's postulates ( / ˈ k ɔː x / ) [2] are four criteria designed to establish a causative relationship between a microbe and a disease . The postulates were formulated by Robert Koch and Friedrich Loeffler in 1884, based on earlier concepts described by Jakob Henle , [3] and refined and published by Koch in 1890. [ citation needed ] Koch applied the postulates to describe the etiology of cholera and tuberculosis , both of which are now ascribed to bacteria . The postulates have been controversially generalized to other diseases. [ citation needed ] More modern concepts in microbial pathogenesis cannot be examined using Koch's postulates, including viruses (which are obligate cellular parasites) and asymptomatic carriers . [ citation needed ] They have largely been supplanted by other criteria such as the Bradford Hill criteria for infectious disease causality in modern public health , [ citation needed ] and Falkow's criteria for microbial pathogenesis .
Koch's postulates are the following:
However, Koch later abandoned the universalist requirement of the first postulate altogether when he discovered asymptomatic carriers of cholera [4] and, later, of typhoid fever . [ citation needed ] Asymptomatic or subclinical infection carriers are now known to be a common feature of many infectious diseases, especially viral diseases such as polio , herpes simplex , HIV/AIDS , and hepatitis C . As a specific example, all doctors and virologists agree that poliovirus causes paralysis in just a few infected subjects. [ citation needed ]
The second postulate may also be suspended for certain microorganisms or entities that cannot (at the present time) be grown in pure culture. [5] Viruses also require host cells to grow and reproduce and therefore cannot be grown in pure cultures.
The third postulate specifies "should" not "must" because, as Koch himself proved in regard to both tuberculosis and cholera, [6] not all organisms exposed to an infectious agent will acquire the infection. Noninfection may be due to such factors as general health and proper immune functioning; acquired immunity from previous exposure or vaccination; or genetic immunity, as with the resistance to malaria conferred by possessing at least one sickle cell allele . [ citation needed ]
There are a few other exceptions to Koch's postulates. A single pathogen can cause several disease conditions. Additionally, a single disease condition can be caused by several different microorganisms. Some pathogens cannot be cultured in the lab, and some pathogens only cause disease in humans. [7]
In summary, an infectious agent can be considered to be a sufficient cause for a disease if it satisfies Koch's postulates. Failing that, the postulates suggest that the infectious agent is a necessary, but insufficient, cause for a disease. [ citation needed ]
Koch's postulates were developed in the 19th century as general guidelines to identify pathogens that could be isolated with the techniques of the day. [8] Even in Koch's time, it was recognized that some infectious agents were clearly responsible for disease even though they did not fulfill all of the postulates. [4] [6] Attempts to apply Koch's postulates rigidly to the diagnosis of viral diseases in the late 19th century, at a time when viruses could not be seen or isolated in culture, may have impeded the early development of the field of virology . [9] [10] Koch's postulates have been recognized as largely obsolete by epidemiologists since the 1950s, [11] [3] so, while retaining historical importance and continuing to inform the approach to microbiologic diagnosis, they are not routinely used to demonstrate causality.
Koch's postulates have also influenced scientists who examine microbial pathogenesis from a molecular point of view. In 1988, a molecular version of Koch's postulates was developed to guide the identification of microbial genes encoding virulence factors. [12]
That HIV causes AIDS does not follow from Koch's postulates, [13] which may have supported HIV/AIDS denialism . The role of oncoviruses in causing some cancers also does not follow Koch's postulates. [14]
New discoveries of methods of infections as a result of Koch and many others' work have shown that some diseases and conditions are not always caused by a single microbe species. According to a 2019 study by Todd and Peters, a newly discovered interaction between the pathogen Staphylococcus aureus and "fungal opportunist" Candida albicans is being considered a co-infection that is found in the bodies of sick patients who suffer from different conditions. This kind of synergism was found to be lethal in a separate study conducted by Carlson on mice. When mice were infected with one pathogen independently of the other, sickness resulted but the mice were able to recover. When infected with both pathogens together, the mice had a near-100% mortality rate, showing that some pathogens cannot be as easily isolated or may need extra techniques and steps that better prove causation of the disease. [15]
Koch's postulates have played an important role in microbiology , yet they have major limitations. For example, Koch was well aware in the case of cholera that the causal agent, Vibrio cholerae , could be found in both sick and healthy people, invalidating his first postulate. Furthermore, viral diseases were not yet discovered when Koch formulated his postulates, and there are many viruses that do not cause illness in all infected individuals, a requirement of the first postulate. Additionally, it was known through experimentation that Helicobacter pylori caused mild inflammation of the gastric lining when ingested. As evident as the inflammation was, it still did not immediately convince skeptics that H. pylori was associated with stomach ulcers . Eventually, skeptics were silenced when a newly developed antibiotic treatment eliminated the bacteria and ultimately cured the disease.
Koch's postulates are also of limited effectiveness when evaluating biofilms , Somni cells , and viruses. Cultivation of biofilms requires cultivation by molecular methods rather than traditional methods, and these alternative methods do not detect the cause of infection, which therefore interferes with the third postulate, that microorganisms should cause disease. [16] For example, Somni cells and viruses cannot be cultured. The Somni cells, also called sleeping cells, become dormant due to strain on the cell. This state of sleep prevents the cell from growing in the culture. [17] This is similar to how viruses cannot grow in axenic culture: viruses must be living to replicate, so the culture is not a suitable host. [18]
Byrd and Segre have proposed changes to the postulates to make them more accurate for today's world. Their revisions involve the third postulate: they disagree that a pathogen will always cause disease. Their first revision involves colonization resistance . Colonization resistance allows an organism to feed off of the host and protect it from pathogens that would have caused disease if the organism was not attached to the host. Their second revision is that a community of microbes could help inhibit pathogens even further, preventing the pathogen from spreading disease as it is supposed to. [19] Similar to Byrd and Segre, Rivers suggested revisions to Koch's postulates. He believed that, although the original postulates were made as a guide, they were actually an obstacle. Rivers wanted to show the link between viruses and diseases. Rivers' own postulates are: the virus must be connected to disease consistently; the outcome of experimentation must indicate that the virus is directly responsible for the disease. [18] Contradictions and occurrences such as these have led many to believe that a fifth postulate may be required. If accepted, this postulate would state that sufficient microbial data should allow scientists to treat, cure, or prevent the particular disease. [ citation needed ]
More recently, modern nucleic-acid-based microbial detection methods have made Koch's original postulates even less relevant. These methods enable the identification of microbes that are associated with a disease, but which cannot be cultured. Also, these methods are very sensitive, and can often detect very low levels of viruses in healthy people. [ citation needed ]
These new methods have led to revised versions of Koch's postulates. Fredricks and Relman have suggested a set of postulates for the novel field of microbial pathogenesis . [18] These modifications are still controversial in that they do not account well for established disease associations, such as papillomavirus and cervical cancer , nor do they take into account prion diseases , which have no nucleic acid sequences of their own. [ citation needed ]
In epidemiology , lagging (or exposure lagging ) means excluding the exposure in a time period before registration of an outcome. It may be motivated by that the actual outcome had actually occurred before the registration of it, and that the last exposure before registration did not contribute to the case. [1]
For example, when studying risk factors of cancer, the cancer process may have been triggered long before actual diagnosis of cancer, and that therefore any exposure to risk factors in the lag time between may be unimportant.
It can be used to mitigate protopathic bias , [2] that is, when a treatment for the first symptoms of a disease or other outcome appear to cause the outcome. Protopathic bias is a potential bias when there is a lag time from the first symptoms and start of treatment before actual diagnosis. [3]
Landscape epidemiology draws some of its roots from the field of landscape ecology . [1] Just as the discipline of landscape ecology is concerned with analyzing both pattern and process in ecosystems across time and space, landscape epidemiology can be used to analyze both risk patterns and environmental risk factors. This field emerges from the theory that most vectors, hosts and pathogens are commonly tied to the landscape as environmental determinants control their distribution and abundance. [2] In 1966, Evgeniy Pavlovsky introduced the concept of natural nidality or focality, defined by the idea that microscale disease foci are determined by the entire ecosystem. [3] With the recent availability of new computing technologies such as geographic information systems , remote sensing , statistical methods including spatial statistics and theories of landscape ecology , the concept of landscape epidemiology has been applied analytically to a variety of disease systems, including malaria , [4] hantavirus , Lyme disease and Chagas' disease . [5]
In medicine , a late effect is a condition that appears after the acute phase of an earlier, causal condition has run its course.  A late effect can be caused directly by the earlier condition, or indirectly by the treatment for the earlier condition.  Some late effects can occur decades later.  Historically, late effects have been very difficult to connect with their causes, but as survivorship and life span has increased and "follow up" has become standard practice, these connections are becoming established. A period, often very long, of health unaffected by both the initial and the late effect conditions distinguishes a late effect from a sequela or a complication . A code for such a condition was present in the ICD-9 but is no longer present in the ICD-10 .

This article about a disease , disorder, or medical condition is a stub . You can help Wikipedia by expanding it .
In epidemiology , particularly in the discussion of infectious disease dynamics (modeling), the latent period (also known as the latency period or the pre-infectious period ) is the time interval between when an individual or host is infected by a pathogen and when he or she becomes infectious, i.e. capable of transmitting pathogens to other susceptible individuals . [1] [2] [3]
To understand the spreading dynamics of an infectious disease or an epidemic , three important time periods should be carefully distinguished: incubation period , pre-infectious or latent period and infectious period . [2] Two other relevant and important time period concepts are generation time and serial interval .
The infection of a disease begins when a pathogenic (disease-causing) infectious agent, or a pathogen, is successfully transmitted from one host to another. [3] After pathogens enter the body of the new host, they take a period of time to overcome or evade the immune response of the body and to multiply or replicate after having traveled to their favored sites within the host’s body. [3] When the pathogens become sufficiently numerous and toxic to cause damage to the body, the host begins to display symptoms of a clinical disease (i.e. the host becomes symptomatic). [3] The time interval from the time of invasion by an infectious pathogen to the time of onset (first appearance) of symptoms of the disease in question is called the incubation period . [3] After the incubation period is over, the host enters the symptomatic period . Moreover, at a certain point in time after infection, the host becomes capable of transmitting pathogens to others, i.e. he or she becomes infectious or communicable. [3] Depending on the disease, the host individual may or may not be infectious during the incubation period. [3] The incubation period is important in the dynamics of disease transmission because it determines the time of case detection relative to the time of infection. [1] This helps in the evaluation of the outcomes of control measures based on symptomatic surveillance. The incubation period is also useful to count the number of infected people. [1]
The period from the time of infection to the time of becoming infectious is called the pre-infectious period or the latent period . During the pre-infectious or latent period, a host may or may not show symptoms (i.e. the incubation period may or may not be over), but in both cases, the host is not capable of infecting other hosts i.e. transmitting pathogens to other hosts. The latent period, rather than the incubation period, has more influence on the spreading dynamics of an infectious disease or epidemic. [1]
The time interval during which the host is infectious, i.e. the pathogens can be transmitted directly or indirectly from the infected host to another individual, is called the infectious period (or the period of communicability ), defined as the period from the end of the pre-infectious period or the latent period until the time when the host can no longer transmit the infection to other individuals. During the infectious period, a host may or may not show symptoms, but he or she is capable of infecting other individuals. The duration of the infectious period depends on the ability of the infected host individual to mount an immune response. [2]
In some cases, the pre-infectious or latent period and the incubation period coincide and are mostly of the same duration. In this case, the infected individual becomes infectious at around the same time he starts showing symptoms. In certain other infectious diseases such as smallpox or SARS , [4] the host becomes infectious after the onset of symptoms. In this case, the latent period is longer than the incubation period. In these two cases, the disease can be effectively controlled using symptomatic surveillance. [1] A related term is the shedding period , which is defined as the period during which a host or patient excretes pathogens through saliva, urine, feces or other bodily fluids.
However, for some infectious diseases, the symptoms of the clinical disease may appear after the host becomes infectious. In this case, the pre-infectious or latent period has a shorter duration than the incubation period, the infectious period begins before the end of the incubation period and the host can infect others for some time without showing any noticeable symptoms. This early or mild stage of infection whose symptoms stay below the level of clinical detection is called subclinical infection [5] and the individual concerned is called an asymptomatic carrier of the disease. For example, in HIV/AIDS, the incubation period lasts years longer than the latent period. So an HIV infected individual can show no symptoms and unwittingly infect other susceptible individuals for many years. In COVID-19 , the infectious period begins approximately 2 days before the onset of symptoms and 44% of the secondary infections may happen during this pre-symptomatic stage. [4] In these kinds of cases with a significant number of pre-symptomatic (asymptomatic) transmissions, symptomatic surveillance-based disease control measures (such as isolation, contact tracing, enhanced hygiene, etc.) are likely to have their effectiveness reduced, because a significant portion of the transmission may take place before the onset of symptoms and this has to be taken into account when designing control measures. [1] [4]
The infectious period is a very important element in the infectious disease spreading dynamics. If the infectious period is long, then the measure of secondary infections (represented by the basic reproduction number , R 0 ) will generally be larger, regardless of the infectiousness of the disease. For example, even though HIV/AIDS has a very low transmission potential per sexual act, its basic reproduction number is still very high because of its unusually long infectious period spanning many years. [1] From the viewpoint of controlling an epidemic, the goal is to reduce the effective infectious period either by treatment or by isolating the patient from the community. Sometimes a treatment can paradoxically increase the effective infectious period by preventing death through supportive care and thereby increasing the probability of infection of other individuals. [1]
The generation time (or generation interval ) of an infectious disease is the time interval between the beginning of infection in an individual to the time that person transmits to another individual. The generation time specifies how fast infections are spreading in the community with the passing of each generation. [1] In contrast, the effective reproductive number determines in what number the infections are spreading in the community with the passing of each generation. The latent period and the infectious period helps determine the generation time of an infection. The mean generation time is equal to the sum of the mean latent period and one-half of the mean infectious period, given that infectiousness is evenly distributed across the infectious period. [1]
Since the precise moment of infection is very difficult and almost impossible to detect, the generation time is not properly observable for two successive hosts. Generally, in infectious disease statistics, the onset of clinical symptoms for all the hosts are reported. For two successive generations (or cases or hosts) in a chain of infection, the serial interval is defined as the period of time between the onset of clinical symptoms in the first host and the onset of analogous clinical symptoms in the second host. Just like the generation time, the length of the serial interval depends on the lengths of the latent period, the infectious period and the incubation period. Therefore the serial interval is often used as a proxy measure to estimate the generation time. [1] [2]
Outside of epidemiology, the usage of the term "latent" is often confusing [ by whom? ] because it is widely used [ citation needed ] by educated speakers for and defined in dictionaries as being the time interval between infection by a pathogen and the onset of symptoms [6] [7] [8] or for the sometimes long time period in which a virus or other pathogen is present in a body without causing disease. [9] The term "latent period" is widely used even in medicine for the period between the onset of a non-infectious disease such as cancer and the onset of symptoms. [ citation needed ] So doctors and medical journals speak of latent tumors, for example. Even more confusing is that the term is used even in medicine when talking about infectious diseases in referring to asymptomatic periods of, for example, syphilis that include both infectious and non infectious phases. [ citation needed ]
Lead time is the length of time between the detection of a disease (usually based on new, experimental criteria) and its usual clinical presentation and diagnosis (based on traditional criteria). [1] It is the time between early diagnosis with screening and the time in which diagnosis would have been made without screening. [1] It is an important factor when evaluating the effectiveness of a specific test. [2]
By screening, the intention is to diagnose a disease earlier than it would be without screening. Without screening, the disease may be discovered later, when symptoms appear. [1]
Early diagnosis by screening may not prolong the life of someone but just determine the propensity of the person to a disease or medical condition such as by DNA testing. [1] No additional life span has been gained and the patient may even be subject to added anxiety as the patient must live for longer with knowledge of the disease. For example, the genetic disorder Huntington's disease is diagnosed when symptoms appear at around 50, and the person dies at around 65. The typical patient, therefore, lives about 15 years after diagnosis. A genetic test at birth makes it possible to diagnose this disorder earlier. If this newborn baby dies at around 65, the person will have "survived" 65 years after diagnosis, without having actually lived any longer than those diagnosed without DNA detection.
Raw statistics can make screening appear to increase survival time (called lead time). If the person dies at a time in life that previously has been the usual course of the disease than when detected by early screening, the person's life has not been prolonged.
Detection by advanced screening does not always mean prolonged survival. [1]
Lead time bias is a type of selection bias , and can affect interpretation of the five-year survival rate , effectively making it appear that people survive longer with cancer even in cases where the course of cancer is the same as in those who were diagnosed later. [3]
Length time bias (or length bias ) is an overestimation of survival duration due to the relative excess of cases detected that are asymptomatically slowly progressing, while fast progressing cases are detected after giving symptoms. It is a form of selection bias , a statistical distortion of results that can lead to incorrect conclusions about factual data.  While the raw data of a study may itself be objective and independent, statistical analysis requires parametric inputs of frequency and length of time, which is some arbitrary choice of design originating in the statistician and not the data.  If points are chosen randomly in an attempt to prevent observer selection bias, this choice of method itself amounts to a grand bias, because longer or more complex intervals increase possibilities for false detection of significance .
Length time bias is often discussed in the context of the benefits of cancer screening, and it can lead to the perception that screening leads to better outcomes when in reality it has no effect. Fast-growing tumors generally have a shorter asymptomatic phase than slower-growing tumors. Thus, there is a shorter period of time during which the cancer is present in the body (and so might be detected by screening) but not yet large enough to cause symptoms, that would cause the patient to seek medical care and be diagnosed without screening.
As a result, if the same number of slow-growing and fast-growing tumors appear in a year, the screening test detects more slow-growers than fast-growers. If the slow growing tumors are less likely to be fatal than the fast growers, the people whose cancer is detected by screening do better, on average, than the people whose tumors are detected from symptoms (or at autopsy) even if there is no real benefit to catching the cancer earlier. That can give the impression that detecting cancers by screening causes cancers to be less dangerous even if less dangerous cancers are simply more likely to be detected by screening. [1]
Worldwide, two to three million people are estimated to be permanently disabled because of leprosy . [4] India has the greatest number of cases, with Brazil second and Indonesia third.
In 1999, the world incidence of Hansen's disease was estimated to be 640,000. In 2000, 738,284 new cases were identified. [5] In 2000, the World Health Organization (WHO) listed 91 countries in which Hansen's disease is endemic . India, Myanmar and Nepal contained 70% of cases. India reports over 50% of the world's leprosy cases. [6] In 2002, 763,917 new cases were detected worldwide, and in that year the WHO listed India, Brazil, Madagascar, Mozambique, Tanzania and Nepal as having 90% of Hansen's disease cases.
According to recent figures from the WHO, 208,619 new cases of leprosy were reported in 2018 from 127 countries. [7] A total of 16,000 new child cases were detected in 2018.
In the United States, Hansen's disease is tracked by the Centers for Disease Control and Prevention (CDC), with a total of 92 cases being reported in 2002. [8] Although the number of cases worldwide continues to fall, pockets of high prevalence continue in certain areas such as Brazil, South Asia (India, Nepal ), some parts of Africa ( Tanzania , Madagascar , Mozambique ) and the western Pacific.
At highest risk are those living in endemic areas with poor conditions such as inadequate bedding, contaminated water and insufficient diet, or other diseases (such as HIV ) that compromise immune function. Recent research suggests that there is a defect in cell-mediated immunity that causes susceptibility to the disease. Less than ten percent of the world's population is actually capable of acquiring the disease. [9]
The region of DNA responsible for this variability is also involved in Parkinson disease , [ citation needed ] giving rise to current speculation that the two disorders may be linked in some way at the biochemical level. In addition, men are twice as likely to contract leprosy as women. [ citation needed ] According to The Leprosy Mission Canada, most people—about 95% of the population—are naturally immune. [9]
Although the number of new leprosy cases occurring each year is important as a measure of transmission, it is difficult to measure in leprosy due to its long incubation period, delays in diagnosis after onset of the disease and the lack of laboratory tools to detect leprosy in its very early stages. Instead, the registered prevalence is used. Registered prevalence is a useful proxy indicator of the disease burden as it reflects the number of active leprosy cases diagnosed with the disease and receiving treatment with MDT at a given point in time. The prevalence rate is defined as the number of cases registered for MDT treatment among the population in which the cases have occurred, again at a given point in time. [10]
New case detection is another indicator of the disease that is usually reported by countries on an annual basis. It includes cases diagnosed with onset of disease in the year in question (true incidence) and a large proportion of cases with onset in previous years (termed a backlog prevalence of undetected cases).
Endemic countries also report the number of new cases with established disabilities at the time of detection, as an indicator of the backlog prevalence. Determination of the time of onset of the disease is generally unreliable, is very labor-intensive and is seldom done in recording these statistics.
As reported to WHO by 115 countries and territories in 2006, and published in the Weekly Epidemiological Record, the global registered prevalence of leprosy at the beginning of the year was 219,826 cases. [11] New-case detection during the previous year (2005 - the last year for which full country information is available) was 296,499. The reason for the annual detections being higher than the prevalence at the end of the year can be explained by the fact that a proportion of new cases complete their treatment within the year and, therefore, no longer remain on the registers. The global detection of new cases continues to show a sharp decline, falling by 110,000 cases (27%) during 2005 compared with the previous year.
Table 1 shows that global annual detection has been declining since 2001. The African region reported an 8.7% decline in the number of new cases compared with 2004. The comparable figure for the Americas was 20.1%, for South-East Asia 32%, and for the Eastern Mediterranean 7.6%. The Western Pacific area, however, showed a 14.8% increase during the same period.
Table 2 shows the leprosy situation in the four major countries that have yet to achieve the goal of elimination at the national level. Elimination is defined as a prevalence of less than 1 case per 10,000 population. Madagascar reached elimination at the national level in September 2006. Nepal detection reported from mid-November 2004 to mid-November 2005. D.R. Congo officially reported to WHO in 2008 that it had reached elimination by the end of 2007 at the national level.
In 2013, there were 550 cases, of which 277 were considered as new cases. Between 350 and 400 cases of leprosy are diagnosed every year. [12] In 1926 the law 11,359 ordered compulsory quarantine for the leper, that law was abolished by the law 22,964 in 1983, that ordered compulsory quarantine for the leper only if they refuse medical indications and involve some risk for the healthy population.
Five leper colonies were built:
Only the Baldomero Sommer continues serving as a leper colony.
Leprosy is a serious public health problem in Brazil, between 2001 and 2013 over 500,000 cases were reported and in 2015 alone over 25,000 cases were diagnosed. [13]
The People's Republic of China has many leprosy recovered patients who have been isolated from the rest of society. [14] In the 1950s the Chinese Communist government created "Recovered Villages" in rural remote mountaintops for the recovered patients. Although leprosy is now curable with the advent of the multi-drug treatment, the villagers remain because they have been stigmatized by the outside world. [14] Health NGOs such as Joy in Action have arisen in China to especially focus on improving the conditions of "Recovered Villages". The number of leprosy cases in China has been showing a steady decline over recent years, with a prevalence of 2697 registered cases in 2017 which is a 55.3% reduction compared to that in 2010. [15]
British India enacted the Leprosy Act of 1898 which institutionalized those affected and segregated them by sex to prevent reproduction. The Act was difficult to enforce but was repealed in 1983 only after MDT had become widely available. In 1983, the National Leprosy Elimination Programme, previously the National Leprosy Control Programme, changed its methods from surveillance to the treatment of people with leprosy. India reported a far larger decline in leprosy cases than any other country - from 473,658 new cases in 2002 to 161,457 in 2005. [16] According to WHO 16 million people worldwide were cured of leprosy since the past 20 years. India has estimated three million people with disability or health issues stemming from leprosy. India announced that leprosy had been “eliminated as a public health problem,” meaning that there would be fewer than one new case per 10,000 people (as defined by the WHO). Reported new cases exceed 125,000 per year (60% of the world total). 135,485 new leprosy cases were detected in India in 2017. [17] [18]
Malaysia was announced to be eliminated of leprosy by WHO in 1994, which signifies a reduction in the prevalence rate of the disease to less than 1 case per 10,000 people. However, it was reported that there is a rise in incidence across the country over recent years, reaching 1.02 cases per 10,000 people in 2014. [19]
The Hospital-Colónia Rovisco Pais (the Rovisco Pais Hospital-Colony) was founded in Portugal in 1947 as a national center for the treatment of leprosy. It was renamed in 2007 as the Centro de Medicina de Reabilitação da Região Centro-Rovisco Pais. It still retains a leprosy service in which 25 ex patients live. Between 1988 and 2003 102 patients were treated in Portugal for leprosy. [20]
The Sanatorio de Fontilles (Fontilles Sanatorium) in Spain was founded in 1902 and admitted its first patient in 1909. In 2002, the Sanatorio had 68 in-patients in the Sanatorium, and more than 150 receiving out-patient treatment. A small number of cases continue to be reported. [21]
Two indigenous cases were reported from Greece in 2009. [22]
One case was reported in France in 2009 [23]
Leprosy was almost eradicated in most of Europe by 1700 but sometime after 1850 leprosy was re introduced into East Prussia by Lithuanian rural workers immigrating from the Russian empire. The first leprosarium was founded in 1899 in Memel (now Klaipėda in Lithuania). Legislation was introduced in 1900 and 1904 requiring patients to be isolated and not allowed to work with others.
The last confirmed case of Leprosy being transmitted within the UK was in 1953. [24] Between 2003 & 2012, an average of 139 cases/year of leprosy were diagnosed (& notified) in the UK, none of which are believed to have been acquired within the UK. [24] The UK's national referral service for leprosy is run by Prof. Diana Lockwood, the UK's only Leprologist, at the Hospital for Tropical Diseases , London. [25]
The first documented case of leprosy ( erga corpore morbi leprae ) in Malta in a Gozitan woman (Garita Xejbais) was in 1492 but it is certain that it was present on the island before this time. [26] The next recorded case was in 1630 in a Dominican friar. A report in 1687 recorded five cases. A further three cases were reported in 1808. Between 1839 and 1858 an additional seven cases were recorded. In 1890 a population survey recorded a total of 69 cases. A later survey in 1957 identified 151 people infected with leprosy. [ citation needed ]
In June 1972 an eradication programme was started. The project was based on the work of Enno Freerksen, Director of the Borsal Institute in Hamburg . Dr Freerksen's earlier trial had used rifampacin, izoniazid, dapsone and prothionamide. The Malta project used rifampacin, dapsone and clofamazine. The project formally concluded in 1999 having treated about 300 patients. [ citation needed ]
The last leper colony in Europe is at Tichileşti , Romania. Until 1991 patients were not allowed to leave the colony. At this colony patients get food, a place to sleep, clothes and medical attention. Some live in long pavilions and others in houses with vegetable and flower gardens. There are two churches in the colony - Orthodox and Baptist - and a farm where the colony grows its own corn.
There were cases of leprosy in Atlantic Canada at the end of the nineteenth century, beginning in 1815. [27] The patients were first housed on Sheldrake Island in the Miramichi River and later transferred to Tracadie .  Catholic nuns (the religieuses hospitalières de Saint-Joseph , RHSJ) came to take care of the sick.  They opened the first French-language hospital in New Brunswick and many more followed.  Many hospitals opened by the RHSJ nuns are still in use today.  The last hospital to house lepers in Tracadie was demolished in 1991.  Its lazaretto section had been closed since 1965.  In a century of existence, it had housed not only Acadian victims of the disease, but people from all over Canada as well as sick immigrants from Iceland, Russia and China, among other nations. [28]
Cape Breton Island also suffered an outbreak, in the Lake Ainslie region.  Nine people were affected, but it died out soon after 1882.  Another outbreak of twenty cases occurred in the Lake O'Law region around 1852. [29]
In the United States, the first definite reference to the disease was in Florida in 1758. In 2004, there were 131 total cases of the disease in the United States.  Of the 131 cases, two-thirds were male.  Also out of the 131 cases, 25 (19%) were of individuals who were born in the country.  Mexico (18.3%), Micronesia (11.5%), Brazil (9.2%), and the Philippines (7.6%) were the next leading countries where those with the disease were originally born. A total of 20 cases were found to be white, not of Hispanic origin. As of October, 2005, 3,604 patients on the United States registry were currently receiving care. [30] In 2018 there are about 5,000 people who no longer have leprosy but have long-term complications of disease and continue to receive care. [31] The disease is tracked by the Centers for Disease Control and Prevention (CDC), with a total of 166 new cases reported in the US in 2005.  Most (100 or 60%) of these new cases were reported in California, Louisiana, Massachusetts, New York, and Texas. [32]
Media related to Epidemiology of leprosy at Wikimedia Commons
Lethality (also called deadliness or perniciousness ) is how capable something is of causing death . Most often it is used when referring to diseases , chemical weapons , biological weapons , or their toxic chemical components.  The use of this term denotes the ability of these weapons to kill, but also the possibility that they may not kill.  Reasons for the lethality of a weapon to be inconsistent, or expressed by percentage, can be as varied as minimized exposure to the weapon, previous exposure to the weapon minimizing susceptibility, degradation of the weapon over time and/or distance, and incorrect deployment of a multi-component weapon.
This term can also refer to the after-effects of weapon use, such as nuclear fallout , which has highest lethality nearest the deployment site, and in proportion to the subject's size and nature; e.g. a child or small animal.
Lethality can also refer to the after-effects of a chemical explosion.  A lethality curve can be developed for process safety reasons to protect people and equipment.  The impact is typically greatest closest to the explosion site and lessens to the outskirts of the impact zone.  Pressure, toxicity and location affect the lethality.
Lethality is also a term used by microbiologists and food scientists as a measure of the ability of a process to destroy bacteria. Lethality may be determined by enumeration of survivors after incremental exposures.
The life course approach , also known as the life course perspective or life course theory , refers to an approach developed in the 1960s for analyzing people's lives within structural , social , and cultural contexts. The origins of this approach can be traced back to pioneering studies of the 1920s such as Thomas' and Znaniecki's "The Polish Peasant in Europe and America" and Mannheim's essay on the "Problem of generations". [1]
The life course approach examines an individual's life history and investigates, for example, how early events influenced future decisions and events such as marriage and divorce, [2] engagement in crime, or disease incidence . [3] The primary factor promoting standardization of the life course was improvement in mortality rates brought about by the management of contagious and infectious diseases such as smallpox. [4] A life course is defined as "a sequence of socially defined events and roles that the individual enacts over time". [5] In particular, the approach focuses on the connection between individuals and the historical and socioeconomic context in which these individuals lived. [1] [6] The method encompasses observations including history , sociology , demography , developmental psychology , biology , public health and economics . So far, empirical research from a life course perspective has not resulted in the development of a formal theory . [7]
Life course theory, more commonly termed the life course perspective, refers to a multidisciplinary paradigm for the study of people's lives, structural contexts, and social change. This approach encompasses ideas and observations from an array of disciplines, notably history, sociology, demography, developmental psychology, biology, and economics. In particular, it directs attention to the powerful connection between individual lives and the historical and socioeconomic context in which these lives unfold. Glen H. Elder, Jr. theorized the life course as based on five key principles: life-span development, human agency, historical time and geographic place, timing of decisions, and linked lives. As a concept, a life course is defined as "a sequence of socially defined events and roles that the individual enacts over time" (Giele and Elder 1998, p. 22). These events and roles do not necessarily proceed in a given sequence, but rather constitute the sum total of the person's actual experience. Thus the concept of life course implies age-differentiated social phenomena distinct from uniform life-cycle stages and the life span. Life span refers to duration of life and characteristics that are closely related to age but that vary little across time and place.
In contrast, the life course perspective elaborates the importance of time, context, process, and meaning on human development and family life (Bengtson and Allen 1993). The family is perceived as a micro social group within a macro social context—a "collection of individuals with shared history who interact within ever-changing social contexts across ever increasing time and space" (Bengtson and Allen 1993, p. 470). Aging and developmental change, therefore, are continuous processes that are experienced throughout life. As such, the life course reflects the intersection of social and historical factors with personal biography and development within which the study of family life and social change can ensue (Elder 1985; Hareven 1996).
Life course theory also has moved in a constructionist direction. Rather than taking time, sequence, and linearity for granted, in their book Constructing the Life Course , Jaber F. Gubrium and James A. Holstein (2000) take their point of departure from accounts of experience through time.  This shifts the figure and ground of experience and its stories, foregrounding how time, sequence, linearity, and related concepts are used in everyday life. It presents a radical turn in understanding experience through time, moving well beyond the notion of a multidisciplinary paradigm, providing an altogether different paradigm from traditional time-centered approaches.  Rather than concepts of time being the principal building blocks of propositions, concepts of time are analytically bracketed and become focal topics of research and constructive understanding.
The life course approach has been applied to topics such as  the occupational health of immigrants, [8] and retirement age. [9] It has also become increasingly important in other areas such as in the role of childhood experiences affecting the behaviour of students later in life [10] or physical activity in old age. [11]
The life-years lost (or years of lost life ) [1] is a unit to measure the number of expected years of human life lost following a unexpected event, such as death by illness, crime or war.
Life-years lost is a flexible measure which have been used to measure the effects of overall mortality of non-communicable diseases, [1] drug misuse [1] and suicide, [1] epidemics (example COVID-19 pandemic ), [1] [2] wars, and natural disasters such as earthquakes . Life-years lost are based on both the number of deaths and the age of those who died. It estimates the number of years that those who died would have lived if they did not met their accidental a deadly fate. Higher YLLs can be due to larger numbers of death, few sharply younger deaths or some combination of the two.
There were 795 million undernourished people in the world in 2014, a decrease of 216 million since 1990, [2] despite the fact that the world already produces enough food to feed everyone—7 billion people—and could feed more than that—12 billion people. [3]
Reducing malnutrition is key part of Sustainable Development Goal 2 ( SDG2 ) "Zero hunger" with a malnutrition target alongside reducing under nutrition and  stunted child growth. [4] Because of the Sustainable development goals, various UN agencies are responsible for measuring and coordinating action to reduce malnutrtion. According to the World Food Programme (WFP) 135 million suffer from acute hunger, [5] largely due to manmade conflicts, climate changes, and economic downturns. COVID-19 could double the number of people at risk of suffering acute hunger by the end of 2020. [6]
The number of undernourished people (million) in 2010–2012 and 2014–2016 (projected).
According to the FAO , these countries had 5 million or more undernourished people in 2001–2003 and in 2005–2007. [10]
Note: This table measures "undernourishment", as defined by FAO , and represents the number of people consuming (on average for years 2010 to 2012) less than the minimum amount of food energy (measured in kilocalories per capita per day) necessary for the average person to stay in good health while performing light physical activity. It is a conservative indicator that does not take into account the extra needs of people performing extraneous physical activity, nor seasonal variations in food consumption or other sources of variability such as inter-individual differences in energy requirements. Malnutrition and undernourishment are cumulative or average situations, and not the work of a single day's food intake (or lack thereof). This table does not represent the number of people who "went to bed hungry today."
The below is a list of countries by percentage of population with undernourishment , as defined by the United Nations World Food Programme and the UN Food and Agriculture Organization in its "The State of Food Insecurity in the World" 2009 report.
Malnutrition rates in Iraq had risen from 19% before the US-led invasion to a national average of 28% four years later. [11] By 2010, according to the UN Food and Agriculture Organization, only 8% were malnourished. (See data above.)
According to the Global Hunger Index , South Asia (also known as the Indian Subcontinent ) has the highest child malnutrition rate of world's regions. [12] India , a largely vegetarian country and second largest country in the world by population, contributes most number in malnutrition in the region. The 2006 report mentioned that "the low status of women in South Asian countries and their lack of nutritional knowledge are important determinants of high prevalence of underweight children in the region" and was concerned that South Asia has "inadequate feeding and caring practices for young children". [13]
30% children of India are underweight, [14] one of the highest rates in the world and nearly double the rate of Sub-Saharan Africa . [15]
Research on overcoming persistent under-nutrition published by the Institute of Development Studies , argues that the co-existence of India as an 'economic powerhouse' and home to one-third of the world's under-nourished children reflects a failure of the governance of nutrition: "A poor capacity to deliver the right services at the right time to the right populations, an inability to respond to citizens' needs and weak accountability are all features of weak nutrition governance." [16] The research suggests that to make under-nutrition history in India the governance of nutrition needs to be strengthened and new research needs to focus on the politics and governance of nutrition. At the current rate of progress the MDG1 target for nutrition will only be reached in 2042 with severe consequences for human wellbeing and economic growth. [16]
According to the United States Department of Agriculture in 2015, 50 million Americans experienced food insecurity in 2009, including 17 million children. This represents nearly one in four American children. [17] [18] [19]
Although the United States Department of Agriculture reported in 2012 that an estimated 85.5 percent of households in the country are food secure, millions of people in America struggle with the threat of hunger or experience hunger on a daily basis. [20] The USDA defines food security as the economic condition of a household wherein which there is reliable access to a sufficient amount of food so all household members can lead a healthy productive life. [21] Hunger is most commonly related to poverty since a lack of food helps perpetuate the cycle of poverty . Most obviously, when individuals live in poverty they lack the financial resources to purchase food or pay for unexpected events, such as a medical emergency. When such emergencies arise, families are forced to cut back on food spending so they can meet the financial demands of the unexpected emergency. [22] There is not one single cause of hunger but rather a complex interconnected web of various factors. Some of the most vulnerable populations to hunger are the elderly, children, people from a low socioeconomic status, and minority groups; however, hunger's impact is not limited to these individuals.
The largest nonprofit food relief organization in the United States, Feeding America , feeds 46.5 million citizens a year to address the nation's food insecurity issue. [21] This equates to one in seven Americans requiring their aid in a given year. An organization that focuses on providing food for the elderly population is Meals on Wheels , which is a nonprofit that delivers meals to seniors' homes. The government also works towards providing relief through programs such as the Supplemental Nutrition Assistance Program (SNAP) which was formerly known to the public as Food Stamps. Another well known government program is the National School Lunch Program (NSLP) which provides free or reduced lunches to students who qualify for the program.
The number of Americans suffering from hunger rose after the 2008 financial crisis, with children and working adults now making up a large proportion of those affected. In 2012, Gleaners Indiana Food bank reported that there were now 50 million Americans struggling with food insecurity (about 1 in 6 of the population), and that the number of folks seeking help from food banks had increased by 46% since 2005. [23] According to a 2012 study by UCLA Center for Health Policy Research , even married couples who both work but have low incomes sometimes require the aid of food banks. [24] [25]
Childhood malnutrition is generally thought of as being limited to developing countries, but although most malnutrition occurs there, it is also an ongoing presence in developed nations. For example, in the United States of America, one out of every six children is at risk of hunger. [ citation needed ] A study, based on 2005–2007 data from the U.S. Census Bureau and the Agriculture Department , shows that an estimated 3.5 million children under the age of five are at risk of hunger in the United States . [26]
In developed countries, this persistent hunger problem is not due to lack of food or food programs, but is largely due to an underutilization of existing programs designed to address the issue, such as food stamps or school meals . Many citizens of rich countries such as the United States of America attach stigmas to food programs or otherwise discourage their use. In the USA, only 60% of those eligible for the food stamp program actually receive benefits. [27]
The U.S. Department of Agriculture reported that in 2003, 1 out of 200 U.S. households with children were so severely food insecure that any of the children went hungry at least once during the year. A substantially larger proportion of these same households (3.8 percent) had adult members who were hungry at least one day during the year because of their households' inability to afford enough food. [28]
According to World Vision there are 257 million people in Africa who are experiencing malnutrition. [29] This is around 20% of the entire population of Africa. [29] The regions in Africa with the highest rates of malnutrition are the Sub-Saharan region and parts of southern Africa. [29] In the Sub-Saharan region, the countries that have the highest rates include, but are not limited to South Sudan, Sudan, Central African Republic, and Chad. [30] In this region there are 237 million people who are experiencing hunger [30] and according to Action Against Hunger, there are 319 million people without a reliable source of drinking water. [31] In the Southern region of Africa, the countries that have the highest rates include, but are not limited to Mozambique, Zimbabwe, Zambia, and Angola. [30] In this region there are 41 million people who are food insecure and 9 million who are in a food crisis and need immediate assistance with food. [30]
There are many factors that contribute to malnutrition in Africa. [32] There are environmental factors such as degradation of land and unexpected weather changes. [32] The changes in weather such as droughts and storms, impact their food and water supply. [32] Another factor that contributes to malnutrition is conflict. [32] Conflict can lead to uncertainty in resources, which puts them at a higher risk of malnutrition. [32] In addition, the areas in Africa with the highest rates of malnutrition also experience poverty which impact and limit the supply of food and necessary services. [32] For example, some experience limited access to health services, sanitation, clean water, consistent food supply. [32] Not only do these things directly contribute to malnutrition, but they can also lead to illnesses such as malaria and water-borne disease. [32]
The use of epidemiological tools in health care management can be described as managerial epidemiology . Several formal definitions have been proposed for managerial epidemiology. These include:
The potential value of epidemiology in health care management has long been recognized. [4] [5] [6] Academics were encouraging use of epidemiological methods in health care management for quality improvement and planning before the term ‘managerial epidemiology’ was coined. (See for example Rohrer 1989. [7] )  Epidemiology became a required subject in some health care management programs and textbooks were written for those courses.  Managerial epidemiology might be considered a type of health services research , since it involves the study of health services.
After almost 40 years of research, a handful of researchers provided examples of using managerial epidemiology and the importance for healthcare managers to use the practice. However, the perspectives of healthcare leaders on the use of managerial epidemiology were never studied until 2020.  In 2020, a study was conducted to explore the adoption of managerial epidemiology by ambulatory healthcare leaders across the United States (See Schenning 2020 [8] [9] ).  The adoption was found to be poor; yet, critically important for improving overall health system performance including the triple aim and impacting population health.  From the findings, Dr. Schenning developed a framework for accelerating adoption of managerial epidemiology (See Schenning 2020).   She also discussed the importance of using managerial epidemiology for pandemic preparedness and response.

An important distinction can be drawn between population epidemiology and clinical epidemiology . If the US health care system had fully evolved in a direction that entailed management of care for populations rather than patients, then the concepts, methods and perspectives drawn from population epidemiology would have been ideal tools for use by managers. This indeed was anticipated by authors of textbooks on managerial epidemiology. (See Dever [10] ). In each cycle of health reform, the utility of epidemiology in planning medical services for populations was recognized.
However, the attention of most health care managers remains focused on patients rather than communities. Hospitals do not serve enrolled populations; they serve the patients who are treated in their beds and in the clinics.  Consequently, the tools and perspectives of clinical epidemiology may be as or more relevant to health care managers than those drawn from population epidemiology. Managers employing epidemiology in hospitals might not conduct many community surveys. Instead, they would extract clinical information from medical records to analyze variations in outcomes, complications, and services used.
However, healthcare leaders should use managerial epidemiology especially population epidemiology for population health strategies and overall system performance (See Schenning [9] ).  This is seen with the rise in addressing social determinants of health and further realized during the COVID-19 pandemic (See Schenning [9] ).
Methods drawn from clinical epidemiology that are employed in health care management to assess quality and cost include the following. [3]
Managerial epidemiology differs from clinical epidemiology in that it addresses the concerns of management. For example, clinical epidemiologists who seek to control hospital-acquired infections would not be engaged in managerial epidemiology unless they described the infections as quality indicators and proposed or tested organizational changes that might reduce infection rates. Another distinction between clinical epidemiology and managerial epidemiology is that while clinical epidemiologists test the efficacy of particular treatments, managers are concerned with how clinical outcomes differ between hospitals, bed sections, clinics, or programs. Information of this kind can lead to reallocation of resources so as to improve efficiency and effectiveness of the organization as a whole. [3]
Marginal structural models are a class of statistical models used for causal inference in epidemiology . [1] Such models handle the issue of time-dependent confounding in evaluation of the efficacy of interventions by inverse probability weighting for receipt of treatment. For instance, in the study of the effect of zidovudine in AIDS -related mortality, CD4 lymphocyte is used both for treatment indication, is influenced by treatment, and affects survival. Time-dependent confounders are typically highly prognostic of health outcomes and applied in dosing or indication for certain therapies, such as body weight or lab values such as alanine aminotransferase or bilirubin .
The first marginal structural models were introduced in 2000. The works of James Robins and Miguel Hernán provided an intuitive theory and an easy-to-implement software which made them popular for the analysis of longitudinal data. [2]
Mark and recapture is a method commonly used in ecology to estimate an animal population 's size where it is impractical to count every individual. [1] A portion of the population is captured, marked, and released. Later, another portion will be captured and the number of marked individuals within the sample is counted. Since the number of marked individuals within the second sample should be proportional to the number of marked individuals in the whole population, an estimate of the total population size can be obtained by dividing the number of marked individuals by the proportion of marked individuals in the second sample. Other names for this method, or closely related methods, include capture-recapture , capture-mark-recapture , mark-recapture , sight-resight , mark-release-recapture , multiple systems estimation , band recovery , the Petersen method , [2] and the Lincoln method .
Another major application for these methods is in epidemiology , [3] where they are used to estimate the completeness of ascertainment of disease registers. Typical applications include estimating the number of people needing particular services (i.e. services for children with learning disabilities , services for medically frail elderly living in the community), or with particular conditions (i.e. illegal drug addicts, people infected with HIV , etc.). [4]
Typically a researcher visits a study area and uses traps to capture a group of individuals alive. Each of these individuals is marked with a unique identifier (e.g., a numbered tag or band), and then is released unharmed back into the environment.  A mark-recapture method was first used for ecological study in 1896 by C.G. Johannes Petersen to estimate plaice, Pleuronectes platessa , populations. [5]
Sufficient time is allowed to pass for the marked individuals to redistribute themselves among the unmarked population. [5]
Next, the researcher returns and captures another sample of individuals. Some individuals in this second sample will have been marked during the initial visit and are now known as recaptures. [6] Other organisms captured during the second visit, will not have been captured during the first visit to the study area. These unmarked animals are usually given a tag or band during the second visit and then are released. [5]
Population size can be estimated from as few as two visits to the study area. Commonly, more than two visits are made, particularly if estimates of survival or movement are desired.  Regardless of the total number of visits, the researcher simply records the date of each capture of each individual.  The "capture histories" generated are analyzed mathematically to estimate population size, survival, or movement. [5]
When capturing and marking organisms, ecologists need to consider the welfare of the organisms. If the chosen identifier harms the organism, then its behavior might become irregular.
Let
A biologist wants to estimate the size of a population of turtles in a lake.  She captures 10 turtles on her first visit to the lake, and marks their backs with paint. A week later she returns to the lake and captures 15 turtles.  Five of these 15 turtles have paint on their backs, indicating that they are recaptured animals. This example is (n, K, k) = (10, 15, 5). The problem is to estimate N .
The Lincoln–Petersen method [7] (also known as the Petersen–Lincoln index [5] or Lincoln index ) can be used to estimate population size if only two visits are made to the study area.  This method assumes that the study population is "closed". In other words, the two visits to the study area are close enough in time so that no individuals die, are born, or move into or out of the study area between visits.  The model also assumes that no marks fall off animals between visits to the field site by the researcher, and that the researcher correctly records all marks.
Given those conditions, estimated population size is:
It is assumed [8] that all individuals have the same probability of being captured in the second sample, regardless of whether they were previously captured in the first sample (with only two samples, this assumption cannot be tested directly).
This implies that, in the second sample, the proportion of marked individuals that are caught ( k / K {\displaystyle k/K} ) should equal the proportion of the total population that is marked ( n / N {\displaystyle n/N} ). For example, if half of the marked individuals were recaptured, it would be assumed that half of the total population was included in the second sample.
In symbols,
A rearrangement of this gives
the formula used for the Lincoln–Petersen method. [8]
In the example (n, K, k) = (10, 15, 5) the Lincoln–Petersen method estimates that there are 30 turtles in the lake.
The Lincoln–Petersen estimator is asymptotically unbiased as sample size approaches infinity, but is biased at small sample sizes. [9] An alternative less biased estimator of population size is given by the Chapman estimator : [9]
The example (K, n, k) = (10, 15, 5) gives
Note that the answer provided by this equation must be truncated not rounded. Thus, the Chapman method estimates 28 turtles in the lake.
Surprisingly, Chapman's estimate was one conjecture from a range of possible estimators: "In practice, the whole number immediately less than ( K +1)( n +1)/( k +1) or even Kn /( k +1) will be the estimate. The above form is more convenient for mathematical purposes." [9] (see footnote, page 144). Chapman also found the estimator could have considerable negative bias for small Kn / N [9] (page 146), but was unconcerned because the estimated standard deviations were large for these cases.
An approximate 100 ( 1 − α ) % {\displaystyle 100(1-\alpha )\%} confidence interval for the population size N can be obtained as:
K + n − k − 0.5 + ( K − k + 0.5 ) ( n − k + 0.5 ) ( k + 0.5 ) exp ⁡ ( ± z α / 2 σ ^ 0.5 ) , {\displaystyle K+n-k-0.5+{\frac {(K-k+0.5)(n-k+0.5)}{(k+0.5)}}\exp(\pm z_{\alpha /2}{\hat {\sigma }}_{0.5}),}
where z α / 2 {\textstyle z_{\alpha /2}} corresponds to the 1 − α / 2 {\displaystyle 1-\alpha /2} quantile of a standard normal random variable, and
σ ^ 0.5 = 1 k + 0.5 + 1 K − k + 0.5 + 1 n − k + 0.5 + k + 0.5 ( n − k + 0.5 ) ( K − k + 0.5 ) . {\displaystyle {\hat {\sigma }}_{0.5}={\sqrt {{\frac {1}{k+0.5}}+{\frac {1}{K-k+0.5}}+{\frac {1}{n-k+0.5}}+{\frac {k+0.5}{(n-k+0.5)(K-k+0.5)}}}}.}
The example ( K, n, k ) = (10, 15, 5) gives the estimate N ≈ 30 with a 95% confidence interval of 22 to 65.
It has been shown that this confidence interval has actual coverage probabilities that are close to the nominal 100 ( 1 − α ) % {\displaystyle 100(1-\alpha )\%} level even for small populations and extreme capture probabilities (near to 0 or 1), in which cases other confidence intervals fail to achieve the nominal coverage levels. [10]
The mean value ± standard deviation is
where
A derivation is found here: Talk:Mark and recapture#Statistical treatment .
The example ( K, n, k ) = (10, 15, 5) gives the estimate N ≈ 42 ± 21.5
The capture probability refers to the probability of a detecting an individual animal or person of interest, [11] and has been used in both ecology and epidemiology for detecting animal or human diseases, [12] respectively.
The capture probability is often defined as a two-variable model, in which f is defined as the fraction of a finite resource devoted to detecting the animal or person of interest from a high risk sector of an animal or human population, and q is the frequency of time that the problem (e.g., an animal disease) occurs in the high-risk versus the low-risk sector. [13] For example, an application of the model in the 1920s was to detect typhoid carriers in London, who were either arriving from zones with high rates of tuberculosis (probability q that a passenger with the disease came from such an area, where q >0.5), or low rates (probability 1− q ). [14] It was posited that only 5 out of every 100 of the travelers could be detected, and 10 out of every 100 were from the high risk area. Then the capture probability P was defined as:
where the first term refers to the probability of detection (capture probability) in a high risk zone, and the latter term refers to the probability of detection in a low risk zone. Importantly, the formula can be re-written as a linear equation in terms of f :
Because this is a linear function, it follows that for certain versions of q for which the slope of this line (the first term multiplied by f ) is positive, all of the detection resource should be devoted to the high-risk population ( f should be set to 1 to maximize the capture probability), whereas for other value of q , for which the slope of the line is negative, all of the detection should be devoted to the low-risk population ( f should be set to 0. We can solve the above equation for the values of q for which the slope will be positive to determine the values for which f should be set to 1 to maximize the capture probability:
which simplifies to:
This is an example of linear optimization . [13] In more complex cases, where more than one resource f is devoted to more than two areas, multivariate optimization is often used, through the simplex algorithm or its derivatives.
The literature on the analysis of capture-recapture studies has blossomed since the early 1990s [ citation needed ] . There are very elaborate statistical models available for the analysis of these experiments. [15] A simple model which easily accommodates the three source, or the three visit study, is to fit a Poisson regression model. Sophisticated mark-recapture models can be fit with several packages for the Open Source R programming language .  These include "Spatially Explicit Capture-Recapture (secr)", [16] "Loglinear Models for Capture-Recapture Experiments (Rcapture)", [17] and "Mark-Recapture Distance Sampling (mrds)". [18] Such models can also be fit with specialized programs such as MARK [19] or M-SURGE . [20]
Other related methods which are often used include the Jolly–Seber model (used in open populations and for multiple census estimates) and Schnabel estimators [21] (described above as an expansion to the Lincoln–Petersen method for closed populations). These are described in detail by Sutherland. [22]
Modelling mark-recapture data is trending towards a more integrative approach, [23] which combines mark-recapture data with population dynamics models and other types of data. The integrated approach is more computationally demanding, but extracts more information from the data improving parameter and uncertainty estimates. [24]
The administration of drugs to whole populations irrespective of disease status is referred to as mass drug administration (MDA) .
This article describes the administration of antimalarial drugs to whole populations an intervention which has been used as a malaria -control measure for more than 70 years. Recent proposals to eliminate or even to eradicate malaria have led to a renewed interest in mass drug administrations in areas with very high malaria endemicity . [1] Drugs have been administered either directly as a full therapeutic course of treatment or indirectly through the fortification of salt. Mass drug administrations were generally unsuccessful in interrupting transmission but, in some cases, had a marked effect on parasite prevalence and on the incidence of clinical malaria. MDAs are likely to encourage the spread of drug-resistant parasites and so have only a limited role in malaria control. They may have a part to play in the management of epidemics and in the control of malaria in areas with a very short transmission season. In order to reduce the risk of spreading drug resistance, MDAs should use more than one drug and, preferably include a drug, such as an artemisinin , which has an effect on gametocytes . MDAs have low acceptance in areas with low malaria endemicity.
Another example of mass drug administration is mass deworming of children to remove helminth infections (intestinal worms).
Reports of attempts to control malaria through mass treatment with antimalarial drugs date back to at least 1932. [2] In the 1950s, the WHO included mass drug administration (MDA) of antimalarial drugs as a tool for malaria eradication ‘in exceptional conditions when conventional control techniques have failed. [3] In 1971, the WHO expert committee on malaria still recommended MDA in special circumstances. [4] Subsequently, MDA was linked to the emergence of drug resistance and its overall benefit was questioned. [5] [6] [7] [8] Concomitantly, the goal of malaria eradication was replaced by one of prevention of malaria morbidity and mortality through the provision of effective treatment. Considering the short lasing benefit of mass drug administration one modification has been to repeat mass drug administrations which has led to the development of intermittent preventive therapy .
Two methods of MDA, direct and indirect, have been used. In direct MDA, also referred to as ‘Mass drug treatment’, a therapeutic dose of the antimalarial drug, usually in the form of tablets, is given to an entire population. In indirect MDA, the antimalarial drug is added to food, such as the fortification of salt.
The first, well documented use of direct MDA took place in a rubber plantation in Liberia in 1931. [2] Two doses of the 8- aminoquinoline plasmoquine were given weekly to workers and their families in two camps. The prevalences of malaria parasite infections in humans and anopheline mosquitoes before and after treatment were studied. The authors concluded that ‘the fall in the mosquito infection rate of the two plasmoquine treated camps was so large as to indicate a local disappearance, or at least a great reduction, in gametocyte carriers in the treated population’. No long-term follow up data were provided for this study or most of the trials reported subsequently. The next documented use of MDA in sub-Saharan Africa took place in 1948 and 1949 in tea estates in Kericho, Kenya . [9] Ten thousand inhabitants of the tea estates received twice weekly proguanil from April to July 1948. The intervention was supplemented with DDT spraying in March and June of the following year. Before the intervention the mean malaria incidence in July, the peak of the malaria transmission season, was 56 cases per 1000 population. Following the intervention 4 malaria cases were detected in July 1949. The author therefore recommended continuation of twice weekly proguanil prophylaxis on the estates.
The Nandi district of Kenya was the scene of a large MDA in 1953 and 1954. [10] [11] [12] The target population of 83,000 received a single dose of pyrimethamine at the beginning of the malaria season in 1953 and 1954. The coverage was estimated to be around 95%.  Before the intervention severe malaria epidemics had been reported in the area. Following the intervention the parasite prevalence dropped from 23% to 2.3%.  The author states that in a control area parasite prevalence rose over the same period  to over 50%. It was felt that the MDA was effective in curbing severe malaria epidemics. In the following three years, 1955 to 1957, pyrimethamine administration was replaced with Dieldrin spraying to consolidate malaria control, which makes an assessment of the long-term effect of this MDA impossible.
During a pilot programme in Uganda in 1959 mass administration of chloroquine / pyrimethamine was combined with spraying of residual insecticides (DDT). [13] The success of the pilot programme led to a larger study targeted at a population of 16,000. Because of logistic problems, only half of the target population received the first round of MDA. According to the investigators, the intervention resulted in the eradication of the vector and rapid elimination of malaria from the area. [14]
Two large trials of MDA combined with household spraying with DDT were conducted in Cameroun and Upper Volta ( Burkina Faso ) in 1960–1961. [15] [16] [17] In both trials, substantial reductions in the prevalence of parasitaemia were achieved but transmission was not interrupted. [18] In Bobo-Dioulasso , where primaquine was used in combination with either chloroquine or amodiaquine , the prevalence of gametocytes and Anopheles gambiae sporozoites were reduced substantially. A MDA was also combined with DDT spraying in Zanzibar (Dola 1974). After the MDA, the parasite prevalence in children decreased, but the overall parasite prevalence increased slightly, thus failing to deplete the reservoir of infection. [19]
Two trials in Northern Nigeria combined multiple rounds of MDA and insecticide spraying. The first trial, in Kankiya, included 11 rounds of MDA combined with 8 rounds of DDT indoor spraying. [20] The study was based on computer-aided models that showed that MDA could eradicate malaria in the study area if combined with an appropriate ‘insecticide attack’. [21] Following MDAs, parasite prevalence dropped from 19% to 1%. The investigators did not consider this a success because parasite prevalence increased again after the interventions were stopped. Entomological indices also showed only a temporary reduction in transmission, which was completely reversed after the control measures ceased. Because the investigators felt that the failure of the trial to interrupt transmission was due to operational inadequacies, they recommended a much larger and more sophisticated evaluation of insecticide spraying combined with MDA. This recommendation helped to launch the Garki project, also in Northern Nigeria, in 1969. [22] In the Garki project, all 164 study villages in the catchment area were sprayed with propoxur , a residual insecticide. In addition, in 60 villages, MDA with sulfalene / pyrimethamine was given at 10-week intervals for two years. In two small village clusters, house spraying was supplemented with larvicide and MDA every two weeks. With biweekly MDA, parasite prevalence fell to 1% in the dry season and to 5% in the rainy season. MDA given every 10 weeks resulted in a parasite prevalence of 2% in the dry season and 28% in the rainy season. Transmission was not interrupted with either MDA regime. The authors concluded that spraying of residual insecticides and MDA did not result in a sustainable interruption of malaria transmission.
In 1999 in The Gambia residents living in 33 of 42 villages in the catchment area received a single dose of sulfadoxine / pyrimethamine (SP) combined with artesunate while the residents of nine control villages received placebo. [23] Following the MDA, 1388 children ≤10 years of age  living in nine control villages and in nine matched villages which had been allocated active treatment were kept under surveillance for clinical malaria throughout the transmission season. Initially, during July and August, the mean malaria incidence rate in treated villages was significantly lower than in the control villages. In subsequent months, the incidence was slightly higher in the MDA villages. The difference between the two groups was not statistically significant. Overall no benefit of the mass drug administration was detected over the course of the malaria transmission season.
A mass drug administration campaign using S/P, artesunate and primaquine was completed in Moshi district, Tanzania in 2008. The findings have yet to be published.
Outside of sub-Saharan Africa one of the larger reported malaria-control projects using MDA took place in Nicaragua in 1981 following the overthrow of the Somoza regime. [24] An estimated 70% of Nicaragua’s total population (1.9 million people) received chloroquine and primaquine during the peak period of disease transmission (November). An estimated 9200 cases of malaria were prevented. The campaign had better results in preventing and curing malaria infections than in interrupting transmission. However, the mass administration of antimalarials was not sustainable and, as with other malaria-control efforts, collapsed following the return of politically conservative forces. [25]
In three malaria-control projects conducted in the Indian states of Andhra Pradesh , Uttar Pradesh , and Orissa in the early 1960s, MDA had an ancillary role and was mentioned only briefly in reports on these interventions. [26] [27] [28] More detailed information is available following a focal outbreak in two villages in Gujarat State during 1978-1979. [29] Here a mass administration of chloroquine was part of a programme of intensified surveillance, case management, health education, and residual spraying. The incidence of malaria decreased so that, by the end of 1979, the authors considered the intervention to be a success. In 1980, in areas of Andhra Pradesh State in India, residual spraying was combined with a MDA. [30] During the period of lowest malaria incidence a single dose of chloroquine plus primaquine was distributed to the whole population in eight villages. A second dose was given after an interval of 2–3 months. This project failed to reduce malaria incidence and was considered to be a failure.
In 1984, MDA was added to the distribution of insecticide-impregnated bed nets ( ITNs ) in Sabah ( Malaysia ), but this failed to interrupt malaria transmission. [31] A MDA in Sumatra , Indonesia in 1987 focused on schoolchildren. [32] Eight months after the MDA, Plasmodium falciparum prevalence had decreased from 14% to 1%.
The only reported project with an MDA component which succeeded in permanently interrupting malaria transmission took place on the island of Aneityum , Vanuatu . [33] [34] Starting in September 1991, three malaria-control activities were employed – permethrin -impregnated bednets , larvivorous fish and the administration of three antimalarials. This MDA comprised 300 mg chloroquine base and 45 mg pyrimethamine weekly for nine weeks. An additional 300 mg chloroquine and 75 mg pyrimethamine plus 1500 mg sulfadoxine was added to this regimen in the first, fifth, and ninth week. Children received an adjusted equivalent of the adult dose. Follow-up consisted of yearly parasite surveillance. During the seven surveillance years following the MDA, no P.falciparum infections were detected.
MDA is included in the malaria-control policy of the People’s Republic of China . Following the first malaria-control phase from 1955 to 1962, which was mostly focused on malaria surveys, mass administrations were added to vector control measures and improved case management in 10 of China’s 33 provinces. [35] The drugs used in the administrations, mostly chloroquine and piperaquine, were provided free of charge by the central government. The economic reforms instituted by Deng Xiaoping , which ultimately put an end to the provision of free health care through the central government and the emergence of resistance against the most widely used antimalarials modified the use of mass drug administrations after 1980. MDAs are now targeted at high-risk populations, specifically non-immune migratory workers who receive repeated courses during the high transmission season. According to government guidelines, piperaquine , chloroquine , or sulfadoxine combined with primaquine can be used for mass administrations. [36] The artemisinin derivatives are not used in mass drug administrations and are reserved for treatment failures. Malaria burden and control measures are shown in Table 1. Between 1990 and 2000 the malaria prevalence dropped from 10.6 to 1.9 / 100,000, the number of reported malaria cases dropped from 117,359 to 24,088 while the number of reported deaths attributable to malaria remained stable. [37] These data, reported to the national government, depend on reporting from health care providers and like all data depending on passive surveillance tend to underestimate the true disease burden. However, there is no reason to think that the level of underreporting has changed over the last decade. Therefore, the proportional reduction in malaria disease burden is likely to be true. Malaria-control measures, including MDA, as well as major ecologic changes during the second half of the last century are likely to have been responsible for the more than 100-fold reduction in malaria burden in China since the initial surveys in 1955. [38] The widespread use of antimalarials has been followed by the emergence of drug resistance especially in regions with high drug use. By 1995 more than 95% of P.falciparum strains isolated in the South of Yunnan province were found to be resistant to chloroquine , and piperaquine while in the remainder of Yunnan and Hainan province the resistance rates were 85%and 38% respectively. [39]
A different approach to MDA consists of adding an antimalarial to an essential foodstuff, usually salt. Chloroquinized salt for malaria suppression was introduced by Pinotti in 1952 and gave promising results in a number of field trials and malaria-control programmes in Brazil. [40] [41] [42]
In 1959, the WHO conducted a trial in West New Guinea (later known as Irian Jaya ). [43] Salt crystals were mixed with pyrimethamine so as to provide a 0.07% pyrimethamine salt. As there were no shops in the catchment area, each family unit received fortnightly a quantity of salt from the local teacher or another member of the village community. Within three and a half months of the onset of the campaign, clinically significant levels of pyrimethamine resistance were reported. It was then decided to mix the remaining stock of pyrimethaminized salt with chloroquine powder. The chloroquine base content was 0.04% or 140 mg per adult per week based on a 5g per day salt consumption. The emergence of chloroquine resistance was investigated, but this was not detected. The distribution of medicated salts otherwise had no effect and it was concluded that ‘Pinotti’s method holds no prospect of malaria eradication…’ . The explanation for this finding given by the author is that ‘the salt consumption by children was too small to reduce significantly the parasite reservoir of the younger age groups’ .
Between 1961 and 1965, the use of chloroquinized salt was made compulsory over an area of 109,000km2 in Guyana , covering a population of 48,500 individuals. [44] The chloroquinized salt was prepared at a state salt plant so as to provide a 0.43% chloroquine concentration. The salt was sold in two pound plastic bags. The state held the monopoly for the salt. The only alternative source was salt smuggled from Brazil. Although the chloroquinized salt was used, its popularity was limited by the occurrence of a photo-allergic dermatitis popularly called ‘salt itch’ noted in all treatment areas. Chloroquine resistance was first observed in 1962 in the area with the lowest relative uptake of chloroquinized salt. In the course of the following months, a complete replacement of the susceptible strains with resistant P. falciparum strains was observed. Following the reintroduction of DDT spraying, the prevalence of P. falciparum declined.
In Southeast Asia, the medicated salt project at Pailin , on the Kampuchea - Thai border, demonstrated how drug resistance can develop when a large population of P. falciparum undergoing high transmission rates is exposed to intense drug pressure. [45] The project was launched in 1960 and covered a population of approximately 20,000. Sea salt was mixed with pyrimethamine at a concentration of 0.05%. Between 1960 and 1961, 77 tons of medicated salt were distributed in the area. After widespread pyrimethamine resistance was reported, pyrimethamine was replaced by chloroquine . From 1961 to 1962, 75 tons of chloroquine were distributed.  In two indicator districts, the parasite rates decreased from 40% to 7% and from 27% to 14%. [46] Chloroquine resistant P.falciparum isolates were first detected in Pailin in 1962 which appeared to be widespread by 1966. However no survey was undertaken to document the prevalence in the area. The factors leading to the emergence and spread of drug resistance appear to have been the continuous introduction of non-immune migrants, attracted by the promise of quick wealth from mining of precious stones, and prolonged drug pressure resulting from individual drug consumption and mass drug administration. Unrelated to MDAs the emergence of artemisinin resistant P.falciparum strains was reported in Pailin in 2008, this may have been related to overuse of artemisinin derivatives including counterfeit drugs but was not related to programmatic MDAs. [47] [48] [49]
Further malaria-control projects have used MDA, but have never been published, or have been published as technical reports. [50] [51]
Whether MDAs can be considered successful or not depends on the expectation of what they might achieve; many studies do not define whether their main aim was to interrupt transmission or to control disease. When MDAs were used as part of an attempt to interrupt transmission completely, they almost always failed. Only one project, conducted on Aneityum , a small isolated island in the Pacific, succeeded in permanently interrupting transmission using MDA as one of several malaria-control strategies. However, although unable to interrupt transmission, many MDA projects led to a marked reduction in parasite prevalence and probably had a marked also transient effect on malaria-related morbidity and mortality . Most of the early trials used study designs which would now be considered inadequate to provide a definitive answer on study outcome. For example, before-and-after comparisons were used frequently. Such comparisons are especially unreliable for vector-borne diseases which may show marked variations in incidence from season to season as well as from year to year. Furthermore, in several studies only a single intervention and control area or group were compared despite the fact a single control group cannot provide statistically interpretable results (see n = 1 fallacy ).
The deficiencies in the study designs mentioned above reflect the evolution of research methodology over the last 50 years. The evaluation of an intervention such as MDA is complicated by the fact that the effect of the intervention on transmission can only be measured at the community and not at the individual level. Trial methods which use a community, a village, or a cluster as unit of inference have taken longer to evolve than those used for individually randomized trials . There are, with some notable exceptions, few properly designed and analysed cluster randomized trials conducted by health care researchers prior to 1978. One major handicap for researchers who need to use the cluster approach, besides the need for a large sample size, is the need to use statistical methods that differ from the familiar methods used in individually randomized trials. Significant progress has been made in the development of statistical methods for the analysis of correlated data .
The present unpopularity of MDA is not only due to doubts regarding the health benefit of this intervention but to the fear that MDAs will facilitate the spread of drug resistance . Concern that MDA would cause pyrimethamine and later chloroquine resistance was first raised in the early 1960s. Circumstantial evidence linked the use of medicated salts to the emergence of chloroquine resistance in the 1980s: Chloroquine resistance emerged first in three foci, namely South America ( Colombia , Venezuela , Brazil ), Southeast Asia ( Thailand / Kampuchea ), and Africa ( Tanzania / Kenya ). Payne has argued that the one common factor between these three epidemiologically diverse areas was widespread distribution of medicated salts prior to the emergence of chloroquine resistance. [8]
In contrast to indirect MDA, emergence of drug resistance has not been linked to the administration of therapeutic doses of antimalarials through direct MDA programmes. The likely explanation lies in the different pharmacokinetic profiles that result from these two methods of drug administration. The administration of therapeutically dosed antimalarial drugs results in a single peak drug level which kills all susceptible strains. Only during the terminal half life of the drug when the concentration drops below the C min , the inhibitory concentration which kills the large majority of a parasite population, will new infections with more resistant strains have a survival advantage . Thus drugs with a very short terminal half-life, including artemisinin derivatives , carry a lower risk of selecting resistant parasites than longer acting drugs. In contrast, the administration of medicated salts is likely to result in drug levels undulating in the sub-lethal range, which reach a steady state after several doses have been administered. The situation is worse if drugs such as chloroquine are used which accumulate progressively. This situation, a steady increase in drug concentration, is identical to the experimental design used for the in vitro induction of drug resistance. [52] Medicated salt projects can be considered as large scale in vivo experiments designed to select resistant parasites.
The administration of antimalarials to large numbers of individuals with little or no preliminary screening could result in significant toxicity as nearly all antimalarials in common use can occasionally cause serious adverse events. For example, the widespread use of 8-aminoquinolines in areas where Glucose-6-phosphate dehydrogenase deficiency is common carries the risk of precipitating episodes of haemolysis . Few MDA projects have reported specifically on adverse events. No life-threatening outcomes have been reported as a result of an MDA but a rare serious adverse event such as a blood dyscrasia would probably not have been detected without active surveillance for adverse events which was not reported in any of the studies. There is a theoretical risk that administration of antimalarial drugs during the course of MDAs to women in the first trimester of pregnancy, some of whom may not know that they are pregnant, could lead to foetal abnormalities. The benefit of malaria control has to be weighed against potential problems. Hence MDA is likely to be only used in areas with very high malaria endemicity.
The Mathematical Biosciences Institute (MBI) is an institution of higher learning affiliated with the Ohio State University in Columbus, Ohio . [2] [3] MBI received major funding from the National Science Foundation .
Under the leadership of founding director Avner Friedman , MBI opened in September 2002, holding its first workshop, hosting its first visiting researchers, and starting its first cohort of postdocs in that month. MBI holds 10–12 scientific workshops each year, and hosts about 25 postdoctoral and visiting researchers in residence at any given time. Through its collective events and programs, MBI draws over 1000 visits by researchers in the broadly defined area of mathematical biology throughout the year. MBI’s long term planning is overseen by its Directorate and its Board of Trustees, while its scientific activities are overseen by its Directorate and its Scientific Advisory Committee.
MBI organizes Emphasis Semesters consisting of three or four week-long workshops. Emphasis Semesters are organized around selected themes which have included Mathematical Neuroscience, Cancer and Its Environment, and Analysis of Complex Data in Biological Systems. Outside of Emphasis Semesters, Current Topic Workshops focus on emerging topics in the mathematical biosciences. Most of the Institute's programs are conducted on The Ohio State University campus, but MBI also sponsors conferences and workshops at its academic Institute Partners. MBI accepts proposals for future programs. [4] [5]
MBI postdoctoral fellows engage in an integrated program of tutorials, working seminars, workshops, and interactions with their mathematical and bioscience mentors. These activities are geared toward providing the tools to pursue an independent research program with an emphasis on collaborative research in the mathematical biosciences.
MBI has a program of support for visitors to spend an extended period of time in residence. During their time at the institute, which can range from a few weeks to many months, visitors can focus on their research while benefiting from participation in MBI workshops and seminars and collaborating with others in the MBI community. Visitors also participate in the Visiting Lecturer Program through which they can share their research.
Early Career Awards are aimed at non-tenured scientists who have continuing employment and who hold a doctorate in any of the mathematical, statistical and computational sciences, or in any of the biological, medical, and related sciences. Award winners are supported to spend a period of time in residence at MBI.
The MBI Summer Undergraduate Research Program aims to give outstanding undergraduate students the opportunity to conduct meaningful research in the mathematical biosciences. MBI works with partner institutions to facilitate an eight-week Research Experience for Undergraduates (REU) , supported by the National Science Foundation . Students also participate in a mathematical biology bootcamp at MBI and present their completed research at the Undergraduate Capstone Conference.
MBI co-sponsors a rotating annual summer school with the National Institute for Mathematical and Biological Synthesis (NIMBioS) and Centre for Applied Mathematics in Bioscience and Medicine (CAMBAM) . The school brings together graduate students in mathematics, biology, and related fields to engage in a focused course of study on a current topic in mathematical biology. [6]
Past programs include the workshop for Women Advancing Mathematical Biology, Workshop for Young Researchers in Mathematical Biology, Blackwell Tapia Conference, and the Science Sundays lecture series.
The MBI Institute Partner (IP) program promotes the involvement of the international math biosciences community in MBI programs. Institute Partners receive direct benefits and opportunities enabling them to support, guide and participate in MBI research and education programs.
Mathematical models can project how infectious diseases progress to show the likely outcome of an epidemic and help inform public health interventions. Models use basic assumptions or collected statistics along with mathematics to find parameters for various infectious diseases and use those parameters to calculate the effects of different interventions, like mass vaccination programmes. The modelling can help decide which intervention(s) to avoid and which to trial, or can predict future growth patterns, etc.
The modeling of infectious diseases is a tool that has been used to study the mechanisms by which diseases spread, to predict the future course of an outbreak and to evaluate strategies to control an epidemic. [1]
The first scientist who systematically tried to quantify causes of death was John Graunt in his book Natural and Political Observations made upon the Bills of Mortality , in 1662. The bills he studied were listings of numbers and causes of deaths published weekly. Graunt's analysis of causes of death is considered the beginning of the "theory of competing risks" which according to Daley and Gani [1] is "a theory that is now well established among modern epidemiologists".
The earliest account of mathematical modelling of spread of disease was carried out in 1760 by Daniel Bernoulli . Trained as a physician, Bernoulli created a mathematical model to defend the practice of inoculating against smallpox . [2] The calculations from this model showed that universal inoculation against smallpox would increase the life expectancy from 26 years 7 months to 29 years 9 months. [3] Daniel Bernoulli's work preceded the modern understanding of germ theory .
In the early 20th century, William Hamer [4] and Ronald Ross [5] applied the law of mass action to explain epidemic behaviour.
The 1920s saw the emergence of compartmental models. The Kermack–McKendrick epidemic model (1927) and the Reed–Frost epidemic model (1928) both describe the relationship between susceptible , infected and immune individuals in a population. The Kermack–McKendrick epidemic model was successful in predicting the behavior of outbreaks very similar to that observed in many recorded epidemics. [6]
Recently, agent-based models (ABMs) have been used in exchange for simpler compartmental models . [7] For example, epidemiological ABMs have been used to inform public health (nonpharmaceutical) interventions against the spread of SARS-CoV-2 . [8] Epidemiological ABMs, in spite of their complexity and requiring high computational power, have been criticized for simplifying and unrealistic assumptions. [9] [10] Still, they can be useful in informing decisions regarding mitigation and suppression measures in cases when ABMs are accurately calibrated. [11]
Models are only as good as the assumptions on which they are based. If a model makes predictions that are out of line with observed results and the mathematics is correct, the initial assumptions must change to make the model useful.
"Stochastic" means being or having a random variable. A stochastic model is a tool for estimating probability distributions of potential outcomes by allowing for random variation in one or more inputs over time. Stochastic models depend on the chance variations in risk of exposure, disease and other illness dynamics.
When dealing with large populations, as in the case of tuberculosis, deterministic or compartmental mathematical models are often used.  In a deterministic model, individuals in the population are assigned to different subgroups or compartments, each representing a specific stage of the epidemic.
The transition rates from one class to another are mathematically expressed as derivatives, hence the model is formulated using differential equations.  While building such models, it must be assumed that the population size in a compartment is differentiable with respect to time and that the epidemic process is deterministic. In other words, the changes in population of a compartment can be calculated using only the history that was used to develop the model. [6]
The basic reproduction number (denoted by R 0 ) is a measure of how transferable a disease is. It is the average number of people that a single infectious person will infect over the course of their infection. This quantity determines whether the infection will spread exponentially, die out, or remain constant: if R 0 > 1, then each person on average infects more than one other person so the disease will spread; if R 0 < 1, then each person infects fewer than one person on average so the disease will die out; and if R 0 = 1, then each person will infect on average exactly one other person, so the disease will become endemic: it will move throughout the population but not increase or decrease.
An infectious disease is said to be endemic when it can be sustained in a population without the need for external inputs. This means that, on average, each infected person is infecting exactly one other person (any more and the number of people infected will grow exponentially and there will be an epidemic , any less and the disease will die out). In mathematical terms, that is:
The basic reproduction number ( R 0 ) of the disease, assuming everyone is susceptible, multiplied by the proportion of the population that is actually susceptible ( S ) must be one (since those who are not susceptible do not feature in our calculations as they cannot contract the disease). Notice that this relation means that for a disease to be in the endemic steady state , the higher the basic reproduction number, the lower the proportion of the population susceptible must be, and vice versa. This expression has limitations concerning the susceptibility proportion, e.g. the R 0 equals 0.5 implicates S has to be 2, however this proportion exceeds to population size.
Assume the rectangular stationary age distribution and let also the ages of infection have the same distribution for each birth year. Let the average age of infection be A , for instance when individuals younger than A are susceptible and those older than A are immune (or infectious). Then it can be shown by an easy argument that the proportion of the population that is susceptible is given by:
We reiterate that L is the age at which in this model every individual is assumed to die. But the mathematical definition of the endemic steady state can be rearranged to give:
Therefore, due to the transitive property :
This provides a simple way to estimate the parameter R 0 using easily available data.
For a population with an exponential age distribution ,
This allows for the basic reproduction number of a disease given A and L in either type of population distribution.
Compartmental models are formulated as Markov chains . [12] A classic compartmental model in epidemiology is the SIR model, which may be used as a simple model for modeling epidemics. Multiple other types of compartmental models are also employed.
In 1927, W. O. Kermack and A. G. McKendrick created a model in which they considered a fixed population with only three compartments: susceptible, S ( t ) {\displaystyle S(t)} ; infected, I ( t ) {\displaystyle I(t)} ; and recovered, R ( t ) {\displaystyle R(t)} . The compartments used for this model consist of three classes: [13]
There are many modifications of the SIR model, including those that include births and deaths, where upon recovery there is no immunity (SIS model), where immunity lasts only for a short period of time (SIRS), where there is a latent period of the disease where the person is not infectious ( SEIS and SEIR ), and where infants can be born with immunity (MSIR).
For evaluating the epidemic threshold in the SIS model on networks see Parshani et al. [14]
Mathematical models need to integrate the increasing volume of data being generated on host - pathogen interactions. Many theoretical studies of the population dynamics , structure and evolution of infectious diseases of plants and animals, including humans, are concerned with this problem. [ citation needed ] A model to assess the probability for a worldwide spreading and declare pandemic has been recently developed by Valdez et al. [15] Research topics include:
If the proportion of the population that is immune exceeds the herd immunity level for the disease, then the disease can no longer persist in the population. Thus, if this level can be exceeded by vaccination, the disease can be eliminated. An example of this being successfully achieved worldwide is the global smallpox eradication , with the last wild case in 1977. The WHO is carrying out a similar vaccination campaign to eradicate polio . [ citation needed ]
The herd immunity level will be denoted q . Recall that, for a stable state:
In turn,
which is approximately:
S will be (1 − q ), since q is the proportion of the population that is immune and q + S must equal one  (since in this simplified model, everyone is either susceptible or immune). Then:
Remember that this is the threshold level. If the proportion of immune individuals exceeds this level due to a mass vaccination programme, the disease will die out.
We have just calculated the critical immunisation threshold (denoted q c ). It is the minimum proportion of the population that must be immunised at birth (or close to birth) in order for the infection to die out in the population.
Because the fraction of the final size of the population p that is never infected can be defined as:
Hence,
Solving for R 0 {\displaystyle R_{0}} , we obtain:
If the vaccine used is insufficiently effective or the required coverage cannot be reached (for example due to popular resistance ), the programme may fail to exceed q c . Such a programme can, however, disturb the balance of the infection without eliminating it, often causing unforeseen problems.
Suppose that a proportion of the population q (where q < q c ) is immunised at birth against an infection with R 0 > 1. The vaccination programme changes R 0 to R q where
This change occurs simply because there are now fewer susceptibles in the population who can be infected. R q is simply R 0 minus those that would normally be infected but that cannot be now since they are immune.
As a consequence of this lower basic reproduction number , the average age of infection A will also change to some new value A q in those who have been left unvaccinated.
Recall the relation that linked R 0 , A and L . Assuming that life expectancy has not changed, now:
But R 0 = L / A so:
Thus the vaccination programme will raise the average age of infection, another mathematical justification for a result that might have been intuitively obvious. Unvaccinated individuals now experience a reduced force of infection due to the presence of the vaccinated group.
However, it is important to consider this effect when vaccinating against diseases that are more severe in older people. A vaccination programme against such a disease that does not exceed q c may cause more deaths and complications than there were before the programme was brought into force as individuals will be catching the disease later in life. These unforeseen outcomes of a vaccination programme are called perverse effects . [ citation needed ]
If a vaccination programme causes the proportion of immune individuals in a population to exceed the critical threshold for a significant length of time, transmission of the infectious disease in that population will stop. This is known as elimination of the infection and is different from eradication . [ citation needed ]
Models have the advantage of examining multiple outcomes simultaneously, rather than making a single forecast. Models have shown broad degrees of reliability in past pandemics, such as SARS , Swine flu , MERS and Ebola . [16]
In human geography , McNeill's law is the process outlined in William H. McNeill's book Plagues and Peoples .  The process described concerns the role of microbial disease in the conquering of people-groups. [1] Particularly, it describes how diseases such as smallpox , measles , typhus , scarlet fever , and sexually-transmitted diseases have significantly reduced native populations so that they are unable to resist colonization. [2]
According to McNeill's Law, the microbiological aspect of conquest and invasion has been the deciding principle or one of the deciding principles in both the expansion of certain empires (as during the emigration to the Americas) and the containment in others (as during the crusades ). [3] The argument is that less civilized peoples were easily subjugated due to the immunological advantages of those coming from civilized countries. [4] An evidence presented to support the hypothesis involves the manner diseases associated with Europeans were rebuffed in their forays into disease-experienced countries such as China and Japan . [5]
McNeill's law also maintains that parasites are not only natural but also social in the sense that these organisms are part of the social continuum and that the human social evolution is inextricably linked with genetic transformations. [4]
The first people-group fully wiped out due to European expansion (with the possible exception of the Arawaks ) was the Guanches of the Canary Islands .  Despite an inbred ferocity, superior knowledge of the land and even a possible tactical superiority, they were eventually wiped out through the concentrated efforts of the Spanish and Portuguese .  McNeill's Law would place the deciding factor squarely on the introduction of deadly diseases and parasites from the mainland to the previously geographically isolated islanders.
This is the likely explanation, as what records still exist show numerous deaths by disease on the islands and a declining birth-rate, leading eventually to the almost complete end of the Guanches as a race.
Other instances include the devastation of the Incas by smallpox .

This history article is a stub . You can help Wikipedia by expanding it .
Measuring attractiveness through a categorical-based evaluation technique [1] [2] [3] is the goal of the MACBETH approach that was designed by Carlos António Bana e Costa , from the University of Lisbon, in cooperation with Professor Jean-Claude Vansnick and Dr. Jean-Marie De Corte, from the Université de Mons. [ citation needed ]
MACBETH permits the evaluation of options against multiple criteria. The key distinction between MACBETH and other multiple-criteria decision analysis (MCDA) methods is that it needs only qualitative judgements about the difference of attractiveness between two elements at a time, in order to generate numerical scores for the options in each criterion and to weight the criteria. The seven MACBETH semantic categories are: no, very weak, weak, moderate, strong, very strong, and extreme difference of attractiveness. [ citation needed ]
MACBETH has been extensively applied in various evaluation contexts, namely: [2]
Several decision support systems implement the MACBETH approach, namely:
In epidemiology , Mendelian randomization is a method of using measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in observational studies . The design was first proposed in 1986 [1] and subsequently described by Gray and Wheatley [2] as a method for obtaining unbiased estimates of the effects of a putative causal variable without conducting a traditional randomised trial . These authors also coined the term Mendelian randomization . The design has a powerful control for reverse causation and confounding , which often impede or mislead epidemiological studies. [3]
An important focus of observational epidemiology is to identify modifiable causes of diseases of public health concern. In order to have firm evidence that some prospective intervention will have the desired beneficial effect on public health, the association observed between the particular risk factor and disease must imply that the risk factor either aggravates or actually causes the disease.
Well-known successes include the identified causal links between smoking and lung cancer, and between blood pressure and stroke. However, there have also been notable failures when identified exposures were later shown by randomised controlled trials to be non-causal. For instance, it was previously thought that hormone replacement would prevent cardiovascular disease , but it is now known to have no such benefit and may even adversely affect health. [4] Another example is some observational studies found an association between habitual coffee consumption and improved cardiovascular health , from which some inferred that coffee consumption has cardiovascular health benefits. However, it has since been suggested that the true causality is reversed, and researchers using the Mendelian randomization technique found statistical evidence that people do tend to reduce their coffee consumption in response to their own blood pressure levels and/or heart rate. [5]
Spurious findings in observational epidemiology are most likely caused by social, behavioural, or physiological confounding factors, which are particularly difficult to measure accurately and difficult to control for. Moreover, many epidemiological findings cannot be ethically replicated in clinical trials.
“Genetics is indeed in a peculiarly favoured condition in that Providence has shielded the geneticist from many of the difficulties of a reliably controlled comparison. The different genotypes possible from the same mating have been beautifully randomised by the meiotic process. A more perfect control of conditions is scarcely possible, than that of different genotypes appearing in the same litter.” — R.A. Fisher [6]
Mendelian randomization (MR) is a method that allows one to test for, or in certain cases to estimate, a causal effect from observational data in the presence of confounding factors . It uses common genetic polymorphisms with well-understood effects on exposure patterns (e.g., propensity to drink alcohol) or effects that mimic those produced by modifiable exposures (e.g., raised blood cholesterol [1] ). Importantly, the genotype must only affect the disease status indirectly via its effect on the exposure of interest. [7]
Because genotypes are assigned randomly when passed from parents to offspring during meiosis , if we assume that mate choice is not associated with genotype ( panmixia ), then the population genotype distribution should be unrelated to the confounding factors that typically plague observational epidemiology studies. In this regard, Mendelian randomization can be thought of as a “naturally” randomized controlled trial. Because the polymorphism is the instrument, Mendelian randomization is dependent on prior genetic association studies having provided good candidate genes for response to risk exposure.
From a statistical perspective, Mendelian randomization (MR) is an application of the technique of instrumental variables [8] [9] with genotype acting as an instrument for the exposure of interest. The method has also been used in economic research studying the effects of obesity on earnings, and other labor market outcomes. [10]
Accuracy of MR depends on a number of assumptions: That there is no direct relationship between the instrumental variable and the dependent variables, and that there are no direct relations between the instrumental variable and any possible confounding variables. In addition to being misled by direct effects of the instrument on the disease, the analyst may also be misled by linkage disequilibrium with unmeasured directly-causal variants, genetic heterogeneity , pleiotropy (often detected as a genetic correlation ), or population stratification . [11] Mendelian randomization is widely used in analyzing data of the large-scale Genome-wide association study , which usually adopts a case-control design. The conventional assumptions for instrumental variables under a case-control design are instead made in the population of controls. [12] Ignoring the ascertainment bias of a case-control study when performing a Mendelian randomization can lead to considerable bias in the estimation of causal effects. [12]
The basics of MR were invented by Martijn B. Katan in 1986, when he suggested the use of apolipoprotein E alleles, that had known effects on blood cholesterol levels, to study the causality between blood cholesterol and cancer. [1] [3] However, MR is based on instrumental variables of econometrics, which were already invented in 1928 by Philip Green Wright and Sewall Wright . [13] The term "Mendelian randomization" was first used by Richard Gray and Keith Wheatley in 1991. [2] [14] It comes from the name of Gregor Mendel and the fact that alleles are distributed randomly in people at fertilisation . [14] MR studies have become more common between 2007–2010 due to coincidental progress of omics -type of genetic research, which has provided lots of previously unknown connections between alleles and modifiable exposures. [15]
Metascreen is an advanced non-invasive metabolic screening test distributed by Cordlife Group Limited ("Cordlife"). It can detect as many as 110 inborn errors of metabolism ("IEMs", or metabolic disorders) from a urine specimen. Cordlife owns the brand name and trademark, "Metascreen".
Metascreen, as a newborn metabolic screening test, was first launched by Cordlife in India in October 2013. [1] Since April 2014, Metascreen became available also in Hong Kong and the Philippines through Cordlife for parents looking for more comprehensive screening of metabolic disorders for their children. As a newborn suffering from certain metabolic disorder, such as isovaleric acidemia, may appear asymptomatic in the first few days or even weeks of life, early detection and treatment is key in preventing irreversible lifelong complications, such as physical disability or mental retardation.
Unlike the conventional dried blood spot test for newborn screening that involves a painful heel prick, Metascreen uses urine specimen, collected without harm or discomfort to the newborn, to detect as many as 110 metabolic disorders. The urine specimen is collected on a filter paper, which is then air-dried and sent to the laboratory for analysis using a gas chromatography-mass spectrometry instrument ("GC-MS").  GC-MS is a FDA approved method for urinary analyte detection, a gold standard for lipids , drug metabolites and environmental analysis. [2]
Many of the IEMs that are classified as "organic acidemia", in which organic acids accumulate in the urine of newborns with these disorders, [3] are easily and accurately picked up by GC-MS.. The GC-MS platform is recommended by the American College of Medical Genetics for the detection of organic and amino acidemias through the urine. [4] Furthermore, the platform has also been shown to be reliable in detecting other types of IEMs, such as sugar metabolism disorders and fatty acid oxidation disorders. [3] [5] Indeed, GC-MS analysis is increasingly becoming a common way to diagnose IEMs for earlier intervention and treatment, resulting in a better outcome and quality of life.
As a major organ for excretion, the kidney removes waste materials and chemicals from the body, [6] such as increased concentrations of intermediary metabolites of a particular pathway, making urine (the waste product from the kidney) particularly useful for medical diagnostics. The key advantages of using urine as a biofluid are: (1) its sterility; (2) accessibility and non-invasive method of collection; and (3) it being largely free from interfering proteins or lipids. [7]
Although the human urine metabolome is a subset of the human serum metabolome, more than 484 compounds identified in urine by Bouatra et al. (either experimentally or via literature review) were not previously reported to be in blood. [7] The same group hypothesised that this is because the kidneys do an extraordinary job of removing and/or concentrating certain metabolites from the blood, hence, compounds far below the limit of detection in blood (using today’s instrumentation) are well above the detection limit in urine. [7] This difference, combined with the ability of the kidneys to handle abnormally high or abnormally low concentrations of metabolites, makes urine a particularly useful biofluid for medical diagnostics. [7] In fact, urinary metabolites have been used to characterize nearly 220 diseases. [7]
Cordlife announced in October 2013 that Cordlife India has introduced Metascreen service through a strategic collaboration with Navigene Genetic Science, a genetic diagnostic and research company. [8] In 2014, Navigene was embroiled in a controversy where its founders were accused of data and scientific research theft and fined for INR 3 Million by the Adjudicating Authority of the Department of Information Technology, Government of India. [9] [10] Navigene rejected all allegations [11] and has filed a writ petition (No. WP/3291/2014) with the High Court of Mumbai to challenge and squash the order.
The Mexican paradox is the observation that Mexican people exhibit a surprisingly low incidence of low birth weight (especially foreign-born Mexican mothers [1] ), contrary to what would be expected from their socioeconomic status (SES). This appears as an outlier in graphs correlating SES with low-birth-weight rates. The medical causes of lower rates of low birth weights among birthing Mexican mothers has been called into question. [2]
The hispanic paradox refers to the same phenomenon observed across the populations of South and Central America, where Mexicans remain the healthier. [3]
The results of a study showed that the mean birth weight of Mexican-American babies was 3.34 kg (7.37 lbs), while that of non-Hispanic White babies was 3.39 kg (7.48 lbs.). This finding re-emphasized the independence of mean birth weight and LBW. This however did not refute the discrepancies in LBW for Mexicans. The study also showed that the overall preterm birth rate was higher among Mexican Americans (10.6%) than non-Hispanic Whites (9.3%). In North Carolina, from 1996 to 2000, the infant death rate was 6.1 for Mexican-born infants, in comparison to 6.6 for White infants, and 15 for Black infants. [4] The overall hypothesis of the authors was that this finding reflected an error in recorded gestational age , described in a strongly bimodal birth-weight distribution at young gestational ages for Mexican-Americans.
Other external factors have been identified, such as protein-rich diets, extended family ties, and strong religious beliefs. A 1995 study suggested that the Virgin of Guadeloupe encouraged healthy births thanks to its iconic pregnancy symbolism. [4] Another study suggested that resistance to changes in diet is responsible for the positive birth weight association for Mexican-American mothers. [5] Yet another study showed that Mexicans blend traditional and modern medicine, a potential explanation for the paradox. [3]
It was also observed that, the longer Mexican women live in the United States, the more their infant mortality risks increased. [4] [6] Lower occurrences of periodontal disease with Mexican-American women was also suggested as an explanation for the Mexican paradox. [7] Another study revealed that, beyond healthy births, Mexican infants also have lower developmental outcomes in their development years. [8]
A 2014 study concluded that the Mexican paradox is disappearing slowly, most of the comparison data between Mexican-born infants and White infants being fairly similar in recent years. [1] Another 2016 study showed that the Mexican paradox erodes in the third generation of the immigrated family. [9]
It was also verified that Mexicans have less high blood pressure, cardiovascular diseases and most cancers than the US population in general. [3]
Microbial pathogenesis is a field of microbiology started at least as early as 1988, with the identification of the triune Falkow's criteria aka molecular Koch's postulates . [1] [2] In 1996 Fredricks and Relman proposed a seven-point list of "MOLECULAR GUIDELINES FOR ESTABLISHING MICROBIAL DISEASE CAUSATION", because of "the discovery of nucleic acids " by Watson and Crick "as the source of genetic information and as the basis for precise characterization of an organism ." The subsequent development of the "ability to detect and manipulate these nucleic acid molecules in microorganisms has created a powerful means for identifying previously unknown microbial pathogens and for studying the host-parasite relationship ." [2]
In 1996, Fredricks and Relman suggested the following postulates for the novel field of microbial pathogenesis. [2] [3]
This microbiology -related article is a stub . You can help Wikipedia by expanding it .
Molecular epidemiology is a branch of epidemiology and medical science that focuses on the contribution of potential genetic and environmental risk factors, identified at the molecular level, to the etiology , distribution and prevention of disease within families and across populations. [1] This field has emerged from the integration of molecular biology into traditional epidemiological research. Molecular epidemiology improves our understanding of the pathogenesis of disease by identifying specific pathways, molecules and genes that influence the risk of developing disease. [2] [3] More broadly, it seeks to establish understanding of how the interactions between genetic traits and environmental exposures result in disease. [4]
The term "molecular epidemiology" was first coined by Kilbourne in a 1973 article entitled "The molecular epidemiology of influenza". [5] The term became more formalized with the formulation of the first book on Molecular Epidemiology: Principles and Practice by Schulte and Perera. [6] At the heart of this book is the impact of advances in molecular research that have given rise to and enable the measurement and exploitation of the biomarker as a vital tool to link traditional molecular and epidemiological research strategies to understand the underlying mechanisms of disease in populations.
While most molecular epidemiology studies are using conventional disease designation system for an outcome (with the use of exposures at the molecular level), compelling evidence indicates that disease evolution represents inherently heterogeneous process differing from person to person. Conceptually, each individual has a unique disease process different from any other individual ("the unique disease principle"), [7] considering uniqueness of the exposome and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly, cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges including lack of standardized methodologies and guidelines as well as paucity of interdisciplinary experts and training programs. [8] [9] The use of "molecular epidemiology" for this type of research masked the presence of these challenges, and hindered the development of methods and guidelines. [10] [11] Furthermore, the concept of disease heterogeneity appears to conflict with the premise that individuals with the same disease name have similar etiologies and disease processes.
The genome of a bacterial species fundamentally determines its identity. Thus, gel electrophoresis techniques like pulsed-field gel electrophoresis can be used in molecular epidemiology to comparatively analyze patterns of bacterial chromosomal fragments and to elucidate the genomic content of bacterial cells. Due to its widespread use and ability to analyse epidemiological information about most bacterial pathogens based on their molecular markers, pulsed-field gel electrophoresis is relied upon heavily in molecular epidemiological studies. [12]
Molecular epidemiology allows for an understanding of the molecular outcomes and implications of diet, lifestyle, and environmental exposure, particularly how these choices and exposures result in acquired genetic mutations and how these mutations are distributed throughout selected populations through the use of biomarkers and genetic information. Molecular epidemiological studies are able to provide additional understanding of previously-identified risk factors and disease mechanisms. [13] Specific applications include:
While the use of advanced molecular analysis techniques within the field of molecular epidemiology is providing the larger field of epidemiology with greater means of analysis, Miquel Porta identified several challenges that the field of molecular epidemiology faces, particularly selecting and incorporating requisite applicable data in an unbiased manner. [15] Limitations of molecular epidemiological studies are similar in nature to those of generic epidemiological studies, that is, samples of convenience - both of the target population and genetic information, small sample sizes, inappropriate statistical methods, poor quality control, and poor definition of target populations. [16]
Molecular Koch's postulates are a set of experimental criteria that must be satisfied to show that a gene found in a pathogenic microorganism encodes a product that contributes to the disease caused by the pathogen.  Genes that satisfy molecular Koch's postulates are often referred to as virulence factors. The postulates were formulated by the microbiologist Stanley Falkow in 1988 and are based on Koch's postulates . [1]
The postulates as originally described by Falkow are as follows:
For many pathogenic microorganisms, it is not currently possible to apply molecular Koch's postulates to a gene in question.  Testing a candidate virulence gene requires a relevant animal model of the disease being examined and the ability to genetically manipulate the microorganism that causes the disease.  Suitable animal models are lacking for many important human diseases.  Additionally, many pathogens cannot be manipulated genetically. [ citation needed ]
In 1996, Fredricks and Relman remarked on Falkow's postulates in their seminal paper on microbial pathogenesis . [2] Their seven criteria have subsequently been accepted by most practitioners in the field.

This microbiology -related article is a stub . You can help Wikipedia by expanding it .
Molecular pathological epidemiology ( MPE , also molecular pathologic epidemiology ) is a discipline combining epidemiology and pathology . It is defined as "epidemiology of molecular pathology and heterogeneity of disease". [1] Pathology and epidemiology share the same goal of elucidating etiology of disease, and MPE aims to achieve this goal at molecular, individual and population levels. Typically, MPE utilizes tissue pathology resources and data within existing epidemiology studies. Molecular epidemiology broadly encompasses MPE and conventional-type molecular epidemiology with the use of traditional disease designation systems.
Data from The Cancer Genome Atlas projects indicate that disease evolution is an inherently heterogeneous process. [2] [3] Each patient has a unique disease process (“the unique disease principle”), considering the uniqueness of the exposome and its unique influence on molecular pathologic process. [2] This concept has been adopted in clinical medicine along with precision medicine and personalized medicine .
In MPE, investigators dissect interrelationships between exposures (e.g., environmental, dietary, lifestyle and genetic factors); alterations in cellular or extracellular molecules (disease molecular signatures); and disease evolution and progression. [2] Investigators can analyze genome , methylome , epigenome , metabolome , transcriptome , proteome , microbiome , immunity and interactome . A putative risk factor can be linked to specific molecular signatures.
MPE research enables identification of a new biomarker for potential clinical utility, using large-scale population-based data (e.g., PIK3CA mutation in colorectal cancer to select patients for aspirin therapy). [1] The MPE approach can be used following a genome-wide association study (GWAS), termed "GWAS-MPE approach". [4] Detailed disease endpoint phenotyping can be conducted by means of molecular pathology or surrogate histopathology or immunohistochemistry analysis of diseased tissues and cells within GWAS. [5] [6]
As an alternative approach, potential risk variants identified by GWAS can be examined in combination with molecular pathology analysis on diseased tissues. [7] [8] [9] [10] This GWAS-MPE approach can give not only more precise effect estimates, even larger effects, for specific molecular subtypes of the disease, but also insights into pathogenesis by linking genetic variants to molecular pathologic signatures of disease. [4] Since molecular diagnostics is becoming routine clinical practice, molecular pathology data can aid epidemiologic research.
MPE started as analysis of risk factors (such as smoking) and molecular pathologic findings (such as KRAS oncogene mutation in lung cancer).
Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly, cancer) became increasingly common throughout the 1990s and early 2000s. [11]
The use of molecular pathology in epidemiology posed lacked standardized methodologies and guidelines as well as interdisciplinary experts and training programs. [12] MPE research required a new conceptual framework and methodologies ( epidemiological method ) because MPE examines heterogeneity in an outcome variable. [13]
The term "molecular pathological epidemiology" was used by Shuji Ogino and Meir Stampfer in 2010. [14] Specific principles of MPE developed following 2010. The MPE paradigm is in widespread use globally, [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] and has been a subject in international conferences. [26] [27] [28] The International Molecular Pathological Epidemiology (MPE) Meeting Series has been open to research community around the world, its second and third meetings were held in Boston, in December 2014 and May 2016, [29] [30] respectively.
Mortality displacement is a phenomenon where a period of excess deaths (i.e., more deaths than expected) is followed by a period of mortality deficit (i.e., fewer deaths than expected). It is also known as " harvesting ". [1] [2] It is usually attributable to environmental phenomena such as heat waves , cold spells , epidemics and pandemics , especially influenza pandemics , famine or war .
During heat waves, for instance, there are often additional deaths observed in the population, affecting especially older adults and those who are sick.  After some periods with excess mortality, however, there has also been observed a decrease in overall mortality during the subsequent weeks. Such short-term forward shift in mortality rate is also referred to as harvesting effect . The subsequent, compensatory reduction in mortality suggests that the heat wave especially affected those whose health was already so compromised that they "would have died in the short-term anyway". [3]
Different institutions and initiatives offer weekly data to monitor excess mortality. Significant efforts to capture short term mortality data have been made along 2020 due to the pandemic of the Coronavirus disease 2019 (COVID-19) and its worldwide effects . Eurostat launched in April 2020 a collection of weekly death data that provide for most of the EU countries weekly death data series by 5-year age groups and sex in NUTS3 regions within the countries starting from the year 2000. [4] This temporary data collection was established in order to support the policy and research efforts related to the Covid-19 pandemic. Data are transmitted by the National Statistical Institutes on voluntary basis and it is being updated, depending on the country, weekly. [5]
The Human Mortality Database project launched in May 2020 a new data series, the Short-term Mortality Fluctuation series (STMF) , offering freely available weekly death counts by age and sex for a growing number of countries (34 in October 2020), as well as a visualization tool that captures the excess mortality in a weekly basis. The STMF was established to provide data for scientific analysis of all-cause mortality fluctuations by week within each calendar year in standard formats. As part of the HMD project, is a joint project of two teams based in the Laboratory of Demographic Data at the Max Planck Institute for Demographic Research (MPIDR) and at the Department of Demography of the University of California, Berkeley (UCB).
The collaborative network EuroMOMO (European mortality monitoring activity), monitors mortality across 24 European countries in order to detect and measure excess deaths related to seasonal influenza, pandemics and other public health threats. EuroMOMO is hosted and maintained by the Department of Infectious Disease Epidemiology and Prevention of Copenhagen , Denmark . They offer regular reports (weekly bulletins), graphs and maps showing the present levels of mortality but the network does not publish openly data. Individual partners may decide to share openly some selected national data, like for instance, MoMo-Spain . The study centre at the Statens Serum Institut in Copenhagen publishes a weekly situation report and regular scientific articles. Periods of high excess mortality have also been described for the United States. [6]
Mortality rate , or death rate , [2] : 189,69 is a measure of the number of deaths (in general, or due to a specific cause) in a particular population , scaled to the size of that population, per unit of time. Mortality rate is typically expressed in units of deaths per 1,000 individuals per year; thus, a mortality rate of 9.5 (out of 1,000) in a population of 1,000 would mean 9.5 deaths per year in that entire population, or 0.95% out of the total. It is distinct from " morbidity ", which is either the prevalence or incidence of a disease , and also from the incidence rate (the number of newly appearing cases of the disease per unit of time). [2] : 189 [ verification needed ]
An important specific mortality rate measure is the crude death rate , which looks at mortality from all causes in a given time interval for a given population. As of 2020 [update] , for instance, the CIA estimates that the crude death rate globally will be 7.7 deaths per 1,000 persons in a population per year. [3] In a generic form, [2] : 189 mortality rates can be seen as calculated using ( d / p ) ⋅ 10 n {\displaystyle (d/p)\cdot 10^{n}} , where d represents the deaths from whatever cause of interest is specified that occur within a given time period, p represents the size of the population in which the deaths occur (however this population is defined or limited), and 10 n {\displaystyle 10^{n}} is the conversion factor from the resulting fraction to another unit (e.g., multiplying by 10 3 {\displaystyle 10^{3}} to get mortality rate per 1,000 individuals). [2] : 189
The crude death rate is defined as "the mortality rate from all causes of death for a population," calculated as the "[t]otal number of deaths during a given time interval" divided by the "[m]id-interval population", per 1,000 or 100,000; for instance, the population of the U.S. was around 290,810,000 in 2003, and in that year, approximately 2,419,900 deaths occurred in total, giving a crude death (mortality) rate of 832 deaths per 100,000. [4] : 3–20f As of 2020 [update] , the CIA estimates the U.S. crude death rate will be 8.3 per 1,000, while it estimates that the global rate will be 7.7 per 1,000. [3]
According to the World Health Organization , the ten leading causes of death, globally, in 2016, for both sexes and all ages, were as presented in the table below. [5]
Crude death rate, per 100,000 population
Mortality rate is also measured per thousand. It is determined by how many people of a certain age die per thousand people. Decrease of mortality rate is one of the reasons for increase of population. Development of medical science and other technologies has resulted in the decrease of mortality rate in all the countries of the world for some decades. In 1990, the mortality rate of children under 5 years of age was 144 per thousand, but in 2015 the child mortality rate was 38 per thousand.
Other specific measures of mortality include: [4]
For any of these, a "sex-specific mortality rate" refers to "a mortality rate among either males or females", where the calculation involves both "numerator and denominator... limited to the one sex". [4] : 3–23
In most cases there are few if any ways to obtain exact mortality rates, so epidemiologists use estimation to predict correct mortality rates. Mortality rates are usually difficult to predict due to language barriers, health infrastructure related issues, conflict, and other reasons. Maternal mortality has additional challenges, especially as they pertain to stillbirths , abortions, and multiple births. In some countries, during the 1920s, a stillbirth was defined as "a birth of at least twenty weeks' gestation in which the child shows no evidence of life after complete birth". In most countries, however, a stillbirth was defined as "the birth of a fetus, after 28 weeks of pregnancy, in which pulmonary respiration does not occur". [12]
Ideally, all mortality estimation would be done using vital statistics and census data. Census data will give detailed information about the population at risk of death. The vital statistics provide information about live births and deaths in the population. [13] Often, either census data and vital statistics data is not available. This is common in developing countries, countries that are in conflict, areas where natural disasters have caused mass displacement, and other areas where there is a humanitarian crisis [13]
Household surveys or interviews are another way in which mortality rates are often assessed. There are several methods to estimate mortality in different segments of the population. One such example is the sisterhood method, which involves researchers estimating maternal mortality by contacting women in populations of interest and asking whether or not they have a sister, if the sister is of child-bearing age (usually 15) and conducting an interview or written questions about possible deaths among sisters. The sisterhood method, however, does not work in cases where sisters may have died before the sister being interviewed was born. [14]
Orphanhood surveys estimate mortality by questioning children are asked about the mortality of their parents. It has often been criticized as an adult mortality rate that is very biased for several reasons. The adoption effect is one such instance in which orphans often do not realize that they are adopted. Additionally, interviewers may not realize that an adoptive or foster parent is not the child's biological parent. There is also the issue of parents being reported on by multiple children while some adults have no children, thus are not counted in mortality estimates. [13]
Widowhood surveys estimate adult mortality by responding to questions about the deceased husband or wife. One limitation of the widowhood survey surrounds the issues of divorce, where people may be more likely to report that they are widowed in places where there is the great social stigma around being a divorcee. Another limitation is that multiple marriages introduce biased estimates, so individuals are often asked about first marriage. Biases will be significant if the association of death between spouses, such as those in countries with large AIDS epidemics. [13]
Sampling refers to the selection of a subset of the population of interest to efficiently gain information about the entire population. Samples should be representative of the population of interest. Cluster sampling is an approach to non-probability sampling; this is an approach in which each member of the population is assigned to a group (cluster), and then clusters are randomly selected, and all members of selected clusters are included in the sample. Often combined with stratification techniques (in which case it is called multistage sampling ), cluster sampling is the approach most often used by epidemiologists. In areas of forced migration, there is more significant sampling error . Thus cluster sampling is not the ideal choice. [15]
Causes of death vary greatly between developed and less developed countries ; [ citation needed ] see also list of causes of death by rate for worldwide statistics.
According to Jean Ziegler (the United Nations Special Rapporteur on the Right to Food for 2000 to March 2008), mortality due to malnutrition accounted for 58% of the total mortality in 2006: "In the world, approximately 62 million people, all causes of death combined, die each year. In 2006, more than 36 million died of hunger or diseases due to deficiencies in micronutrients ". [17]
Of the roughly 150,000 people who die each day across the globe, [18] [19] [20] about two thirds—100,000 per day—die of age-related causes. [21] In industrialized nations, the proportion is much higher, reaching 90%. [21]
Scholars have stated that there is a significant relationship between a low standard of living that results from low income; and increased mortality rates. A low standard of living is more likely to result in malnutrition, which can make people more susceptible to disease and more likely to die from these diseases. A lower standard of living may lead to as a lack of hygiene and sanitation, increased exposure to and the spread of disease, and a lack of access to proper medical care and facilities. Poor health can in turn contribute to low and reduced incomes, which can create a loop known as the health-poverty trap. [22] Indian economist and philosopher Amartya Sen has stated that mortality rates can serve as an indicator of economic success and failure. [23] [24] : 27, 32
Historically, mortality rates have been adversely affected by short term price increases. Studies have shown that mortality rates increase at a rate concurrent with increases in food prices . These effects have a greater impact on vulnerable, lower-income populations than they do on populations with a higher standard of living. [24] : 35–36, 70
In more recent times, higher mortality rates have been less tied to socio-economic levels within a given society, but have differed more between low and high-income countries. It is now found that national income, which is directly tied to standard of living within a country, is the largest factor in mortality rates being higher in low-income countries. [25]
These rates are especially pronounced for children under 5 years old, particularly in lower-income, developing countries. These children have a much greater chance of dying of diseases that have become very preventable in higher-income parts of the world. More children die of malaria, respiratory infections, diarrhea, perinatal conditions, and measles in developing nations. Data shows that after the age of 5 these preventable causes level out between high and low-income countries.

Mosquito control manages the population of mosquitoes to reduce their damage to human health, economies, and enjoyment. Mosquito control is a vital public-health practice throughout the world and especially in the tropics because mosquitoes spread many diseases, such as malaria and the Zika virus .
Mosquito-control operations are targeted against three different problems:
Disease organisms transmitted by mosquitoes include West Nile virus , Saint Louis encephalitis virus , Eastern equine encephalomyelitis virus , Everglades virus , Highlands J virus , La Crosse Encephalitis virus in the United States; dengue fever , yellow fever , Ilheus virus , malaria , Zika virus and filariasis in the American tropics ; Rift Valley fever , Wuchereria bancrofti , Japanese encephalitis , chikungunya and filariasis in Africa and Asia; and Murray Valley encephalitis in Australia.
Depending on the situation, source reduction, biocontrol, larviciding (killing of larvae ), or adulticiding (killing of adults) may be used to manage mosquito populations. These techniques are accomplished using habitat modification, pesticide , biological-control agents, and trapping. The advantage of non-toxic methods of control is they can be used in Conservation Areas .
Adult mosquito populations may be monitored by landing rate counts, mechanical traps or by, lidar technology [1] [2] For landing rate counts, an inspector visits a set number of sites every day, counting the number of adult female mosquitoes that land on a part of the body, such as an arm or both legs, within a given time interval. Mechanical traps use a fan to blow adult mosquitoes into a collection bag that is taken back to the laboratory for analysis of catch.  The mechanical traps use visual cues (light, black/white contrasts) or chemical attractants that are normally given off by mosquito hosts (e.g., carbon dioxide , ammonia , lactic acid , octenol ) to attract adult female mosquitoes. These cues are often used in combination. Entomology lidar detection has the possibility of showing the difference between male and female mosquitoes. [1]
Monitoring larval mosquito populations involves collecting larvae from standing water with a dipper or a turkey baster .  The habitat, approximate total number of larvae and pupae , and species are noted for each collection. An alternative method works by providing artificial breeding spots ( ovitraps ) and collecting and counting the developing larvae at fixed intervals.
Monitoring these mosquito populations is crucial to see what species are present, if mosquito numbers are rising or falling, and detecting any diseases they carry.
Since many mosquitoes breed in standing water , source reduction can be as simple as emptying water from containers around the home. This is something that homeowners can accomplish. Mosquito breeding grounds can be eliminated at home by removing unused plastic pools , old tires , or buckets ; by clearing clogged gutters and repairing leaks around faucets ; by regularly (at most every 4 days) changing water in bird baths ; and by filling or draining puddles, swampy areas, and tree stumps.  Eliminating such mosquito breeding areas can be an extremely effective and permanent way to reduce mosquito populations without resorting to insecticides. [3] However, this may not be possible in parts of the developing world where water cannot be readily replaced due to irregular water supply. Many individuals also believe mosquito control is the government's responsibility, so if these methods are not done regularly by homeowners then the effectiveness is reduced. [4]
Open water marsh management (OWMM) involves the use of shallow ditches, to create a network of water flow within marshes and to connect the marsh to a pond or canal. The network of ditches drains the mosquito habitat and lets in fish which will feed on mosquito larvae. This reduces the need for other control methods such as pesticides . Simply giving the predators access to the mosquito larvae can result in long-term mosquito control. [5] Open-water marsh management is used on both the eastern and western coasts of the United States.
Rotational impoundment management (RIM) involves the use of large pumps and culverts with gates to control the water level within an impounded marsh.  RIM allows mosquito control to occur while still permitting the marsh to function in a state as close to its natural condition as possible.  Water is pumped into the marsh in the late spring and summer to prevent the female mosquito from laying her eggs on the soil.  The marsh is allowed to drain in the fall, winter, and early spring.  Gates in the culverts are used to permit fish, crustaceans, and other marsh organisms to enter and exit the marsh.  RIM allows the mosquito-control goals to be met while at the same time reducing the need for pesticide use within the marsh.  Rotational impoundment management is used to a great extent on the east coast of Florida. [6]
Recent studies also explore the idea of using unmanned aerial vehicles as a valid strategy to identify and prioritize water bodies where disease vectors such as Ny . darlingi are most likely to breed. [7]
For the first time, a combination of the nuclear sterile insect technique (SIT) with the incompatible insect technique (IIT) was used in Mosquito Control in Guangzhou, China. The results of the recent pilot trial in Guangzhou, China, carried out with the support of the IAEA in cooperation with the Food and Agriculture Organization of the United Nations (FAO), were published in Nature on 17 July 2019. The results of this pilot trial, using SIT in combination with the IIT, demonstrate the successful near-elimination of field populations of the world's most invasive mosquito species, Aedes albopictus (Asian tiger mosquito). The two-year trial (2016–2017) covered a 32.5-hectare area on two relatively isolated islands in the Pearl River in Guangzhou. It involved the release of about 200 million irradiated mass-reared adult male mosquitoes exposed to Wolbachia bacteria. [8]
Biological pest control , or "biocontrol", is the use of the natural enemies of pests like mosquitoes to manage the pest's populations. There are several types of biocontrol, including the direct introduction of parasites, pathogens, and predators to target mosquitoes. Effective biocontrol agents include predatory fish that feed on mosquito larvae such as mosquitofish ( Gambusia affinis ) and some cyprinids (carps and minnows) and killifish . Tilapia also consume mosquito larvae. [9] Direct introduction of tilapia and mosquitofish into ecosystems around the world have had disastrous consequences. [10] However, utilizing a controlled system via aquaponics provides the mosquito control without the adverse effects to the ecosystem.
Other predators include dragonfly (fly) naiads , which consume mosquito larvae in the breeding waters, adult dragonflies , which eat adult mosquitoes, and some species of lizard and gecko . [11] Biocontrol agents that have had lesser degrees of success include the predator mosquito Toxorhynchites and predator crustaceans — Mesocyclops copepods , [12] nematodes and fungi . [13] Predators such as birds, bats, lizards, and frogs have been used, but their effectiveness is only anecdotal.
Like all animals, mosquitoes are subject to disease. Invertebrate pathologists study these diseases in the hope that some of them can be utilized for mosquito management.  Microbial pathogens of mosquitoes include viruses, bacteria, fungi, protozoa, nematodes and microsporidia. [14] [ page needed ] [15]
Dead spores of the soil bacterium Bacillus thuringiensis , especially Bt israelensis (BTI) interfere with larval digestive systems.  It can be dispersed by hand or dropped by helicopter in large areas. BTI loses effectiveness after the larvae turn into pupae, because they stop eating.
Two species of fungi can kill adult mosquitoes: Metarhizium anisopliae and Beauveria bassiana . [16]
Integrated pest management (IPM) is the use of the most environmentally appropriate method or combination of methods to control pest populations. Typical mosquito-control programs using IPM first conduct surveys, in order to determine the species composition, relative abundance and seasonal distribution of adult and larval mosquitoes, and only then is a control strategy defined.
Introducing large numbers of sterile males is another approach to reducing mosquito numbers. This is called Sterile Insect Technique (SIT). [17] Radiation is used to disrupt DNA in the mosquitoes and randomly create mutations. Males with mutations that disrupt their fertility are selected and released in mass into the wild population. These sterile males mate with wild type females and no offspring is produced, reducing the population size. [18]
Another control approach under investigation for Aedes aegypti uses a strain that is genetically modified to require the antibiotic tetracycline to develop beyond the larval stage. Modified males develop normally in a nursery while they are supplied with this chemical and can be released into the wild. However, their subsequent offspring will lack tetracycline in the wild and never mature. [19] Field trials were conducted in the Cayman Islands, Malaysia and Brazil to control the mosquitoes that cause dengue fever. In April 2014, Brazil's National Technical Commission for Biosecurity approved the commercial release of the modified mosquito. [20] [21] The FDA is the lead agency for regulating genetically-engineered mosquitoes in the United States. [22] In 2014 and 2018 research was reported into other genetic methods including cytoplasmic incompatibility, chromosomal translocations, sex distortion and gene replacement. [23] Although several years away from the field trial stage, if successful these other methods have the potential to be cheaper and to eradicate the Aedes aegypti mosquito more efficiently. [24]
A pioneering experimental demonstration of the gene drive method eradicated small populations of Anopheles gambiae . [25] [26]
In 2020, Oxitec 's OX5034 mosquito was approved for release by state and federal authorities for use in Florida in 2021 and 2022. [27] The mosquito also won federal approval to be released into Texas, beginning in 2021. [27]
This is a process of achieving sustainable mosquito control in an eco friendly manner by providing artificial breeding grounds with an ovitrap [28] or an ovillanta [29] utilizing common household utensils and destroying larvae by non-hazardous natural means such as throwing them in dry places or feeding them to larvae eating fishes like Gambusia affinis , or suffocating them by spreading a thin plastic sheet over the entire water surface to block atmospheric air. Shifting the water with larvae to another vessel and pouring a few drops of kerosene oil or insecticide/larvicide in it is another option for killing wrigglers, but not preferred due to its environmental impact . Most of the ornamental fishes eat mosquito larvae.
In several experiments, researchers utilized mosquito traps. [30] This process allowed both the opportunity to determine which mosquitoes were affected, and provided a group to be re-released with genetic modifications resulting in the OX513A variant to reduce reproduction. Adult mosquitoes are attracted inside the trap where they die of dehydration.
Instead of chemical insecticides, some researchers are studying biocides. Most notably, scientists in Burkina Faso were studying the Metarhizium fungal species. This fungus in a high concentration can slowly kill mosquitoes. To increase the lethality of the fungus, a gene from a spider was inserted into the fungus causing it to produce a neurotoxin. But it is only activated when in mosquito hemolymph. Malicious research was done to show the fungi would not affect other insects or humans. [31] [32] [33] [34]
An oil drip can or oil drip barrel was a common and nontoxic antimosquito measure. [35] [36] [37] [38] [39] [40] The thin layer of oil on top of the water prevents mosquito breeding in two ways: [41] mosquito larvae in the water cannot penetrate the oil film with their breathing tube, and so drown and die; also adult mosquitoes do not lay eggs on the oiled water.
Control of larvae can be accomplished through use of contact poisons, growth regulators, surface films, stomach poisons (including bacterial agents), and biological agents such as fungi, nematodes, copepods, and fish. [42] A chemical commonly used in the United States is methoprene , considered slightly toxic to larger animals, which mimics and interferes with natural growth hormones in mosquito larvae, preventing development. Methoprene is frequently distributed in time-release briquette form in breeding areas.
It is believed by some researchers that the larvae of Anopheles gambiae (important vectors of malaria) can survive for several days on moist mud, and that treatments should therefore include mud and soil several meters from puddles. [43]
Control of adult mosquitoes is the most familiar aspect of mosquito control to most of the public. It is accomplished by ground-based applications or via aerial application [44] of residual chemical insecticides such as Duet . Generally modern mosquito-control programs in developed countries use low-volume applications of insecticides, although some programs may still use thermal fogging. Beside fogging there are some other insect repellents for indoors and outdoors. An example of a synthetic insect repellent is DEET . A naturally occurring repellent is citronella . Indoor Residual Spraying ( IRS ) is another method of adulticide. Walls of properties are sprayed with an insecticide, the mosquitoes die when they land on the surface covered in insecticide. [45]
To control adult mosquitoes in India, van mounted fogging machines and hand fogging machines are used. [46] [47] [48]
DDT was formerly used throughout the world for large area mosquito control, but it is now banned in most developed countries. [49]
Controversially, DDT remains in common use in many developing countries (14 countries were reported to be using it in 2009 [49] ), which claim that the public-health cost of switching to other control methods would exceed the harm caused by using DDT. It is sometimes approved for use only in specific, limited circumstances where it is most effective, such as application to walls.
The role of DDT in combating mosquitoes has been the subject of considerable controversy. Although DDT has been proven to affect biodiversity and cause eggshell thinning in birds such as the bald eagle, some say that DDT is the most effective weapon in combating mosquitoes, and hence malaria. While some of this disagreement is based on differences in the extent to which disease control is valued as opposed to the value of biodiversity, [50] there is also genuine disagreement amongst experts about the costs and benefits of using DDT. [ dubious – discuss ]
Notwithstanding, DDT-resistant mosquitoes have started to increase in numbers, especially in tropics due to mutations, reducing the effectiveness of this chemical; these mutations can rapidly spread over vast areas if pesticides are applied indiscriminately (Chevillon et al. 1999). In areas where DDT resistance is encountered, malathion , propoxur or lindane is used.

A traditional approach to controlling mosquito populations is the use of ovitraps or lethal ovitraps , which provide artificial breeding spots for mosquitoes to lay their eggs. While ovitraps only trap eggs, lethal ovitraps usually contain a chemical inside the trap that is used to kill the adult mosquito and/or the larvae in the trap. Studies have shown that with enough of these lethal ovitraps, Aedes mosquito populations can be controlled. [51] A recent approach is the automatic lethal ovitrap, which works like a traditional ovitrap but automates all steps needed to provide the breeding spots and to destroy the developing larvae. [52]
In 2016 researchers from Laurentian University released a design for a low cost trap called an Ovillanta which consists of attractant-laced water in a section of discarded rubber tire.  At regular intervals the water is run through a filter to remove any deposited eggs and larva. The water, which then contains an 'oviposition' pheromone deposited during egg-laying, is reused to attract more mosquitoes.  Two studies have shown that this type of trap can attract about seven times as many mosquito eggs as a conventional ovitrap. [53] [54] [55] [56]
Some newer mosquito traps or known mosquito attractants emit a plume of carbon dioxide together with other mosquito attractants such as sugary scents, lactic acid , octenol , warmth, water vapor and sounds. [57] By mimicking a mammal's scent and outputs, the trap draws female mosquitoes toward it, where they are typically sucked into a net or holder by an electric fan where they are collected. According to the American Mosquito Control Association, the trap will kill some mosquitoes, but their effectiveness in any particular case will depend on a number of factors such as the size and species of the mosquito population and the type and location of the breeding habitat.  They are useful in specimen collection studies to determine the types of mosquitoes prevalent in an area but are typically far too inefficient to be useful in reducing mosquito populations.
Research is being conducted that indicates that dismantling a protein associated with eggshell organization, factor EOF1 (factor 1), which may be unique to mosquitoes, may be a means to hamper their reproduction effectively in the wild without creating a resistant population or affecting other animals. [58] [59]
Some biologists have proposed the deliberate extinction of certain mosquito species. Biologist Olivia Judson has advocated " specicide " of thirty mosquito species by introducing a genetic element which can insert itself into another crucial gene, to create recessive " knockout genes ". [60] She says that the Anopheles mosquitoes (which spread malaria ) and Aedes mosquitoes (which spread dengue fever , yellow fever , elephantiasis , zika , and other diseases) represent only 30 out of some 3,500 mosquito species; eradicating these would save at least one million human lives per year, at a cost of reducing the genetic diversity of the family Culicidae by 1%. She further argues that since species become extinct "all the time" the disappearance of a few more will not destroy the ecosystem : "We're not left with a wasteland every time a species vanishes. Removing one species sometimes causes shifts in the populations of other species — but different need not mean worse." In addition, anti-malarial and mosquito control programs offer little realistic hope to the 300 million people in developing nations who will be infected with acute illnesses this year. Although trials are ongoing, she writes that if they fail: "We should consider the ultimate swatting." [60]
Biologist E. O. Wilson has advocated the extinction of several species of mosquito, including malaria vector Anopheles gambiae . Wilson stated, "I'm talking about a very small number of species that have co-evolved with us and are preying on humans, so it would certainly be acceptable to remove them. I believe it's just common sense." [61]
Insect ecologist Steven Juliano has argued that "it's difficult to see what the downside would be to removal, except for collateral damage". Entomologist Joe Conlon stated that "If we eradicated them tomorrow, the ecosystems where they are active will hiccup and then get on with life. Something better or worse would take over." [62]
However, David Quammen has pointed out that mosquitoes protect forests from human exploitation and may act as competitors for other insects. [63] In terms of malaria control, if populations of mosquitoes were temporarily reduced to zero in a region, then this would exterminate malaria, and the mosquito population could then be allowed to rebound. [64]

Worldwide it was estimated that 1.25 million people were killed and many millions more were injured in motor vehicle collisions in 2013. [2] This makes motor vehicle collisions the leading cause of death among young adults of 15–29 years of age (360,000 die a year) and the ninth cause of death for all ages worldwide. [3] In the United States, 40,100 people died and 2.8 million were injured in crashes in 2017, [4] and around 2,000 children under 16 years old die every year. [5] It is estimated that motor vehicle collisions caused the deaths of around 60 million people during the 20th century, [6] around the same as the number of World War II casualties .
At worldwide level, traffic accidents is the main cause of unnatural death. [ citation needed ]
Road toll figures in developed nations show that car collision fatalities have declined since 1980. Japan is an extreme example , with road deaths decreasing to 5,115 in 2008, which is 25% of the 1970 rate per capita and 17% of the 1970 rate per vehicle distance travelled. In 2008, for the first time, more pedestrians than vehicle occupants were killed in Japan by cars. [7] Besides improving general road conditions like lighting and separated walkways, Japan has been installing intelligent transportation system technology such as stalled-car monitors to avoid crashes.
In developing nations, statistics may be grossly inaccurate or hard to get. Some nations have not significantly reduced the total death rate, which stands at 12,000 in Thailand in 2007, for example. [8]
In the United States, twenty-eight states had reductions in the number of automobile crash fatalities between 2005 and 2006. [9] 55% of vehicle occupants 16 years or older in 2006 were not using seat belts when they crashed. [10]
Road fatality trends tend to follow Smeed's law , [11] an empirical schema that correlates increased fatality rates per capita with traffic congestion .
Crashes are categorized by what is struck and the direction of impact, or impacts. These are some common crash types, based on the total number that occurred in the US in 2005, the percentage of total crashes, and the percentage of fatal crashes: [14]
Rollover, head-on, pedestrian, and bicyclist crashes combined are only 6.1% of all crashes, but cause 34.5% of traffic-related fatalities.
Sometimes the vehicles in the collision can suffer more than one type of impact, such as during a shunt or high-speed spin. This is called a "second harmful event," such as when a vehicle is redirected by the first crash into another vehicle or fixed object.
(thousands of km 2 )
(millions)
(thousands)
of road network (kilometers)
(millions of vehicles x km)
inhabitants
Representation of regional death statistics on map reveals significant differences even between neighboring regions. [17]
The 28 EU-28 countries, for the 28 members, computed an indicator named "per 10 billion pkm".
Pkm is an indicator of traffic volume which is used for not having consistent vehicle-kilometre data. Are counted cars and estimated motorised two-wheelers. In 2016, this indicator ranges from 23 for Sweden to 192 for Romania, with a value of 52 for the EU-28.  In Germany, France, the UK and Italy, this score is respectively 33, 46, 28, 44. [18]
In 2019, the 27 members states of the European Union had 51 road deaths per million inhabitants. [19] Because the UK had less fatalities than the average EU and due to Brexit; this rate raised to 51. Including the UK, the rate of the 28 would have been 48. [20]
The safest of those 28 nations was Sweden (22 deaths/million inhabitants) while Romania reported the highest fatality rates of the EU in 2019. (96/million). [21]
In 2019, the nhtsa counted 36,096 fatalities in motor vehicle traffic crashes, that is 1.10 fatalities per 100 million vehicle miles traveled. [22]
Nations:
Mutual Standardisation is a term used within spatial epidemiology to refer to when ecological bias results as a consequence of adjusting disease rates for confounding at the area level but leaving the exposure unadjusted and vice versa. This bias is prevented by adjusting in the same way both the exposure and disease rates. This adjustment is rarely possible as it requires data on within-area distribution of the exposure and confounder variables. (Elliot, 2001)
The National Institute for Communicable Diseases ( NICD ) is a national public health institute of South Africa , [1] [2] providing reference to microbiology , virology , epidemiology , surveillance and public health research to support the government's response to communicable disease threats. [3] [4]
The NICD serves as a resource of knowledge and expertise of communicable diseases to the South African Government , Southern African Development Community countries and the African continent . The institution assists in the planning of policies and programmes to support and respond to communicable diseases. [5]
The main goal of the NICD is to be the national organ for South Africa for public health surveillance of communicable disease. [6]

This article about South African government is a stub . You can help Wikipedia by expanding it .
The National Outbreak Reporting System (NORS) is an electronic, web-accessible system designed to improve the quality, quantity, and availability of data for waterborne , foodborne , enteric person-to-person, and enteric zoonotic (animal-to-person) disease outbreaks in the United States.
NORS launched in 2009 for use by staff working within public health departments in individual states, territories, and the Freely Associated States (composed of the Republic of the Marshall Islands , the Federated States of Micronesia and the Republic of Palau ; formerly parts of the U.S.-administered Trust Territories of the Pacific Islands). Health departments are responsible for determining which staff members have access to NORS.
NORS replaced the electronic Foodborne Outbreak Reporting System (eFORS) - the primary tool for reporting foodborne disease outbreaks to the U.S. Centers for Disease Control and Prevention (CDC) since 2001. [1] NORS also replaced the paper-based reporting system used during 1971-2008 to report waterborne disease outbreaks to the Waterborne Disease and Outbreak Reporting System (WBDOSS) . [2] [3] The transition to electronic waterborne disease outbreak reporting is in large part a response to the Council of State and Territorial Epidemiologists (CSTE) position statement titled " Improving Detection, Investigation, and Reporting of Waterborne Disease Outbreaks ."
Separate sections in NORS for enteric person-to-person and animal-to-person disease outbreak reports are intended to enhance the information available to quantify, describe and understand these types of outbreaks at a national level.
Waterborne Disease and Outbreak Reporting System (WBDOSS)
A natural experiment is an empirical study in which individuals (or clusters of individuals) are exposed to the experimental and control conditions that are determined by nature or by other factors outside the control of the investigators. The process governing the exposures arguably resembles random assignment . Thus, natural experiments are observational studies and are not controlled in the traditional sense of a randomized experiment (an intervention study ). Natural experiments are most useful when there has been a clearly defined exposure involving a well defined subpopulation (and the absence of exposure in a similar subpopulation) such that changes in outcomes may be plausibly attributed to the exposure. [1] [2] In this sense, the difference between a natural experiment and a non-experimental observational study is that the former includes a comparison of conditions that pave the way for causal inference , but the latter does not.
Natural experiments are employed as study designs when controlled experimentation is extremely difficult to implement or unethical, such as in several research areas addressed by epidemiology (like evaluating the health impact of varying degrees of exposure to ionizing radiation in people living near Hiroshima at the time of the atomic blast [3] ) and economics (like estimating the economic return on amount of schooling in US adults [4] ). [1] [2]
One of the best-known early natural experiments was the 1854 Broad Street cholera outbreak in London , England. On 31 August 1854, a major outbreak of cholera struck Soho . Over the next three days, 127 people near Broad Street died. By the end of the outbreak 616 people died. The physician John Snow identified the source of the outbreak as the nearest public water pump, using a map of deaths and illness that revealed a cluster of cases around the pump.
In this example, Snow discovered a strong association between the use of the water from the pump, and deaths and illnesses due to cholera. Snow found that the Southwark and Vauxhall Waterworks Company , which supplied water to districts with high attack rates, obtained the water from the Thames downstream from where raw sewage was discharged into the river. By contrast, districts that were supplied water by the Lambeth Waterworks Company , which obtained water upstream from the points of sewage discharge, had low attack rates. Given the near-haphazard patchwork development of the water supply in mid-nineteenth century London, Snow viewed the developments as "an experiment...on the grandest scale." [5] Of course, the exposure to the polluted water was not under the control of any scientist. Therefore, this exposure has been recognized as being a natural experiment. [6] [7] [8]
An aim of a study Angrist and Evans (1998) [9] was to estimate the effect of family size on the labor market outcomes of the mother. For at least two reasons, the correlations between family size and various outcomes (e.g., earnings) do not inform us about how family size causally affects labor market outcomes. First, both labor market outcomes and family size may be affected by unobserved "third" variables (e.g., personal preferences). Second, labor market outcomes themselves may affect family size (called "reverse causality"). For example, a woman may defer having a child if she gets a raise at work. The authors observed that two-child families with either two boys or two girls are substantially more likely to have a third child than two-child families with one boy and one girl. The sex of the first two children, then, constitutes a kind of natural experiment: it is as if an experimenter had randomly assigned some families to have two children and others to have three. The authors were then able to credibly estimate the causal effect of having a third child on labor market outcomes. Angrist and Evans found that childbearing had a greater impact on poor and less educated women than on highly educated women although the earnings impact of having a third child tended to disappear by that child's 13th birthday. They also found that having a third child had little impact on husbands' earnings. [9]
Within economics, game shows are a frequently studied form of natural experiment. While game shows might seem to be artificial contexts, they can be considered natural experiments due to the fact that the context arises without interference of the scientist. Game shows have been used to study a wide range of different types of economic behavior, such as decision making under risk [10] and cooperative behavior. [11]
In Helena, Montana a smoking ban was in effect in all public spaces, including bars and restaurants, during the six-month period from June 2002 to December 2002. Helena is geographically isolated and served by only one hospital. The investigators observed that the rate of heart attacks dropped by 40% while the smoking ban was in effect. Opponents of the law prevailed in getting the enforcement of the law suspended after six months, after which the rate of heart attacks went back up. [12] This study was an example of a natural experiment, called a case-crossover experiment , where the exposure is removed for a time and then returned. The study also potentially suggests that the inability to control variables in natural experiments can impede investigators from drawing firm conclusions. Critics argued that the particularly large percentage fluctuation in the rate of myocardial infarction was likely due to chance, given the small population size. [13]
Nuclear weapons testing released large quantities of radioactive isotopes into the atmosphere, some of which could be incorporated into biological tissues. The release stopped after the Partial Nuclear Test Ban Treaty in 1963, which prohibited atmospheric nuclear tests. This resembled a large-scale pulse-chase experiment , but could not have been performed as a regular experiment in humans due to scientific ethics. Several types of observations were made possible (in people born before 1963), such as determination of the rate of replacement for cells in different human tissues.
An important question in economics research is what determines earnings. Angrist (1990) evaluated the effects of military service on lifetime earnings. [14] Using statistical methods developed in econometrics , [15] Angrist capitalized on the approximate random assignment of the Vietnam War draft lottery , and used it as an instrumental variable associated with eligibility (or non-eligibility) for military service. Because many factors might predict whether someone serves in the military, the draft lottery frames a natural experiment whereby those drafted into the military can be compared against those not drafted because the two groups should not differ substantially prior to military service. Angrist found that the earnings of veterans were, on average, about 15 percent less than the earnings of non-veterans.
With the Industrial Revolution in the nineteenth century, many species of moth, including the well-studied peppered moth , responded to the atmospheric pollution of sulphur dioxide and soot around cities with industrial melanism , a dramatic increase in the frequency of dark forms over the formerly abundant pale, speckled forms. In the twentieth century, as regulation improved and pollution fell, providing the conditions for a large-scale natural experiment, the trend towards industrial melanism was reversed, and melanic forms quickly became scarce. The effect led the evolutionary biologists L. M. Cook and J. R. G. Turner to conclude that " natural selection is the only credible explanation for the overall decline". [16]
The natural history of disease is the course a disease takes in individual people from its pathological onset ("inception") until its resolution (either through complete recovery or eventual death). [1] The inception of a disease is not a firmly defined concept. [1] The natural history of a disease is sometimes said to start at the moment of exposure to causal agents . [2] Knowledge of the natural history of disease ranks alongside causal understanding in importance for disease prevention and control. Natural history of disease is one of the major elements of descriptive epidemiology . [2]
This medical article is a stub . You can help Wikipedia by expanding it .
In infectious disease ecology and epidemiology , a natural reservoir , also known as a disease reservoir or a reservoir of infection , is the population of organisms or the specific environment in which an infectious pathogen naturally lives and reproduces, or upon which the pathogen primarily depends for its survival. A reservoir is usually a living host of a certain species, such as an animal or a plant, inside of which a pathogen survives, often (though not always) without causing disease for the reservoir itself. By some definitions a reservoir may also be an environment external to an organism, such as a volume of contaminated air or water. [1] [2]
Because of the enormous variety of infectious microorganisms capable of causing disease, precise definitions for what constitutes a natural reservoir are numerous, various, and often conflicting. The reservoir concept applies only for pathogens capable of infecting more than one host population and only with respect to a defined target population – the population of organisms in which the pathogen causes disease. The reservoir is any population of organisms (or any environment) which harbors the pathogen and transmits it to the target population. Reservoirs may comprise one or more different species, may be the same or a different species as the target, and, in the broadest sense, may include vector species , [2] which are otherwise distinct from natural reservoirs. Significantly, species considered reservoirs for a given pathogen may not experience symptoms of disease when infected by the pathogen.
Identifying the natural reservoirs of infectious pathogens has proven useful in treating and preventing large outbreaks of disease in humans and domestic animals, especially those diseases for which no vaccine exists. In principle, zoonotic diseases can be controlled by isolating or destroying the pathogen's reservoirs of infection. The mass culling of animals confirmed or suspected as reservoirs for human pathogens, such as birds that harbor avian influenza , has been effective at containing possible epidemics in many parts of the world; for other pathogens, such as the ebolaviruses , the identity of the presumed natural reservoir remains obscure.
The great diversity of infectious pathogens, their possible hosts, and the ways in which their hosts respond to infection has resulted in multiple definitions for "natural reservoir", many of which are conflicting or incomplete. In a 2002 conceptual exploration published in the CDC 's Emerging Infectious Diseases , the natural reservoir of a given pathogen is defined as "one or more epidemiologically connected populations or environments in which the pathogen can be permanently maintained and from which infection is transmitted to the defined target population." [2] The target population is the population or species in which the pathogen causes disease; it is the population of interest because it suffers from disease when infected by the pathogen (for example, humans are the target population in most medical epidemiological studies). [3]
A common criterion in other definitions distinguishes reservoirs from non-reservoirs by the degree to which the infected host shows symptoms of disease. By these definitions, a reservoir is a host that does not experience the symptoms of  disease when infected by the pathogen, whereas non-reservoirs show symptoms of the disease. [4] The pathogen still feeds, grows, and reproduces inside a reservoir host, but otherwise does not significantly affect its health; the relationship between pathogen and reservoir is more or less commensal , whereas in susceptible hosts that do suffer disease caused by the pathogen, the pathogen is considered parasitic .
What further defines a reservoir for a specific pathogen is where it can be maintained and from where it can be transmitted. A "multi-host" organism is capable of having more than one natural reservoir.
Natural reservoirs can be divided into three main types: human, animal (non-human), and environmental. [1]
Human reservoirs are human beings infected by pathogens that exist on or within the human body. [1] Poliomyelitis and smallpox exist exclusively within a human reservoir. [5] Humans can act as reservoirs for sexually transmitted diseases, measles, mumps, streptococcal infection, various respiratory pathogens, and the smallpox virus. [1]
Animal (non-human) reservoirs consist of domesticated and wild animals infected by pathogens. [1] [2] For example, the bacterium Vibrio cholerae , which causes cholera in humans, has natural reservoirs in copepods , zooplankton , and shellfish . Parasitic blood-flukes of the genus Schistosoma , responsible for schistosomiasis , spend part of their lives inside freshwater snails before completing their life cycles in vertebrate hosts. [7] Viruses of the taxon Ebolavirus , which causes Ebola virus disease , are thought to have a natural reservoir in bats or other animals exposed to the virus. [8] Other zoonotic diseases that have been transmitted from animals to humans include: rabies , blastomycosis , psittacosis , trichinosis , cat-scratch disease , histoplasmosis , coccidiomycosis and salmonella . [9]
Common animal reservoirs include: bats, rodents, cows, pigs, sheep, swine, rabbits, raccoons, dogs, other mammals. [1]
Numerous zoonotic diseases have been traced back to bats. [10] There are a couple theories that serve as possible explanations as to why bats carry so many viruses. One proposed theory is that there exist so many bat-borne illnesses because there exist a large amount of bat species and individuals. The second possibility is that something about bats' physiology make them especially good reservoir hosts. [10] Perhaps bats' "food choices, population structure, ability to fly, seasonal migration and daily movement patterns, torpor and hibernation, life span, and roosting behaviors" are responsible for making them especially suitable reservoir hosts. [11] Lyssaviruses (including the Rabies virus ), Henipaviruses , Menangle and Tioman viruses, SARS-CoV -Like Viruses, and Ebola viruses have all been traced back to different species of bats. [11] Fruit bats in particular serve as the reservoir host for Nipah virus (NiV). [12]
Rats are known to be the reservoir hosts for a number of zoonotic diseases . Norway rats were found to be infested with the Lyme disease spirochetes. [13] In Mexico rats are known carriers of Trypanosoma cruzi , which causes Chagas disease . [14]
White-footed mice ( Peromyscus leucopus ) are one of the most important animal reservoirs for the Lyme disease spirochete ( Borrelia burgdorferi ). [15] Deer mice serve as reservoir hosts for Sin Nombre virus , which causes Hantavirus Pulmonary Syndrome (HPS). [16]
The Zika virus originated from monkeys in Africa. In São José do Rio Preto and Belo Horizonte, Brazil the zika virus has been found in dead monkeys. Genome sequencing  has revealed the virus to be very similar to the type that infects humans. [17]
Environmental reservoirs include living and non-living reservoirs that harbor infectious pathogens outside the bodies of animals. These reservoirs may exist on land (plants and soil), in water, or in the air. [1] Pathogens found in these reservoirs are sometimes free-living. The bacteria Legionella pneumophila , a facultative intracellular parasite which causes Legionnaires' disease , and Vibrio cholerae , which causes cholera , can both exist as free-living parasites in certain water sources as well as in invertebrate animal hosts. [1] [18]
A disease reservoir acts as a transmission point between a pathogen and a susceptible host. [1] Transmission can occur directly or indirectly.
Direct transmission can occur from direct contact or direct droplet spread. Direct contact transmission between two people can happen through skin contact, kissing, and sexual contact. Humans serving as disease reservoirs can be symptomatic (showing illness) or asymptomatic (not showing illness), act as disease carriers, and often spread illness unknowingly. Human carriers commonly transmit disease because they do not realize they are infected, and consequently take no special precautions to prevent transmission. Symptomatic persons who are aware of their illness are not as likely to transmit infection because they take precautions to reduce possible transmission of the disease and/or seek out treatment to prevent the spread of the disease. [1] Direct droplet spread is due to solid particles or liquid droplet suspended in air for some time. Droplet spread is considered the transmission of the pathogen to susceptible host within a meter of distance, they can spread from coughing, sneezing, and talking.
Indirect transmission can occur by airborne transmission, by vehicles (including fomites), and by vectors.
Airborne transmission is different from direct droplet spread as it is defined as disease transmission that takes place over a distance larger than a meter. Pathogens that can be transmitted through airborne sources are carried by particles such as dust or dried residue (referred to as droplet nuclei).
Vehicles such as food, water, blood and fomites can act as passive transmission points between reservoirs and susceptible hosts. Fomites are inanimate objects (doorknobs, medical equipment, etc.) that become contaminated by a reservoir source or someone/something that is a carrier. A vehicle, like a reservoir, may also be a favorable environment for the growth of an infectious agent, as coming into contact with a vehicle leads to its transmission.
Vector transmission occurs most often from insect bites from mosquitoes, flies, fleas, and ticks. There are two sub-categories of vectors: mechanical (an insect transmits the pathogen to a host without the insect itself being affected) and biological (reproduction of the pathogen occurs within the vector before the pathogen is transmitted to a host). To give a few examples, Morbillivirus ( measles ) is transmitted from an infected human host to a susceptible host as they are transmitted by respiration through airborne transmission. Campylobacter ( campylobacteriosis ) is a common bacterial infection that is spread from human or non-human reservoirs by vehicles such as contaminated food and water. Plasmodium falciparum ( malaria ) can be transmitted from an infected mosquito, an animal (non-human) reservoir, to human host by biological vector transmission.
LH Taylor found that 61% of all human pathogens are classified as zoonotic. [19] Thus, the identification of the natural reservoirs of pathogens prior to zoonosis would be incredibly useful from a public health standpoint. Preventive measures can be taken to lessen the frequency of outbreaks, such as vaccinating the animal sources of disease or preventing contact with reservoir host animals. [20] In an effort to predict and prevent future outbreaks of zoonotic diseases, the U.S. Agency for International Development started the Emerging Pandemic Threats initiative in 2009. In alliance with University of California-Davis , EcoHealth Alliance , Metabiota Inc., Smithsonian Institution , and Wildlife Conservation Society with support from Columbia and Harvard universities , the members of the PREDICT project are focusing on the "detection and discovery of zoonotic diseases at the wildlife-human interface." [21] There are numerous other organizations around the world experimenting with different methods to predict and identify reservoir hosts. Researchers at the University of Glasgow created a machine learning algorithm that is designed to use "viral genome sequences to predict the likely natural host for a broad spectrum of RNA viruses, the viral group that most often jumps from animals to humans." [20]
Neuroepidemiology is a branch of epidemiology involving the study of neurological disease distribution and determinants of frequency in human populations. The term was first introduced by Dr. Len Kurland, Dr. Milton Alter and Dr. John F. Kurtzke in 1967. [1] Traditionally, neuroepidemiology has been perceived for a long time as a science of incidence , prevalence , risk factors , natural history and prognosis of neurological disorders. However, this is only one part of neuroepidemiology, called non-experimental neuroepidemiology. The other integral, but commonly forgotten, part of neuroepidemiology is experimental neuroepidemiology, which is research based on clinical trials of effectiveness or efficacy of various interventions in neurological disorders.
In 1982, Karger set up a new journal entitled " [2] ". [3] This periodical is the only international journal devoted to the study of neurological disease distribution and determinants of frequency in human populations.
Since the time of its inception in 1982, the scope of "Neuroepidemiology" journal has evolved considerably. At present, the journal publishes manuscripts on all aspects of epidemiology of neurological disorders, including clinical trials and systematic reviews . [4] Its primary focus is on chronic and acute neurological disorders of major importance to clinical medicine, public health , and health care delivery . The journal also welcomes manuscripts dealing with methodological issues in neuroepidemiological studies. In 2017, Dr. George Jelinek , Head of the Neuroepidemiology Unit at The University of Melbourne, and founder of Overcoming Multiple Sclerosis , LTD, an Australian not-for-profit organization [5] was posted as Specialty Chief Editor of Frontiers in Neurology section.
To reflect modern achievements in our knowledge in non-experimental and experimental (clinical trials) epidemiology of neurological disorders, the First International Congress of Clinical Neuroepidemiology is planned to be held in 2009. This International Congress, for the first time, will bring together scientists and experts in all major fields of experimental and non-experimental neuroepidemiology. Combining scientific sessions in these two interrelated fields of neuroepidemiology with two corresponding half-day teaching courses and a one year-free on-line subscription to the journal of "Neuroepidemiology" for all registered participants are additional unique features of the Congress.
The main topics of the Congress that will be addressed during plenary, platform and poster sessions include stroke, TIA, dementia, Parkinson’s disease, multiple sclerosis, epilepsy, migraine, traumatic brain injury , peripheral neuropathy, neuromuscular disorders, central nervous system infections and tumours, neurological aspects of aging, neuropsychology and neuropsychiatric disorders. The Congress will feature internationally recognized invited speakers, platform lectures, short oral presentations and poster sessions, and will provide an ideal platform for continuing education in all fields of experimental and non-experimental clinical neuroepidemiology.
To reflect modern achievements in our knowledge in non-experimental and experimental (clinical trials) epidemiology of neurological disorders, the First International Congress of Clinical Neuroepidemiology will be held in Munich, Germany on August 27–30, 2009 (www.neuro2009.com). This International Congress, for the first time, will bring together scientists and experts in all major fields of experimental and non-experimental neuroepidemiology. Combining scientific sessions in these two interrelated fields of neuroepidemiology with two corresponding half-day teaching courses and a one year-free on-line subscription to the journal of "Neuroepidemiology" for all registered participants are additional unique features of the Congress.
The main topics of the Congress that will be addressed during plenary, platform and poster sessions include stroke, TIA, dementia, Parkinson’s disease, multiple sclerosis, epilepsy, migraine, traumatic brain injury, peripheral neuropathy, neuromuscular disorders, central nervous system infections and tumours, neurological aspects of aging, neuropsychology and neuropsychiatric disorders. The Congress will feature internationally recognized invited speakers, platform lectures, short oral presentations and poster sessions, and will provide an ideal platform for continuing education in all fields of experimental and non-experimental clinical neuroepidemiology.
Several institutions in the United States offer formal training and research experience in neuroepidemiology, including:
In addition, the Center for Stroke Research in the Department of Neurology and Rehabilitation at the University of Illinois College of Medicine offers a fellowship in neuroepidemiology. [9] Michigan State University also offers a neuroepidemiology fellowship as part of the International Neurologic & Psychiatric Epidemiology Program. [10]
As the field of neuroepidemiology continues to expand, research groups have developed at some of the leading medical research institutes across the United States. Currently active research groups can be found at:
Other prominent organizations such as the National Institute of Environmental Health Sciences and Kaiser Permanente have established research programs in neuroepiemiology. [15] [16]
The American Academy of Neurology provides additional information on career paths in neuroepidemiology. [17]
[2] Neuroepidemiology University of Pittsburgh, Graduate School of Public Health, Department of Epidemiology
Newborn screening ( NBS ) is a public health program of screening in infants shortly after birth for conditions that are treatable, but not clinically evident in the newborn period. The goal is to identify infants at risk for these conditions early enough to confirm the diagnosis and provide intervention that will alter the clinical course of the disease and prevent or ameliorate the clinical manifestations.  NBS started with the discovery that the amino acid disorder phenylketonuria (PKU) could be treated by dietary adjustment, and that early intervention was required for the best outcome.  Infants with PKU appear normal at birth, but are unable to metabolize the essential amino acid phenylalanine , resulting in irreversible intellectual disability .  In the 1960s, Robert Guthrie developed a simple method using a bacterial inhibition assay that could detect high levels of phenylalanine in blood shortly after a baby was born. Guthrie also pioneered the collection of blood on filter paper which could be easily transported, recognizing the need for a simple system if the screening was going to be done on a large scale. Newborn screening around the world is still done using similar filter paper.  NBS was first introduced as a public health program in the United States in the early 1960s, and has expanded to countries around the world.
Screening programs are often run by state or national governing bodies with the goal of screening all infants born in the jurisdiction for a defined panel of treatable disorders. The number of diseases screened for is set by each jurisdiction, and can vary greatly. Most NBS tests are done by measuring metabolites or enzyme activity in whole blood samples collected on filter paper. Bedside tests for hearing loss using automated auditory brainstem response and congenital heart defects using pulse oximetry are included in some NBS programs. Infants who screen positive undergo further testing to determine if they are truly affected with a disease or if the test result was a false positive . Follow-up testing is typically coordinated between geneticists and the infant's pediatrician or primary care physician .
Robert Guthrie is given much of the credit for pioneering the earliest screening for phenylketonuria in the late 1960s using a bacterial inhibition assay (BIA) to measure phenylalanine levels in blood samples obtained by pricking a newborn baby's heel on the second day of life on filter paper . [1] Congenital hypothyroidism was the second disease widely added in the 1970s. [2] Guthrie and colleagues also developed bacterial inhibition assays for the detection of maple syrup urine disease and classic galactosemia . [3] The development of tandem mass spectrometry (MS/MS) screening in the early 1990s led to a large expansion of potentially detectable congenital metabolic diseases that can be identified by characteristic patterns of amino acids and acylcarnitines . [4] In many regions, Guthrie's BIA has been replaced by MS/MS profiles, however the filter paper he developed is still used worldwide, and has allowed for the screening of millions of infants around the world each year. [5]
In the United States , the American College of Medical Genetics recommended a uniform panel of diseases that all infants born in every state should be screened for.  They also developed an evidence-based review process for the addition of conditions in the future.  The implementation of this panel across the United States meant all babies born would be screened for the same number of conditions. This recommendation is not binding for individual states, and some states may screen for disorders that are not included on this list of recommended disorders. Prior to this, babies born in different states had received different levels of screening.  On April 24, 2008, President George W. Bush signed into law the Newborn Screening Saves Lives Act of 2007 . This act was enacted to increase awareness among parents, health professionals, and the public on testing newborns to identify certain disorders. It also sought to improve, expand, and enhance current newborn screening programs at the state level.
Newborn screening programs initially used screening criteria based largely on criteria established by JMG Wilson and F. Jungner in 1968. [6] Although not specifically about newborn population screening programs, their publication, Principles and practice of screening for disease proposed ten criteria that screening programs should meet before being used as a public health measure.  Newborn screening programs are administered in each jurisdiction, with additions and removals from the panel typically reviewed by a panel of experts.  The four criteria from the publication that were relied upon when making decisions for early newborn screening programs were:
As diagnostic techniques have progressed, debates have arisen as to how screening programs should adapt. Tandem mass spectrometry has greatly expanded the potential number of diseases that can be detected, even without satisfying all of the other criteria used for making screening decisions. [7] [8] Duchenne muscular dystrophy is a disease that has been added to screening programs in several jurisdictions around the world, despite the lack of evidence as to whether early detection improves the clinical outcome for a patient. [7]
Newborn screening is intended as a public health program to identify infants with treatable conditions before they present clinically, or suffer irreversible damage.  Phenylketonuria (PKU) was the first disorder targeted for newborn screening, being implemented in a small number of hospitals and quickly expanding across the United States and the rest of the world. [9] After the success of newborn screening for PKU (39 infants were identified and treated in the first two years of screening, with no false negative results), Guthrie and others looked for other disorders that could be identified and treated in infants, eventually developing bacterial inhibition assays to identify classic galactosemia and maple syrup urine disease . [9] [10]
Newborn screening has expanded since the introduction of PKU testing in the 1960s, but can vary greatly between countries.  In 2011, the United States screened for 54 conditions, Germany for 12, the United Kingdom for 2 (PKU and medium chain acyl-CoA dehydrogenase deficiency (MCADD)), while France and Hong Kong only screened for one condition (PKU and congenital hypothyroidism, respectively). [11] The conditions included in newborn screening programs around the world vary greatly, based on the legal requirements for screening programs, prevalence of certain diseases within a population, political pressure, and the availability of resources for both testing and follow-up of identified patients.
Newborn screening originated with an amino acid disorder, phenylketonuria (PKU), which can be easily treated by dietary modifications, but causes severe Intellectual disability if not identified and treated early. Robert Guthrie introduced the newborn screening test for PKU in the early 1960s. [12] With the knowledge that PKU could be detected before symptoms were evident, and treatment initiated, screening was quickly adopted around the world.  Ireland was the first country in the world to introduce a nationwide screening programme in February 1966, [13] Austria started screening the same year [14] and England in 1968. [15]
With the advent of tandem mass spectrometry as a screening tool, several fatty acid oxidation disorders were targeted for inclusion in newborn screening programs. Medium chain acyl-CoA dehydrogenase deficiency (MCADD), which had been implicated in several cases of sudden infant death syndrome [16] [17] [18] was one of the first conditions targeted for inclusion.  MCADD was the first condition added when the United Kingdom expanded their screening program from PKU only. [11] Population based studies in Germany, the United States and Australia put the combined incidence of fatty acid oxidation disorders at 1:9300 among Caucasians.  The United States screens for all known fatty acid oxidation disorders, either as primary or secondary targets, while other countries screen for a subset of these. [19]
The introduction of screening for fatty acid oxidation disorders has been shown to have reduced morbidity and mortality associated with the conditions, particularly MCADD.  An Australian study found a 74% reduction in episodes of severe metabolic decompensation or death among individuals identified by newborn screening as having MCADD versus those who presented clinically prior to screening.  Studies in the Netherlands and United Kingdom found improvements in outcome at a reduced cost when infants were identified before presenting clinically. [19]
Newborn screening programs have also expanded the information base available about some rare conditions.  Prior to its inclusion in newborn screening, short-chain acyl-CoA dehydrogenase deficiency (SCADD) was thought to be life-threatening.  Most patients identified via newborn screening as having this enzyme deficiency were asymptomatic , to the extent that SCADD was removed from screening panels in a number of regions.  Without the cohort of patients identified by newborn screening, this clinical phenotype would likely not have been identified. [19]
The most commonly included disorders of the endocrine system are congenital hypothyroidism (CH) and congenital adrenal hyperplasia (CAH). [20] Testing for both disorders can be done using blood samples collected on the standard newborn screening card.  Screening for CH is done by measuring thyroxin (T4), thyrotropin (TSH) or a combination of both analytes.  Elevated 17α-hydroxyprogesterone (17α-OHP) is the primary marker used when screening for CAH, most commonly done using enzyme-linked immunosorbant assays , with many programs using a second tier tandem mass spectrometry test to reduce the number of false positive results. [20] Careful analysis of screening results for CAH may also identify cases of congenital adrenal hypoplasia , which presents with extremely low levels of 17α-OHP. [20]
CH was added to many newborn screening programs in the 1970s, often as the second condition included after PKU.  The most common cause of CH is dysgenesis of the thyroid gland After many years of newborn screening, the incidence of CH worldwide had been estimated at 1:3600 births, with no obvious increases in specific ethnic groups.  Recent data from certain regions have shown an increase, with New York reporting an incidence of 1:1700.  Reasons for the apparent increase in incidence have been studied, but no explanation has been found. [20]
Classic CAH, the disorder targeted by newborn screening programs, is caused by a deficiency of the enzyme steroid 21-hydroxylase and comes in two forms – simple virilizing and a salt-wasting form.  The incidence of CAH can vary greatly between populations.  The highest reported incidence rates are among the Yupic Eskimos of Alaska (1:280) and on the French island of Réunion (1:2100). [20]
Any condition that results in the production of abnormal hemoglobin is included under the broad category of hemoglobinopathies .  Worldwide, it is estimated that 7% of the population may carry a hemoglobinopathy with clinical significance. [21] The most well known condition in this group is sickle cell disease . [21] Newborn screening for a large number of hemoglobinopathies is done by detecting abnormal patterns using isoelectric focusing , which can detect many different types of abnormal hemoglobins. [21] In the United States, newborn screening for sickle cell disease was recommended for all infants in 1987, however it was not implemented in all 50 states until 2006. [21]
Early identification of individuals with sickle cell disease and other hemoglobinopathies allows treatment to be initiated in a timely fashion. Penicillin has been used in children with sickle cell disease, and blood transfusions are used for patients identified with severe thalassemia . [21]
Most jurisdictions did not start screening for any of the organic acidemias before tandem mass spectrometry significantly expanded the list of disorders detectable by newborn screening.  Quebec has run a voluntary second-tier screening program since 1971 using urine samples collected at three weeks of age to screen for an expanded list of organic acidemias using a thin layer chromatography method. [22] Newborn screening using tandem mass spectrometry can detect several organic acidemias, including propionic acidemia , methylmalonic acidemia and isovaleric acidemia .
Cystic fibrosis (CF) was first added to newborn screening programs in New Zealand and regions of Australia in 1981, by measuring immunoreactive trypsinogen (IRT) in dried blood spots. [23] After the CFTR gene was identified, Australia introduced a two tier testing program to reduce the number of false positives .  Samples with an elevated IRT value were then analyzed with molecular methods to identify the presence of disease causing mutations before being reported back to parents and health care providers. [24] CF is included in the core panel of conditions recommended for inclusion in all 50 states, Texas was the last state to implement their screening program for CF in 2010. [25] Alberta was the first Canadian province to implement CF screening in 2007. [26] Quebec, New Brunswick, Nova Scotia, Newfoundland and Prince Edward Island do not include CF in their screening programs. [27] The United Kingdom as well as many European Union countries screen for CF as well. [27] Switzerland is one of the latest countries to add CF to their newborn screening menu, doing so in January 2011. [23]
Disorders of the distal urea cycle , such as citrullinemia , argininosuccinic aciduria and argininemia are included in newborn screening programs in many jurisdictions that using tandem mass spectrometry to identify key amino acids.  Proximal urea cycle defects, such as ornithine transcarbamylase deficiency and carbamoyl phosphate synthetase deficiency are not included in newborn screening panels because they are not reliably detected using current technology, and also because severely affected infants will present with clinical symptoms before newborn screening results are available. Some regions claim to screen for HHH syndrome (hyperammonemia, hyperornithinemia, homocitrullinuria) based on the detection of elevated ornithine levels in the newborn screening dried blood spot, but other sources have shown that affected individuals do not have elevated ornithine at birth. [28]
Lysosomal storage disorders are not included in newborn screening programs with high frequency.  As a group, they are heterogenous, with screening only being feasible for a small fraction of the approximately 40 identified disorders.  The arguments for their inclusion in newborn screening programs center around the advantage of early treatment (when treatment is available), avoiding a diagnostic odyssey for families and providing information for family planning to couples who have an affected child. [29] The arguments against including these disorders, as a group or individually center around the difficulties with reliably identifying individuals who will be affected with a severe form of the disorder, the relatively unproven nature of the treatment methods, and the high cost / high risk associated with some treatment options. [29]
New York State started a pilot study to screen for Krabbe disease in 2006, largely due to the efforts of Jim Kelly , whose son, Hunter, was affected with the disease. [30] A pilot screening program for four lysosomal storage diseases ( Gaucher disease , Pompe disease , Fabry disease and Niemann-Pick disease was undertaken using anonymised dried blood spots was completed in Austria in 2010.  Their data showed an increased incidence from what was expected in the population, and also a number of late onset forms of disease, which are not typically the target for newborn screening programs. [31]
Undiagnosed hearing loss in a child can have serious effects on many developmental areas, including language, social interactions, emotions, cognitive ability, academic performance and vocational skills, any combination of which can have negative impacts on the quality of life. [32] The serious impacts of a late diagnosis, combined with the high incidence (estimated at 1 - 3 per 1000 live births, and as high as 4% for neonatal intensive care unit patients) have been the driving forces behind screening programs designed to identify infants with hearing loss as early as possible.  Early identification allows these patients and their families to access the necessary resources to help them maximize their developmental outcomes. [32]
Newborn hearing testing is done at the bedside using transiently evoked otoacoustic emissions, automated auditory brainstem responses, or a combination of both techniques.  Hearing screening programs have found the initial testing to cost between $10.20 and $23.37 per baby, depending on the technology used. [32] As these are screening tests only, false positive results will occur. False positive results could be due to user error, a fussy baby, environmental noise in the testing room, or fluid or congestion in the outer/middle ear of the baby.  A review of hearing screening programs found varied initial referral rates (screen positive results) from 0.6% to 16.7%.  The highest overall incidence of hearing loss detection was 0.517%. [32] A significant proportion of screen positive infants were lost to follow-up before a diagnosis could be confirmed or ruled out in all screening programs. [32]
In some cases, critical congenital heart defects (CCHD) are not identified by prenatal ultrasound or postnatal physical examination. Pulse oximetry has been recently added as a bedside screening test for CCHD [33] at 24 to 48 hours after birth. However, not all heart problems can be detected by this method, which relies only on blood oxygen levels.
When a baby tests positive, urgent subsequent examination, such as echocardiography , is undergone to determine the cause of low oxygen levels. Babies diagnosed with CCHD are then seen by cardiologists .
Severe combined immunodeficiency (SCID) caused by T-cell deficiency is a disorder that was recently added to newborn screening programs in some regions of the United States.  Wisconsin was the first state to add SCID to their mandatory screening panel in 2008, and it was recommended for inclusion in all states' panels in 2010. Since December 2018 all US´states perform SCID screening. [34] As the first country in Europe, Norway started nationwide SCID screening January 2018. [35] [36] Identification of infants with SCID is done by detecting T-cell receptor excision circles (TRECs) using real-time polymerase chain reaction (qPCR).  TRECs are decreased in infants affected with SCID. [37]
SCID has not been added to newborn screening in a wide scale for several reasons.  It requires technology that is not currently used in most newborn screening labs, as PCR is not used for any other assays included in screening programs.  Follow-up and treatment of affected infants also requires skilled immunologists , which may not be available in all regions.  Treatment for SCID is a stem cell transplant , which cannot be done in all centers. [37]
Duchenne muscular dystrophy (DMD) is an X-linked disorder caused by defective production of dystrophin .  Many jurisdictions around the world have screened for, or attempted to screen for DMD using elevated levels of creatine kinase measured in dried blood spots.  Because universal newborn screening for DMD has not been undertaken, affected individuals often have a significant delay in diagnosis.  As treatment options for DMD become more and more effective, interest in adding a newborn screening test increases.  At various times since 1978, DMD has been included (often as a pilot study on a small subset of the population) in newborn screening programs in Edinburgh , Germany , Canada , France , Wales , Cyprus , Belgium and the United States .  In 2012, Belgium was the only country that continued to screen for DMD using creatine kinase levels. [38]
As treatments improve, newborn screening becomes a possibility for disorders that could benefit from early intervention, but none was previously available. Adrenoleukodystrophy (ALD), a peroxisomal disease that has a variable clinical presentation is one of the disorders that has become a target for those seeking to identify patients early.  ALD can present in several different forms, some of which do not present until adulthood, making it a difficult choice for countries to add to screening programs.  The most successful treatment option is a stem cell transplant , a procedure that carries a significant risk. [39]
Newborn screening tests are most commonly done from whole blood samples collected on specially designed filter paper, originally designed by Robert Guthrie.  The filter paper is often attached to a form containing required information about the infant and parents.  This includes date and time of birth, date and time of sample collection, the infant's weight and gestational age.  The form will also have information about whether the baby has had a blood transfusion and any additional nutrition the baby may have received ( total parenteral nutrition ).  Most newborn screening cards also include contact information for the infant's physician in cases where follow up screening or treatment is needed.  The Canadian province of Quebec performs newborn screening on whole blood samples collected as in most other jurisdictions, and also runs a voluntary urine screening program where parents collect a sample at 21 days of age and submit it to a provincial laboratory for an additional panel of conditions. [40] [22]
Newborn screening samples are collected from the infant between 24 hours and 7 days after birth, and it is recommended that the infant has fed at least once.  Individual jurisdictions will often have more specific requirements, with some states accepting samples collected at 12 hours, and others recommending to wait until 48 hours of life or later.  Each laboratory will have its own criteria on when a sample is acceptable, or if another would need to be collected.  Samples can be collected at the hospital, or by midwives .  Samples are transported daily to the laboratory responsible for testing.  In the United States and Canada, newborn screening is mandatory, with an option for parents to opt out of the screening in writing if they desire.  In many regions, NBS is mandatory, with an option for parents to opt out in writing if they choose not to have their infant screened. [41] In most of Europe, newborn screening is done with the consent of the parents.  Proponents of mandatory screening claim that the test is for the benefit of the child, and that parents should not be able to opt out on their behalf.  In regions that favour informed consent for the procedure, they report no increase in costs, no decrease in the number of children screened and no cases of included diseases in children who did not undergo screening. [42]
Because newborn screening programs test for a number of conditions, a number of laboratorial methodologies are used, as well as bedside testing for hearing loss using evoked auditory potentials [32] and congenital heart defects using pulse oximetry . [33] Newborn screening started out using simple bacterial inhibition assays to screen for a single disorder, starting with phenylketonuria in the early 1960s. [12] With this testing methodology, newborn screening required one test to detect one condition.  As mass spectrometry became more widely available, the technology allowed rapid determination of a number of acylcarnitines and amino acids from a single dried blood spot.  This increased the number of conditions that could be detected by newborn screening.  Enzyme assays are used to screen for galactosemia and biotinidase deficiency .  Immunoassays measure thyroid hormones for the diagnosis of congenital hypothyroidism and 17α-hydroxyprogesterone for the diagnosis of congenital adrenal hyperplasia .  Molecular techniques are used for the diagnosis of cystic fibrosis and severe combined immunodeficiency .
The goal is to report the results within a short period of time. If screens are normal, a paper report is sent to the submitting hospital and parents rarely hear about it.  If an abnormality is detected, employees of the agency, usually nurses, begin to try to reach the physician, hospital, and/or nursery by telephone. They are persistent until they can arrange an evaluation of the infant by an appropriate specialist physician (depending on the disease). The specialist will attempt to confirm the diagnosis by repeating the tests by a different method or laboratory, or by performing other corroboratory or disproving tests. The confirmatory test varies depending on the positive results on the initial screen. Confirmatory testing can include analyte specific assays to confirm any elevations detected, functional studies to determine enzyme activity, and genetic testing to identify disease-causing mutations.  In some cases, a positive newborn screen can also trigger testing on other family members, such as siblings who did not undergo newborn screening for the same condition or the baby's mother, as some maternal conditions can be identified through results on the baby's newborn screen.  Depending on the likelihood of the diagnosis and the risk of delay, the specialist will initiate treatment and provide information to the family. Performance of the program is reviewed regularly and strenuous efforts are made to maintain a system that catches every infant with these diagnoses.  Guidelines for newborn screening and follow up have been published by the American Academy of Pediatrics [43] and the American College of Medical Genetics . [44]
Newborn screening programs participate in quality control programs as in any other laboratory, with some notable exceptions.  Much of the success of newborn screening programs is dependent on the filter paper used for the collection of the samples.  Initial studies using Robert Guthrie's test for PKU reported high false positive rates that were attributed to a poorly selected type of filter paper. [45] This source of variation has been eliminated in most newborn screening programs through standardization of approved sources of filter paper for use in newborn screening programs.  In most regions, the newborn screening card (which contains demographic information as well as attached filter paper for blood collection) is supplied by the organization carrying out the testing, to remove variations from this source. [45]
Newborn screening tests have become a subject of political controversy in the last decade.  Lawsuits, media attention, and advocacy groups have surfaced a number of different, and possibly countervailing, positions on the use of screening tests.  Some have asked for government mandates to widen the extent of the screening to find detectable and treatable birth defects.  Others have opposed mandatory screening concerned that effective follow-up and treatment may not be available, or that false positive screening tests may cause harm to infants and their families.  Others have learned that government agencies were often secretly storing the results in databases for future genetic research, often without consent of the parents nor limits on how the data could be used in the future.
Many rare diseases have not historically been tested for or testing that has been available has not been mandatory.  One such disease is glutaric acidemia type I , a neurometabolic disease present in approximately 1 out of every 100,000 live births. [46] A short-term California testing pilot project in 2003 and 2004 demonstrated the cost of forgoing rare disease testing on newborns.  While both Zachary Wyvill and Zachary Black were both born with the same disease during the pilot program, Wyvill's birth hospital tested only for four state-mandated diseases while Black was born at a hospital participating in the pilot program.  Wyvill's disease went undetected for over six months during which irreversible damage occurred but Black's disease was treated with diet and vitamin supplements. [47] Both sets of parents became advocates for expanded neonatal testing and testified in favor of expanding tandem mass spectrometry (MS/MS) testing of newborns for rare diseases.  By August, 2004, the California state budget law had passed requiring the use of tandem mass spectroscopy to test for more than 30 genetic illnesses and provided funding. [48] California now mandates newborn screening for all infants and tests for 80 congenital and genetic disorders. [49]
Instituting MS/MS screening often requires a sizable up front expenditure.  When states choose to run their own programs the initial costs for equipment, training and new staff can be significant. Moreover, MS/MS gives only the screening result and not the confirmatory result. The same has to be further done by higher technologies or procedure like GC/MS [ clarification needed ] , Enzyme Assays or DNA Tests. This in effect adds more cost burden and makes physicians lose precious time. [ according to whom? ] To avoid at least a portion of the up front costs, some states such as Mississippi have chosen to contract with private labs for expanded screening.  Others have chosen to form Regional Partnerships sharing both costs and resources.
But for many states, screening has become an integrated part of the department of health which can not or will not be easily replaced.  Thus the initial expenditures can be difficult for states with tight budgets to justify.  Screening fees have also increased in recent years as health care costs rise and as more states add MS/MS screening to their programs. (See Report of Summation of Fees Charged for Newborn Screening, 2001–2005) Dollars spent for these programs may reduce resources available to other potentially lifesaving programs.  It was recommended [ by whom? ] in 2006 that one disorder, Short Chain Acyl-coenzyme A Dehydrogenase Deficiency, or SCAD, be eliminated from screening programs, due to a "spurious association between SCAD and symptoms. [50] However, other [ when? ] studies suggested that perhaps expanded screening is cost effective (see ACMG report page 94-95 [ permanent dead link ] and articles published in Pediatrics [51] '. [52] Advocates are quick to point out studies such as these when trying to convince state legislatures to mandate expanded screening. [ citation needed ]
Expanded newborn screening is also opposed by among some health care providers, who are concerned that effective follow-up and treatment may not be available, that false positive screening tests may cause harm, and issues of informed consent . [53] A recent study by Genetic Alliance and partners suggests that communication between health care providers and parents may be key in minimizing the potential harm when a false positive test occurs. The results from this study also reveal that parents found newborn screening to be a beneficial and necessary tool to prevent treatable diseases. [54] To address the false positive issue, researchers from the University of Maryland, Baltimore and Genetic Alliance established a check-list to assist health care providers communicate with parents about a screen-positive result. [55]
Controversy has also erupted in some countries over collection and storage of blood or DNA samples by government agencies during the routine newborn blood screen.
In the United States, it was revealed that Texas had collected and stored blood and DNA samples on millions of newborns without the parents' knowledge or consent.  These samples were then used by the state for genetic experiments and to set up a database to catalog all of the samples/newborns. As of December 2009 [update] , samples obtained without parents' consent between 2002 and 2009 were slated to be destroyed following the settlement of "a lawsuit filed by parents against the Texas Department of Health Services and Texas A&M; for secretly storing and doing research on newborn blood samples." [56] A similar legal case was filed against the State of Minnesota .  Over 1 million newborn bloodspot samples were destroyed in 2011 "when the state's Supreme Court found that storage and use of blood spots beyond newborn screening panels was in violation of the state's genetic privacy laws.". [57] Nearly US$1 million was required to be paid by the state for the attorney's fees of the 21 families who advanced the lawsuit. [57] An advocacy group that has taken a position against research on newborn blood screening data without parental consent is the Citizens' Council for Health Freedom, who take the position that newborn health screening for "a specific set of newborn genetic conditions" is a very different matter than storing the data or those DNA samples indefinitely to "use them for genetic research without parental knowledge or consent." [57]
As additional tests are discussed for addition to the panels, issues arise.  Many question whether the expanded testing still falls under the requirements necessary to justify the additional tests. [58] Many of the new diseases being tested for are rare and have no known treatment, while some of the diseases need not be treated until later in life. [58] This raises more issues, such as: if there is no available treatment for the disease should we test for it at all? And if we do, what do we tell the families of those with children bearing one of the untreatable diseases? [59] Studies show that the rarer the disease is and the more diseases being tested for, the more likely the tests are to produce false-positives . [60] This is an issue because the newborn period is a crucial time for the parents to bond with the child, and it has been noted that ten percent of parents whose children were diagnosed with a false-positive still worried that their child was fragile and/or sickly even though they were not, potentially preventing the parent-child bond forming as it would have otherwise. [59] As a result, some parents may begin to opt out of having their newborns screened. Many parents are also concerned about what happens with their infant's blood samples after screening. The samples were originally taken to test for preventable diseases, but with the advance in genomic sequencing technologies many samples are being kept for DNA identification and research, [58] [59] increasing the possibility that more children will be opted out of newborn screening from parents who see the kept samples as a form of research done on their child. [58]
In epidemiology , the next-generation matrix is used to derive the basic reproduction number , for a compartmental model of the spread of infectious diseases . In population dynamics it is used to compute the basic reproduction number for structured population models. [1] It is also used in multi-type branching models for analogous computations. [2]
The method to compute the basic reproduction ratio using the next-generation matrix is given by Diekmann et al. (1990) [3] and van den Driessche and Watmough (2002). [4] To calculate the basic reproduction number by using a next-generation matrix, the whole population is divided into n {\displaystyle n} compartments in which there are m < n {\displaystyle m<n} infected compartments. Let x i , i = 1 , 2 , 3 , … , m {\displaystyle x_{i},i=1,2,3,\ldots ,m} be the numbers of infected individuals in the i t h {\displaystyle i^{th}} infected compartment at time t . Now, the epidemic model is
In the above equations, F i ( x ) {\displaystyle F_{i}(x)} represents the rate of appearance of new infections in compartment i {\displaystyle i} . V i + {\displaystyle V_{i}^{+}} represents the rate of transfer of individuals into compartment i {\displaystyle i} by all other means, and V i − ( x ) {\displaystyle V_{i}^{-}(x)} represents the rate of transfer of individuals out of compartment i {\displaystyle i} .
The above model can also be written as
where
and
Let x 0 {\displaystyle x_{0}} be the disease-free equilibrium. The values of the Jacobian matrices F ( x ) {\displaystyle F(x)} and V ( x ) {\displaystyle V(x)} are:
and
respectively.
Here, F {\displaystyle F} and V {\displaystyle V} are m × m matrices, defined as F = ∂ F i ∂ x j ( x 0 ) {\displaystyle F={\frac {\partial F_{i}}{\partial x_{j}}}(x_{0})} and V = ∂ V i ∂ x j ( x 0 ) {\displaystyle V={\frac {\partial V_{i}}{\partial x_{j}}}(x_{0})} .
Now, the matrix F V − 1 {\displaystyle FV^{-1}} is known as the next-generation matrix. The largest eigenvalue or spectral radius of F V − 1 {\displaystyle FV^{-1}} is the basic reproduction number of the model.
